{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95ff0fe-32f2-4c9b-899b-41c080a73681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TOPK = 100\n",
    "\n",
    "def bm25_sphere_retrieval(data):\n",
    "    from pyserini.search import LuceneSearcher\n",
    "    index_path = os.environ.get(\"BM25_SPHERE_PATH\")\n",
    "    print(\"loading bm25 index, this may take a while...\")\n",
    "    searcher = LuceneSearcher(index_path)\n",
    "\n",
    "    print(\"running bm25 retrieval...\")\n",
    "    for d in tqdm(data):\n",
    "        query = d[\"question\"]\n",
    "        try:\n",
    "            hits = searcher.search(query, TOPK)\n",
    "        except Exception as e:\n",
    "            #https://github.com/castorini/pyserini/blob/1bc0bc11da919c20b4738fccc020eee1704369eb/scripts/kilt/anserini_retriever.py#L100\n",
    "            if \"maxClauseCount\" in str(e):\n",
    "                query = \" \".join(query.split())[:950]\n",
    "                hits = searcher.search(query, TOPK)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        docs = []\n",
    "        for hit in hits:\n",
    "            h = json.loads(str(hit.docid).strip())\n",
    "            docs.append({\n",
    "                \"title\": h[\"title\"],\n",
    "                \"text\": hit.raw,\n",
    "                \"url\": h[\"url\"],\n",
    "            })\n",
    "        d[\"docs\"] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091604b8-9af0-40b3-88fb-a5806bae03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtr_build_index(encoder, docs):\n",
    "    with torch.inference_mode():\n",
    "        embs = encoder.encode(docs, batch_size=4, show_progress_bar=True, normalize_embeddings=True)\n",
    "        embs = embs.astype(\"float16\")\n",
    "\n",
    "    GTR_EMB = os.environ.get(\"GTR_EMB\")\n",
    "    with open(GTR_EMB, \"wb\") as f:\n",
    "        pickle.dump(embs, f)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9355e6-3615-45f0-9146-64d42a5fc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtr_wiki_retrieval(data):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"loading GTR encoder...\")\n",
    "    encoder = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\", device = device)\n",
    "\n",
    "    questions = [d[\"question\"] for d in data]\n",
    "    with torch.inference_mode():\n",
    "        queries = encoder.encode(questions, batch_size=4, show_progress_bar=True, normalize_embeddings=True)\n",
    "        queries = torch.tensor(queries, dtype=torch.float16, device=\"cpu\")\n",
    "\n",
    "    # the wikipedia split from DPR repo: https://github.com/facebookresearch/DPR\n",
    "    DPR_WIKI_TSV = os.environ.get(\"DPR_WIKI_TSV\")\n",
    "    docs = []\n",
    "    print(\"loading wikipedia file...\")\n",
    "    with open(DPR_WIKI_TSV) as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            docs.append(row[2] + \"\\n\" + row[1])\n",
    "\n",
    "    GTR_EMB = os.environ.get(\"GTR_EMB\")\n",
    "    if not os.path.exists(GTR_EMB):\n",
    "        print(\"gtr embeddings not found, building...\")\n",
    "        embs = gtr_build_index(encoder, docs)\n",
    "    else:\n",
    "        print(\"gtr embeddings found, loading...\")\n",
    "        with open(GTR_EMB, \"rb\") as f:\n",
    "            embs = pickle.load(f)\n",
    "\n",
    "    del(encoder) # save gpu mem\n",
    "\n",
    "    gtr_emb = torch.tensor(embs, dtype=torch.float16, device=device)\n",
    "\n",
    "    print(\"running GTR retrieval...\")\n",
    "    for qi, q in enumerate(tqdm(queries)):\n",
    "        q = q.to(device)\n",
    "        scores = torch.matmul(gtr_emb, q)\n",
    "        score, idx = torch.topk(scores, TOPK)\n",
    "        ret = []\n",
    "        for i in range(idx.size(0)):\n",
    "            title, text = docs[idx[i].item()].split(\"\\n\")\n",
    "            ret.append({\"id\": str(idx[i].item()+1),\"title\": title, \"text\": text, \"score\": score[i].item()})\n",
    "        data[qi][\"docs\"] = ret\n",
    "        q = q.to(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1661e729-cdce-4aef-b4de-1b0d38eac9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "def doc_to_text_tfidf(doc):\n",
    "    return doc['title'] + ' ' + doc['text']\n",
    "\n",
    "def doc_to_text_dense(doc):\n",
    "    return doc['title'] + '. ' + doc['text']\n",
    "\n",
    "\n",
    "class SearcherWithinDocs:\n",
    "\n",
    "    def __init__(self, docs, retriever, model=None, device=\"cuda\"):\n",
    "        self.retriever = retriever\n",
    "        self.docs = docs\n",
    "        self.device = device\n",
    "        if retriever == \"tfidf\":\n",
    "            self.tfidf = TfidfVectorizer()\n",
    "            self.tfidf_docs = self.tfidf.fit_transform([doc_to_text_tfidf(doc) for doc in docs])\n",
    "        elif \"gtr\" in retriever: \n",
    "            self.model = model\n",
    "            self.embeddings = self.model.encode([doc_to_text_dense(doc) for doc in docs], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def search(self, query):\n",
    "        # Return the top-1 result doc id\n",
    "\n",
    "        if self.retriever == \"tfidf\":\n",
    "            tfidf_query = self.tfidf.transform([query])[0]\n",
    "            similarities = [cosine_similarity(tfidf_doc, tfidf_query) for tfidf_doc in self.tfidf_docs]\n",
    "            best_doc_id = np.argmax(similarities)\n",
    "            return best_doc_id\n",
    "        elif \"gtr\" in self.retriever:\n",
    "            q_embed = self.model.encode([query], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n",
    "            score = torch.matmul(self.embeddings, q_embed.t()).squeeze(1).detach().cpu().numpy()\n",
    "            best_doc_id = np.argmax(score)\n",
    "            return best_doc_id\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67de3256-96b6-45eb-be8c-5623668d748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def remove_citations(sent):\n",
    "    return re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent)).replace(\" |\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def get_max_memory():\n",
    "    \"\"\"Get the maximum memory available for the current GPU for loading models.\"\"\"\n",
    "    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "    max_memory = f'{free_in_GB-6}GB'\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "    return max_memory\n",
    "\n",
    "\n",
    "def make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=None):\n",
    "    # For doc prompt:\n",
    "    # - {ID}: doc id (starting from 1)\n",
    "    # - {T}: title\n",
    "    # - {P}: text\n",
    "    # use_shorter: None, \"summary\", or \"extraction\"\n",
    "\n",
    "    text = doc['text']\n",
    "    if use_shorter is not None:\n",
    "        text = doc[use_shorter]\n",
    "    return doc_prompt.replace(\"{T}\", doc[\"title\"]).replace(\"{P}\", text).replace(\"{ID}\", str(doc_id+1))\n",
    "\n",
    "\n",
    "def get_shorter_text(item, docs, ndoc, key):\n",
    "    doc_list = []\n",
    "    for item_id, item in enumerate(docs):\n",
    "        if key not in item:\n",
    "            if len(doc_list) == 0:\n",
    "                # If there aren't any document, at least provide one (using full text)\n",
    "                item[key] = item['text']\n",
    "                doc_list.append(item)\n",
    "            logger.warn(f\"No {key} found in document. It could be this data do not contain {key} or previous documents are not relevant. This is document {item_id}. This question will only have {len(doc_list)} documents.\")\n",
    "            break\n",
    "        if \"irrelevant\" in item[key] or \"Irrelevant\" in item[key]:\n",
    "            continue\n",
    "        doc_list.append(item)\n",
    "        if len(doc_list) >= ndoc:\n",
    "            break\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def make_demo(item, prompt, ndoc=None, doc_prompt=None, instruction=None, use_shorter=None, test=False):\n",
    "    # For demo prompt\n",
    "    # - {INST}: the instruction\n",
    "    # - {D}: the documents\n",
    "    # - {Q}: the question\n",
    "    # - {A}: the answers\n",
    "    # ndoc: number of documents to put in context\n",
    "    # use_shorter: None, \"summary\", or \"extraction\"\n",
    "\n",
    "    prompt = prompt.replace(\"{INST}\", instruction).replace(\"{Q}\", item['question'])\n",
    "    if \"{D}\" in prompt:\n",
    "        if ndoc == 0:\n",
    "            prompt = prompt.replace(\"{D}\\n\", \"\") # if there is no doc we also delete the empty line\n",
    "        else:\n",
    "            doc_list = get_shorter_text(item, item[\"docs\"], ndoc, use_shorter) if use_shorter is not None else item[\"docs\"][:ndoc]\n",
    "            text = \"\".join([make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=use_shorter) for doc_id, doc in enumerate(doc_list)])\n",
    "            prompt = prompt.replace(\"{D}\", text)\n",
    "\n",
    "    if not test:\n",
    "        answer = \"\\n\" + \"\\n\".join(item[\"answer\"]) if isinstance(item[\"answer\"], list) else item[\"answer\"]\n",
    "        prompt = prompt.replace(\"{A}\", \"\").rstrip() + answer\n",
    "    else:\n",
    "        prompt = prompt.replace(\"{A}\", \"\").rstrip() # remove any space or \\n\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def load_model(model_name_or_path, dtype=torch.float16, int8=False, reserve_memory=10):\n",
    "    # Load a huggingface model and tokenizer\n",
    "    # dtype: torch.float16 or torch.bfloat16\n",
    "    # int8: whether to use int8 quantization\n",
    "    # reserve_memory: how much memory to reserve for the model on each gpu (in GB)\n",
    "\n",
    "    # Load the FP16 model\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    logger.info(f\"Loading {model_name_or_path} in {dtype}...\")\n",
    "    if int8:\n",
    "        logger.warn(\"Use LLM.int8\")\n",
    "    start_time = time.time()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map='auto',\n",
    "        torch_dtype=dtype,\n",
    "        max_memory=get_max_memory(),\n",
    "        load_in_8bit=int8,\n",
    "    )\n",
    "    logger.info(\"Finish loading in %.2f sec.\" % (time.time() - start_time))\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "\n",
    "    # Fix OPT bos token problem in HF\n",
    "    if \"opt\" in model_name_or_path:\n",
    "        tokenizer.bos_token = \"<s>\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "873ea4e9-08d0-486d-ae9f-5f763ee11aa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer, scoring\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from utils import normalize_answer, get_max_memory, remove_citations\n",
    "\n",
    "QA_MODEL=\"gaotianyu1350/roberta-large-squad\"\n",
    "AUTOAIS_MODEL=\"google/t5_xxl_true_nli_mixture\"\n",
    "\n",
    "global autoais_model, autoais_tokenizer\n",
    "autoais_model, autoais_tokenizer = None, None\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    \"\"\"Compute F1 score between two strings.\"\"\"\n",
    "\n",
    "    def _get_tokens(s):\n",
    "        if not s:\n",
    "            return []\n",
    "        return normalize_answer(s).split()\n",
    "\n",
    "    gold_toks = _get_tokens(a_gold)\n",
    "    pred_toks = _get_tokens(a_pred)\n",
    "\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    \"\"\"Check whether two strings are equal up to normalization.\"\"\"\n",
    "\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def exact_presence(short_answers, context):\n",
    "    \"\"\"Verify if any of the answers is present in the given context.\n",
    "    Args:\n",
    "        short_answers: list of short answers to look for in the context\n",
    "        context: a paragraph to search for short answers\n",
    "    Returns:\n",
    "        true if any of the short answers is present in the context\n",
    "    \"\"\"\n",
    "\n",
    "    n_short_answers = [normalize_answer(sa) for sa in short_answers]\n",
    "    n_context = normalize_answer(context)\n",
    "\n",
    "    for ans in n_short_answers:\n",
    "        if ans in n_context:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def compute_rouge(data):\n",
    "    \"\"\"Main function for rouge scoring.\n",
    "    If two references are provided,\n",
    "    the best score is chosen for each instance.\n",
    "    Args:\n",
    "        data: requires field `output` and `answer` (or `annotations` for ASQA)\n",
    "        metrics: list of evaluation metrics\n",
    "    Returns:\n",
    "        dictionary representation of rouge scores\n",
    "    \"\"\"\n",
    "    def _rouge_calculation(hypotheses,\n",
    "                        references1,\n",
    "                        references2=[],\n",
    "                        metrics=['rougeLsum']):\n",
    "\n",
    "        if references2 == []:\n",
    "            references2 = references1\n",
    "\n",
    "        scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for i in range(len(hypotheses)):\n",
    "            scores1 = scorer.score(references1[i], hypotheses[i])\n",
    "            scores2 = scorer.score(references2[i], hypotheses[i])\n",
    "            if scores1['rougeLsum'].fmeasure > scores2['rougeLsum'].fmeasure:\n",
    "                aggregator.add_scores(scores1)\n",
    "            else:\n",
    "                aggregator.add_scores(scores2)\n",
    "\n",
    "        scores = {m: [] for m in metrics}\n",
    "\n",
    "        for m in metrics:\n",
    "            fmeasure = aggregator.aggregate()[m].mid.fmeasure\n",
    "            scores[m].append(fmeasure)\n",
    "\n",
    "        for m in scores:\n",
    "            scores[m] = 100 * sum(scores[m]) / len(scores[m])\n",
    "\n",
    "        return scores\n",
    "\n",
    "    hypotheses = {}\n",
    "    references1 = {}\n",
    "    references2 = {}\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        hypotheses[idx] = item[\"output\"]\n",
    "        if \"annotations\" in item and item['annotations'] is not None: # For ASQA\n",
    "            references1[idx] = item[\"annotations\"][0][\"long_answer\"]\n",
    "            references2[idx] = item[\"annotations\"][1][\"long_answer\"]\n",
    "        else:\n",
    "            references1[idx] = item[\"answer\"]\n",
    "            references2[idx] = item[\"answer\"]\n",
    "\n",
    "    h, r1, r2 = [], [], []\n",
    "\n",
    "    for key in references1:\n",
    "        h.append(hypotheses[key])\n",
    "        r1.append(references1[key])\n",
    "\n",
    "        if references2 is not None:\n",
    "            r2.append(references2[key])\n",
    "\n",
    "    h = ['\\n'.join(sent_tokenize(text.lower())) for text in h]\n",
    "    r1 = ['\\n'.join(sent_tokenize(text.lower())) for text in r1]\n",
    "    r2 = ['\\n'.join(sent_tokenize(text.lower())) for text in r2]\n",
    "    scores = _rouge_calculation(h, r1, r2)\n",
    "\n",
    "    return scores['rougeLsum']\n",
    "\n",
    "\n",
    "def compute_str_em(data):\n",
    "    \"\"\"Compute STR-EM metric (only for ASQA)\n",
    "    Args:\n",
    "        data: requires field `qa_pairs/short_answers` and `output`\n",
    "    Returns:\n",
    "        STR-EM and STR-EM-HIT ()\n",
    "    \"\"\"\n",
    "\n",
    "    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n",
    "        return 0, 0\n",
    "\n",
    "    acc = []\n",
    "    hit = []\n",
    "\n",
    "    for item in data:\n",
    "        loc_acc = []\n",
    "        for qa_pair in item['qa_pairs']:\n",
    "            loc_acc.append(exact_presence(qa_pair['short_answers'], item[\"output\"]))\n",
    "        acc.append(np.mean(loc_acc))\n",
    "        hit.append( int(np.mean(loc_acc) == 1) )\n",
    "\n",
    "    return 100 * np.mean(acc), 100 * np.mean(hit)\n",
    "\n",
    "\n",
    "def compute_len(data):\n",
    "    \"\"\"Compute average length of predictions.\"\"\"\n",
    "\n",
    "    res, cntr = 0, 0\n",
    "    for item in data:\n",
    "        res += len(item[\"output\"].split())\n",
    "        cntr += 1\n",
    "    return res / cntr\n",
    "\n",
    "\n",
    "def compute_qa(data):\n",
    "    \"\"\"Compute QA-based accuracy.\n",
    "    Args:\n",
    "        data: requires filed `qa_pairs/short_answers` and `output`\n",
    "    Returns:\n",
    "        QA metrics (QA-EM, QA-F1, QA-Hit)\n",
    "    \"\"\"\n",
    "\n",
    "    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n",
    "        logger.warn(\"Warning: no QA pairs found in data\")\n",
    "        return {\n",
    "            'QA-EM': 0,\n",
    "            'QA-F1': 0,\n",
    "            'QA-Hit': 0,\n",
    "        }\n",
    "\n",
    "    # Load model\n",
    "    logger.info(\"Loading the RoBERTa-large SQuAD model for QA-based accuracy...\")\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=QA_MODEL, device=0)\n",
    "    logger.info(\"Done\")\n",
    "\n",
    "    # Get prediction\n",
    "    logger.info(\"Computing the QA-based accuracy...\")\n",
    "    em, f1, bins = [], [], []\n",
    "    for item in tqdm(data):\n",
    "        question = [qa_pair['question'] for qa_pair in item['qa_pairs']]\n",
    "        context = item['output'] if len(item['output']) > 0 else \" \"\n",
    "        results = qa_pipeline(question=question, context=context, handle_impossible_answer=True)\n",
    "        loc_counter, loc_em, loc_f1 = 0, 0, 0\n",
    "\n",
    "        for idx, res in enumerate(results):\n",
    "            answers = item[\"qa_pairs\"][idx][\"short_answers\"]\n",
    "            prediction = res[\"answer\"]\n",
    "\n",
    "            loc_em += max([compute_exact(a, prediction) for a in answers])\n",
    "            loc_f1 += max([compute_f1(a, prediction) for a in answers])\n",
    "            loc_counter += 1\n",
    "\n",
    "        em.append(loc_em / loc_counter)\n",
    "        f1.append(loc_f1 / loc_counter)\n",
    "        bins.append(loc_em == loc_counter)\n",
    "\n",
    "    return {\n",
    "        'QA-EM': 100 * np.mean(em),\n",
    "        'QA-F1': 100 * np.mean(f1),\n",
    "        'QA-Hit': 100 * np.mean(bins)\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_mauve(data):\n",
    "    \"\"\"Compute Mauve score.\"\"\"\n",
    "\n",
    "    logger.info(\"Computing MAUVE...\")\n",
    "    human_data = []\n",
    "    model_data = []\n",
    "    for item in data:\n",
    "        # Remove ending punctuations\n",
    "        # Remove any new lines\n",
    "        # Truncate by 100 words\n",
    "        human_data.append(' '.join((item['question'] + \" \" + item['answer'].strip()).split()[:100]).rstrip(string.punctuation))\n",
    "        model_data.append(' '.join((item['question'] + \" \" + item['output'].strip()).split()[:100]).rstrip(string.punctuation))\n",
    "\n",
    "    import mauve\n",
    "    out = mauve.compute_mauve(\n",
    "        p_text=human_data,\n",
    "        q_text=model_data,\n",
    "        device_id=0,\n",
    "        max_text_length=512,\n",
    "        verbose=True,\n",
    "        batch_size=8,\n",
    "        featurize_model_name=\"gpt2-large\"\n",
    "    )\n",
    "    return out.mauve * 100\n",
    "\n",
    "\n",
    "def _run_nli_autoais(passage, claim):\n",
    "    \"\"\"\n",
    "    Run inference for assessing AIS between a premise and hypothesis.\n",
    "    Adapted from https://github.com/google-research-datasets/Attributed-QA/blob/main/evaluation.py\n",
    "    \"\"\"\n",
    "    global autoais_model, autoais_tokenizer\n",
    "    input_text = \"premise: {} hypothesis: {}\".format(passage, claim)\n",
    "    input_ids = autoais_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(autoais_model.device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = autoais_model.generate(input_ids, max_new_tokens=10)\n",
    "    result = autoais_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    inference = 1 if result == \"1\" else 0\n",
    "    return inference\n",
    "\n",
    "\n",
    "def compute_claims(data):\n",
    "    global autoais_model, autoais_tokenizer\n",
    "    if autoais_model is None:\n",
    "        logger.info(\"Loading AutoAIS model...\")\n",
    "        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(AUTOAIS_MODEL, torch_dtype=torch.bfloat16, max_memory=get_max_memory(), device_map=\"auto\")\n",
    "        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n",
    "\n",
    "    logger.info(\"Computing claims...\")\n",
    "    scores = []\n",
    "    for item in tqdm(data):\n",
    "        normalized_output = remove_citations(item['output'])\n",
    "        entail = 0\n",
    "        claims = item[\"claims\"]\n",
    "        for claim in claims:\n",
    "            entail += _run_nli_autoais(normalized_output, claim)\n",
    "        scores.append(entail / len(claims))\n",
    "    return 100 * np.mean(scores)\n",
    "\n",
    "\n",
    "def compute_autoais(data,\n",
    "                    decontext=False,\n",
    "                    concat=False,\n",
    "                    qampari=False,\n",
    "                    at_most_citations=None,):\n",
    "    \"\"\"\n",
    "    Compute AutoAIS score.\n",
    "\n",
    "    Args:\n",
    "        data: requires field `output` and `docs`\n",
    "              - docs should be a list of items with fields `title` and `text` (or `phrase` and `sent` for QA-extracted docs)\n",
    "        citation: check citations and use the corresponding references.\n",
    "        decontext: decontextualize the output\n",
    "    \"\"\"\n",
    "\n",
    "    global autoais_model, autoais_tokenizer\n",
    "    if autoais_model is None:\n",
    "        logger.info(\"Loading AutoAIS model...\")\n",
    "        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(AUTOAIS_MODEL, torch_dtype=torch.bfloat16, max_memory=get_max_memory(), device_map=\"auto\")\n",
    "        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n",
    "\n",
    "    logger.info(f\"Running AutoAIS...\")\n",
    "\n",
    "    def _format_document(doc):\n",
    "        \"\"\"Format document for AutoAIS.\"\"\"\n",
    "\n",
    "        if \"sent\" in doc:\n",
    "            # QA-extracted docs\n",
    "            return \"Title: %s\\n%s\" % (doc['title'], doc['sent'])\n",
    "        else:\n",
    "            return \"Title: %s\\n%s\" % (doc['title'], doc['text'])\n",
    "\n",
    "    ais_scores = []\n",
    "    ais_scores_prec = []\n",
    "\n",
    "    sent_total = 0\n",
    "    sent_mcite = 0\n",
    "    sent_mcite_support = 0\n",
    "    sent_mcite_overcite = 0\n",
    "    autoais_log = []\n",
    "    for item in tqdm(data):\n",
    "        # Get sentences by using NLTK\n",
    "        if qampari:\n",
    "            sents = [item['question'] + \" \" + x.strip() for x in item['output'].rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n",
    "        else:\n",
    "            sents = sent_tokenize(item['output'])\n",
    "        if len(sents) == 0:\n",
    "            continue\n",
    "\n",
    "        target_sents = [remove_citations(sent).strip() for sent in sents]\n",
    "\n",
    "        entail = 0\n",
    "        entail_prec = 0\n",
    "        total_citations = 0\n",
    "        for sent_id, sent in enumerate(sents):\n",
    "            target_sent = target_sents[sent_id] # Citation removed and (if opted for) decontextualized\n",
    "            joint_entail = -1 # Undecided\n",
    "\n",
    "            # Find references\n",
    "            ref = [int(r[1:])-1 for r in re.findall(r\"\\[\\d+\", sent)] # In text citation id starts from 1\n",
    "            logger.info(f\"For `{sent}`, find citations {ref}\")\n",
    "            if len(ref) == 0:\n",
    "                # No citations\n",
    "                joint_entail = 0\n",
    "            elif any([ref_id >= len(item['docs']) for ref_id in ref]):\n",
    "                # Citations out of range\n",
    "                joint_entail = 0\n",
    "            else:\n",
    "                if at_most_citations is not None:\n",
    "                    ref = ref[:at_most_citations]\n",
    "                total_citations += len(ref)\n",
    "                joint_passage = '\\n'.join([_format_document(item['docs'][psgs_id]) for psgs_id in ref])\n",
    "\n",
    "            # If not directly rejected by citation format error, calculate the recall score\n",
    "            if joint_entail == -1: \n",
    "                joint_entail = _run_nli_autoais(joint_passage, target_sent)\n",
    "                autoais_log.append({\n",
    "                    \"question\": item['question'],\n",
    "                    \"output\": item['output'],\n",
    "                    \"claim\": sent,\n",
    "                    \"passage\": [joint_passage],\n",
    "                    \"model_type\": \"NLI\",\n",
    "                    \"model_output\": joint_entail,\n",
    "                })\n",
    "\n",
    "            entail += joint_entail\n",
    "            if len(ref) > 1:\n",
    "                sent_mcite += 1\n",
    "\n",
    "            # calculate the precision score if applicable\n",
    "            if joint_entail and len(ref) > 1:\n",
    "                sent_mcite_support += 1\n",
    "                # Precision check: did the model cite any unnecessary documents?\n",
    "                for psgs_id in ref:\n",
    "                    # condition A\n",
    "                    passage = _format_document(item['docs'][psgs_id]) \n",
    "                    nli_result = _run_nli_autoais(passage, target_sent)\n",
    "\n",
    "                    # condition B\n",
    "                    if not nli_result:\n",
    "                        subset_exclude = copy.deepcopy(ref)\n",
    "                        subset_exclude.remove(psgs_id)\n",
    "                        passage = '\\n'.join([_format_document(item['docs'][pid]) for pid in subset_exclude])\n",
    "                        nli_result = _run_nli_autoais(passage, target_sent)\n",
    "                        if nli_result: # psgs_id is not necessary\n",
    "                            flag = 0\n",
    "                            sent_mcite_overcite += 1 \n",
    "                        else:\n",
    "                            entail_prec += 1\n",
    "                    else:\n",
    "                        entail_prec += 1\n",
    "            else:\n",
    "                entail_prec += joint_entail \n",
    "\n",
    "        sent_total += len(sents)\n",
    "        ais_scores.append(entail / len(sents))\n",
    "        ais_scores_prec.append(entail_prec / total_citations if total_citations > 0 else 0) # len(sents))\n",
    "\n",
    "    if sent_mcite > 0 and sent_mcite_support > 0:\n",
    "        print(\"Among all sentences, %.2f%% have multiple citations, among which %.2f%% are supported by the joint set, among which %.2f%% overcite.\" % (\n",
    "            100 * sent_mcite / sent_total, \n",
    "            100 * sent_mcite_support / sent_mcite, \n",
    "            100 * sent_mcite_overcite / sent_mcite_support\n",
    "        ))\n",
    "\n",
    "    return {\n",
    "        \"citation_rec\": 100 * np.mean(ais_scores),\n",
    "        \"citation_prec\": 100 * np.mean(ais_scores_prec),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_qampari_f1(data, cot=False):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    rec_top5 = []\n",
    "    f1 = []\n",
    "    f1_top5 = []\n",
    "\n",
    "    num_preds = []\n",
    "    for item in data:\n",
    "        if cot:\n",
    "            if \":\" in item['output']:\n",
    "                o = ':'.join(item['output'].split(\":\")[1:]) # try to separate the COT part and the answer list part.\n",
    "            else:\n",
    "                o = \"\"\n",
    "        else:\n",
    "            o = item['output']\n",
    "        preds = [normalize_answer(x.strip()) for x in o.rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n",
    "        preds = [p for p in preds if len(p) > 0] # delete empty answers\n",
    "        num_preds.append(len(preds))\n",
    "        answers = [[normalize_answer(x) for x in ans] for ans in item['answers']]\n",
    "        flat_answers = [item for sublist in answers for item in sublist]\n",
    "        \n",
    "        prec.append(sum([p in flat_answers for p in preds]) / len(preds) if len(preds) > 0 else 0)\n",
    "        rec.append(sum([any([x in preds for x in a]) for a in answers]) / len(answers))\n",
    "        rec_top5.append(min(5, sum([any([x in preds for x in a]) for a in answers])) / min(5, len(answers)))\n",
    "        if (prec[-1] + rec[-1]) == 0:\n",
    "            f1.append(0)\n",
    "        else:\n",
    "            f1.append(2 * prec[-1] * rec[-1] / (prec[-1] + rec[-1]))\n",
    "        if (prec[-1] + rec_top5[-1]) == 0:\n",
    "            f1_top5.append(0) \n",
    "        else:\n",
    "            f1_top5.append(2 * prec[-1] * rec_top5[-1] / (prec[-1] + rec_top5[-1]))\n",
    "\n",
    "    return {\n",
    "        \"num_preds\": np.mean(num_preds),\n",
    "        \"qampari_prec\": 100 * np.mean(prec),\n",
    "        \"qampari_rec\": 100 * np.mean(rec),\n",
    "        \"qampari_rec_top5\": 100 * np.mean(rec_top5),\n",
    "        \"qampari_f1\": 100 * np.mean(f1),\n",
    "        \"qampari_f1_top5\": 100 * np.mean(f1_top5),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--f\", type=str, required=True, help=\"Output file. Should have field `question`, `output`, (ROUGE) `answer`, \\\n",
    "                        (accuracy) `qa_pairs`, (AIS) `docs`\")\n",
    "    parser.add_argument(\"--no_rouge\", action=\"store_true\", help=\"Do not evaluate ROUGE score\")\n",
    "    parser.add_argument(\"--qa\", action=\"store_true\", help=\"Use the QA model\")\n",
    "    parser.add_argument(\"--mauve\", action=\"store_true\", help=\"Use the mauve score model\")\n",
    "    parser.add_argument(\"--citations\", action=\"store_true\", help=\"Evaluation with citation\")\n",
    "    parser.add_argument(\"--at_most_citations\", type=int, default=3, help=\"At most take this many documents (mostly for precision)\")\n",
    "    parser.add_argument(\"--claims_nli\", action=\"store_true\", help=\"Use claims for ELI5\")\n",
    "\n",
    "    # QAMPARI\n",
    "    parser.add_argument(\"--cot\", action=\"store_true\", help=\"For QAMPARI, try to find colon and separate the COT and answer listing\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.f) as f:\n",
    "        data_with_config = json.load(f)\n",
    "    data = data_with_config['data'] \n",
    "\n",
    "    if \"qampari\" in args.f:\n",
    "        args.no_rouge = True\n",
    "        args.qa = False\n",
    "        args.mauve = False\n",
    "        args.decontext = False\n",
    "        qampari = True\n",
    "    else:\n",
    "        qampari = False\n",
    "\n",
    "    # Truncate by newline and remove on the fly search result\n",
    "    logger.warning(\"We remove all the pre/appended space/newlines and we truncate the answer by the first newline.\")\n",
    "    logger.warning(\"We replace any on the fly search result to standard bracket citation format.\")\n",
    "    for i in range(len(data)):\n",
    "        data[i]['output'] = data[i]['output'].strip().split(\"\\n\")[0]\n",
    "        data[i]['output'] = data[i]['output'].replace(\"<|im_end|>\", \"\")\n",
    "\n",
    "\n",
    "    # Remove all citations for all non-AutoAIS evaluation\n",
    "    normalized_data = copy.deepcopy(data)\n",
    "    for i in range(len(normalized_data)):\n",
    "        normalized_data[i]['output'] = remove_citations(normalized_data[i]['output'])\n",
    "\n",
    "    result = {}\n",
    "    result['length'] = compute_len(normalized_data)\n",
    "    result['str_em'], result['str_hit'] = compute_str_em(normalized_data)\n",
    "    if qampari:\n",
    "        result.update(compute_qampari_f1(normalized_data, cot=args.cot))\n",
    "    if not args.no_rouge:\n",
    "        result['rougeLsum'] = compute_rouge(normalized_data)\n",
    "    if args.qa:\n",
    "        result.update(compute_qa(normalized_data))\n",
    "    if args.mauve:\n",
    "        result['mauve'] = compute_mauve(normalized_data)\n",
    "    if args.citations: \n",
    "        result.update(compute_autoais(data, qampari=qampari, at_most_citations=args.at_most_citations))\n",
    "    if args.claims_nli:\n",
    "        result[\"claims_nli\"] = compute_claims(normalized_data)\n",
    "\n",
    "    print(result)\n",
    "    with open(args.f + \".score\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b029ef-3b38-4cdd-9fd7-5935cdc60123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5df767-9348-4eb8-88cb-1c902a49fb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a9c3b-b5b4-4486-bb45-523f8d6468f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce557f-91b2-4349-bfeb-4501b25ceedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60518d-357a-43e8-8d97-cceb2e68d490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
