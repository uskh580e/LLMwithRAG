{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Usman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from groq import Groq\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da52f82243d444c89a389642172fc463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums: 100%|##########| 1/1 [00:14<00:00, 14.67s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Usman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint C:\\MSC.Software\\Adams\\2018\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\1456ab3b3d57d5d29151304b510d08404b6fcc6aceb338619880c4b379f7228f\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "WARNING:tensorflow:From c:\\Users\\Usman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bleurt\\lib\\bert_tokenization.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bert_score = evaluate.load(\"bertscore\")\n",
    "bleurt = evaluate.load(\"bleurt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"chunk_size\": 800,\n",
    "        \"chunk_overlap\": 50,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"llm_model\": \"llama3-8b-8192\",\n",
    "        \"retriever_k\": 20,\n",
    "        \"context_docs\": 3,\n",
    "        \"api_key\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=config[\"chunk_size\"],\n",
    "            chunk_overlap=config[\"chunk_overlap\"]\n",
    "        )\n",
    "        \n",
    "    def process_documents(self, pdf_folder):\n",
    "        all_docs = []\n",
    "        processing_stats = {\"total_files\": 0, \"successful\": 0, \"failed\": 0, \"total_chunks\": 0}\n",
    "        \n",
    "        for file in os.listdir(pdf_folder):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                processing_stats[\"total_files\"] += 1\n",
    "                pdf_path = os.path.join(pdf_folder, file)\n",
    "                try:\n",
    "                    loader = PyMuPDFLoader(pdf_path)\n",
    "                    documents = loader.load()\n",
    "                    docs = self.text_splitter.split_documents(documents)\n",
    "                    all_docs.extend(docs)\n",
    "                    processing_stats[\"successful\"] += 1\n",
    "                    processing_stats[\"total_chunks\"] += len(docs)\n",
    "                except Exception as e:\n",
    "                    processing_stats[\"failed\"] += 1\n",
    "                    print(f\"Error processing {file}: {str(e)}\")\n",
    "        \n",
    "        return all_docs, processing_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptingStrategies:\n",
    "    @staticmethod\n",
    "    def zero_shot(question, context):\n",
    "        return [{\"role\": \"user\", \"content\": f\"Context: {context}\\nQuestion: {question}\"}]\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot(question, context):\n",
    "        examples = \"\"\"Q: How do transformers work?\n",
    "                    A: Self-attention mechanisms.\n",
    "\n",
    "                    Q: What is BERT's objective?\n",
    "                    A: Masked language modeling.\"\"\"\n",
    "        return [{\"role\": \"user\", \"content\": f\"{examples}\\n\\nContext: {context}\\nQuestion: {question}\"}]\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(question, context):\n",
    "        return [{\"role\": \"user\", \"content\": f\"Context: {context}\\nQuestion: {question}\\nLet's solve step by step:\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMetrics:\n",
    "    def __init__(self):\n",
    "        self.rouge = evaluate.load(\"rouge\")\n",
    "        self.bleu = evaluate.load(\"bleu\")\n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    \n",
    "    def compute_semantic_similarity(self, pred, ref):\n",
    "        pred_emb = self.sentence_model.encode([pred])[0]\n",
    "        ref_emb = self.sentence_model.encode([ref])[0]\n",
    "        return np.dot(pred_emb, ref_emb) / (np.linalg.norm(pred_emb) * np.linalg.norm(ref_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_similarity(prediction, reference):\n",
    "    pred_embedding = sentence_model.encode([prediction])[0]\n",
    "    ref_embedding = sentence_model.encode([reference])[0]\n",
    "    return np.dot(pred_embedding, ref_embedding) / (np.linalg.norm(pred_embedding) * np.linalg.norm(ref_embedding))\n",
    "\n",
    "def check_factual_consistency(prediction, context):\n",
    "    pred_ents = set(ent.text.lower() for ent in nlp(prediction).ents)\n",
    "    context_ents = set(ent.text.lower() for ent in nlp(context).ents)\n",
    "    return len(pred_ents.intersection(context_ents)) / len(pred_ents) if pred_ents else 1.0\n",
    "\n",
    "def compute_context_utilization(prediction, context):\n",
    "    pred_words = set(prediction.lower().split())\n",
    "    context_words = set(context.lower().split())\n",
    "    return len(pred_words.intersection(context_words)) / len(pred_words)\n",
    "\n",
    "def compute_recall_at_k(retrieved_docs, ground_truth, k=20):\n",
    "\n",
    "    \n",
    "    ground_truth = ground_truth.lower()\n",
    "    \n",
    "   \n",
    "    top_k_docs = retrieved_docs[:k]\n",
    "    \n",
    "    \n",
    "    for doc in top_k_docs:\n",
    "        if ground_truth in doc.page_content.lower():\n",
    "            return 1.0 \n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "\n",
    "def exact_match(prediction, reference):\n",
    "    return 1 if prediction.strip().lower() == reference.strip().lower() else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.api_key = config[\"api_key\"]\n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "        self.metrics = EnhancedMetrics()\n",
    "        self.doc_processor = DocumentProcessor(config)\n",
    "    \n",
    "    def compute_all_metrics(self, prediction, reference, context, retrieved_docs):\n",
    "        if not prediction or not reference:\n",
    "            return {\n",
    "                \"rouge\": 0,\n",
    "                \"bleu\": 0,\n",
    "                \"bert_score\": 0,\n",
    "                \"bleurt\": 0,\n",
    "                \"semantic_similarity\": 0,\n",
    "                \"factual_consistency\": 0,\n",
    "                \"context_utilization\": 0,\n",
    "                \"recall_at_k\": 0,\n",
    "                \"exact_match\": 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"rouge\": rouge.compute(predictions=[prediction], references=[reference])['rougeL'],\n",
    "            \"bleu\": bleu.compute(predictions=[prediction], references=[[reference]])['bleu'],\n",
    "            \"bert_score\": bert_score.compute(predictions=[prediction], references=[reference], lang=\"en\")['f1'],\n",
    "            \"bleurt\": bleurt.compute(predictions=[prediction], references=[reference])['scores'][0],\n",
    "            \"semantic_similarity\": compute_semantic_similarity(prediction, reference),\n",
    "            \"factual_consistency\": check_factual_consistency(prediction, context),\n",
    "            \"context_utilization\": compute_context_utilization(prediction, context),\n",
    "            \"recall_at_k\": compute_recall_at_k(retrieved_docs, reference),\n",
    "            \"exact_match\": exact_match(prediction, reference)\n",
    "        }\n",
    "\n",
    "\n",
    "    def evaluate_single_prompt(self, vectorstore, strategy_name, strategy_func, question, reference):\n",
    "        start_time = time.time()\n",
    "\n",
    "       \n",
    "        retrieved_docs = vectorstore.similarity_search(question, k=self.config[\"retriever_k\"])\n",
    "        if not retrieved_docs:\n",
    "            context = \"No relevant documents found. Answer based on prior knowledge.\"\n",
    "        else:\n",
    "            context = \"\\n\".join([doc.page_content for doc in retrieved_docs[:self.config[\"context_docs\"]]])\n",
    "\n",
    "        \n",
    "        messages = strategy_func(question, context)\n",
    "\n",
    "       \n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.config[\"llm_model\"],\n",
    "                    messages=messages\n",
    "                )\n",
    "                output = response.choices[0].message.content\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[{strategy_name}] Error: {e}. Retrying... ({attempt + 1}/3)\")\n",
    "                time.sleep(2 ** attempt)\n",
    "                output = \"Error generating response.\"\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "       \n",
    "        metrics = self.compute_all_metrics(output, reference, context, retrieved_docs)\n",
    "        metrics[\"latency\"] = end_time - start_time\n",
    "        metrics[\"strategy\"] = strategy_name\n",
    "        metrics[\"question\"] = question\n",
    "        metrics[\"generated_answer\"] = output\n",
    "        metrics[\"reference\"] = reference\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def extract_wait_time(self, error_message, attempt):\n",
    "        match = re.search(r'Please try again in ([\\d\\.]+)m([\\d\\.]+)s', error_message)\n",
    "        if match:\n",
    "            minutes = float(match.group(1))\n",
    "            seconds = float(match.group(2))\n",
    "            return int(minutes * 60 + seconds)\n",
    "        return min(30 * (2 ** attempt), 300)\n",
    "    \n",
    "    \n",
    "    def run_evaluation(self, questions, pdf_folder):\n",
    "            # Process documents\n",
    "            all_docs, processing_stats = self.doc_processor.process_documents(pdf_folder)\n",
    "            \n",
    "           \n",
    "            embedding_model = HuggingFaceEmbeddings(model_name=self.config[\"embedding_model\"])\n",
    "            vectorstore = FAISS.from_texts([doc.page_content for doc in all_docs], embedding_model)\n",
    "\n",
    "            \n",
    "            results = []\n",
    "\n",
    "          \n",
    "            for question, reference in questions:\n",
    "                for strategy_name, strategy_func in [\n",
    "                    (\"zero-shot\", PromptingStrategies.zero_shot),\n",
    "                    (\"few-shot\", PromptingStrategies.few_shot),\n",
    "                    (\"chain-of-thought\", PromptingStrategies.chain_of_thought)\n",
    "                ]:\n",
    "                    result = self.evaluate_single_prompt(\n",
    "                        vectorstore, strategy_name, strategy_func, question, reference\n",
    "                    )\n",
    "                    results.append(result)\n",
    "\n",
    "            \n",
    "            return pd.DataFrame(results), processing_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \n",
    "    (\"What is OpenScholar and how is it used?\", \n",
    "     \"OpenScholar is a research platform designed to enable large language models to generate and synthesize scientific literature. It incorporates retrieval-augmented generation to improve accuracy in scholarly tasks.\"),\n",
    "    \n",
    "   \n",
    "    (\"What is the ELI5 dataset and what is it used for?\", \n",
    "     \"The ELI5 (Explain Like I'm 5) dataset contains questions and answers from Reddit that aim to simplify complex topics into layman's terms. It is widely used to evaluate language models' ability to generate explanations.\"),\n",
    "    \n",
    "   \n",
    "    (\"What is LLAMA and how does it differ from other language models?\", \n",
    "     \"LLAMA (Large Language Model Meta AI) is a family of large language models developed by Meta. It focuses on efficiency, achieving high performance with fewer parameters compared to models like GPT-3.\"),\n",
    "    \n",
    "   \n",
    "    (\"How is citation generation performed for medical datasets?\", \n",
    "     \"Citation generation for medical data typically involves fine-tuning large language models on biomedical corpora. Retrieval-augmented generation is often used to enhance accuracy by referencing scientific articles during the generation process.\"),\n",
    "\n",
    "     (\"What kind of Questions does WebGPT answer?\",\"Long-Form Questions\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed.\n",
      "Batch 2 processed.\n",
      "\n",
      "All Results Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleurt</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>factual_consistency</th>\n",
       "      <th>context_utilization</th>\n",
       "      <th>recall_at_k</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>latency</th>\n",
       "      <th>strategy</th>\n",
       "      <th>question</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.838688850402832]</td>\n",
       "      <td>-0.901380</td>\n",
       "      <td>0.560973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.764239</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>What is OpenScholar and how is it used?</td>\n",
       "      <td>I'll review the provided context and ask quest...</td>\n",
       "      <td>OpenScholar is a research platform designed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8449322581291199]</td>\n",
       "      <td>-0.906185</td>\n",
       "      <td>0.399810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.738377</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>What is OpenScholar and how is it used?</td>\n",
       "      <td>I've read the context, which appears to be a p...</td>\n",
       "      <td>OpenScholar is a research platform designed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8427514433860779]</td>\n",
       "      <td>-1.253345</td>\n",
       "      <td>0.699241</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.755626</td>\n",
       "      <td>chain-of-thought</td>\n",
       "      <td>What is OpenScholar and how is it used?</td>\n",
       "      <td>I'd be happy to help you verify the informatio...</td>\n",
       "      <td>OpenScholar is a research platform designed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.098104</td>\n",
       "      <td>[0.8890885710716248]</td>\n",
       "      <td>-0.500832</td>\n",
       "      <td>0.747911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.886178</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>What is the ELI5 dataset and what is it used for?</td>\n",
       "      <td>There is no ELI5 dataset mentioned in the prov...</td>\n",
       "      <td>The ELI5 (Explain Like I'm 5) dataset contains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8362592458724976]</td>\n",
       "      <td>-1.149957</td>\n",
       "      <td>0.562324</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.768972</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>What is the ELI5 dataset and what is it used for?</td>\n",
       "      <td>The paper does not mention the ELI5 dataset. T...</td>\n",
       "      <td>The ELI5 (Explain Like I'm 5) dataset contains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8290041089057922]</td>\n",
       "      <td>-0.518310</td>\n",
       "      <td>0.349708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835151</td>\n",
       "      <td>chain-of-thought</td>\n",
       "      <td>What is the ELI5 dataset and what is it used for?</td>\n",
       "      <td>I'd be happy to help you understand the contex...</td>\n",
       "      <td>The ELI5 (Explain Like I'm 5) dataset contains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>[0.7882489562034607]</td>\n",
       "      <td>-1.019790</td>\n",
       "      <td>0.550364</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.305808</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>What is LLAMA and how does it differ from othe...</td>\n",
       "      <td>The text you provided appears to be a set of p...</td>\n",
       "      <td>LLAMA (Large Language Model Meta AI) is a fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8284688591957092]</td>\n",
       "      <td>-0.616609</td>\n",
       "      <td>0.424361</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.756505</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>What is LLAMA and how does it differ from othe...</td>\n",
       "      <td>Transformer architectures are a type of neural...</td>\n",
       "      <td>LLAMA (Large Language Model Meta AI) is a fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.061625</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>[0.8182438611984253]</td>\n",
       "      <td>-0.779397</td>\n",
       "      <td>0.563842</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.405714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.183125</td>\n",
       "      <td>chain-of-thought</td>\n",
       "      <td>What is LLAMA and how does it differ from othe...</td>\n",
       "      <td>The provided text appears to be a collection o...</td>\n",
       "      <td>LLAMA (Large Language Model Meta AI) is a fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.824767529964447]</td>\n",
       "      <td>-1.003538</td>\n",
       "      <td>0.479608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.765095</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>How is citation generation performed for medic...</td>\n",
       "      <td>The paper being referenced is likely \"How well...</td>\n",
       "      <td>Citation generation for medical data typically...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8433223962783813]</td>\n",
       "      <td>-0.840879</td>\n",
       "      <td>0.600894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.487631</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>How is citation generation performed for medic...</td>\n",
       "      <td>Based on the provided context, the answer is:\\...</td>\n",
       "      <td>Citation generation for medical data typically...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8085800409317017]</td>\n",
       "      <td>-0.966831</td>\n",
       "      <td>0.622820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902966</td>\n",
       "      <td>chain-of-thought</td>\n",
       "      <td>How is citation generation performed for medic...</td>\n",
       "      <td>I'll break down the steps to explain how citat...</td>\n",
       "      <td>Citation generation for medical data typically...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8593346476554871]</td>\n",
       "      <td>-1.234564</td>\n",
       "      <td>0.397606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620902</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>What kind of Questions does WebGPT answer?</td>\n",
       "      <td>WebGPT answers short-form questions, specifica...</td>\n",
       "      <td>Long-Form Questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8474259376525879]</td>\n",
       "      <td>-1.411070</td>\n",
       "      <td>0.473537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601496</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>What kind of Questions does WebGPT answer?</td>\n",
       "      <td>According to the text, WebGPT was primarily tr...</td>\n",
       "      <td>Long-Form Questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.8488590717315674]</td>\n",
       "      <td>-1.252160</td>\n",
       "      <td>0.409264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588182</td>\n",
       "      <td>chain-of-thought</td>\n",
       "      <td>What kind of Questions does WebGPT answer?</td>\n",
       "      <td>WebGPT answers a variety of questions, but bas...</td>\n",
       "      <td>Long-Form Questions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rouge      bleu            bert_score    bleurt  semantic_similarity  \\\n",
       "0   0.117647  0.000000   [0.838688850402832] -0.901380             0.560973   \n",
       "1   0.164384  0.000000  [0.8449322581291199] -0.906185             0.399810   \n",
       "2   0.119205  0.000000  [0.8427514433860779] -1.253345             0.699241   \n",
       "3   0.262626  0.098104  [0.8890885710716248] -0.500832             0.747911   \n",
       "4   0.126984  0.000000  [0.8362592458724976] -1.149957             0.562324   \n",
       "5   0.116279  0.000000  [0.8290041089057922] -0.518310             0.349708   \n",
       "6   0.058968  0.007305  [0.7882489562034607] -1.019790             0.550364   \n",
       "7   0.031088  0.000000  [0.8284688591957092] -0.616609             0.424361   \n",
       "8   0.061625  0.008247  [0.8182438611984253] -0.779397             0.563842   \n",
       "9   0.037037  0.000000   [0.824767529964447] -1.003538             0.479608   \n",
       "10  0.121739  0.000000  [0.8433223962783813] -0.840879             0.600894   \n",
       "11  0.075000  0.000000  [0.8085800409317017] -0.966831             0.622820   \n",
       "12  0.235294  0.000000  [0.8593346476554871] -1.234564             0.397606   \n",
       "13  0.206897  0.000000  [0.8474259376525879] -1.411070             0.473537   \n",
       "14  0.139535  0.000000  [0.8488590717315674] -1.252160             0.409264   \n",
       "\n",
       "    factual_consistency  context_utilization  recall_at_k  exact_match  \\\n",
       "0              1.000000             0.615385          0.0            0   \n",
       "1              1.000000             0.352941          0.0            0   \n",
       "2              0.714286             0.511905          0.0            0   \n",
       "3              1.000000             0.595745          0.0            0   \n",
       "4              0.333333             0.500000          0.0            0   \n",
       "5              1.000000             0.731183          0.0            0   \n",
       "6              0.857143             0.716000          0.0            0   \n",
       "7              0.173913             0.411765          0.0            0   \n",
       "8              0.153846             0.405714          0.0            0   \n",
       "9              1.000000             0.772727          0.0            0   \n",
       "10             1.000000             0.793103          0.0            0   \n",
       "11             1.000000             0.693694          0.0            0   \n",
       "12             1.000000             0.833333          1.0            0   \n",
       "13             1.000000             0.857143          1.0            0   \n",
       "14             1.000000             0.787879          1.0            0   \n",
       "\n",
       "     latency          strategy  \\\n",
       "0   2.764239         zero-shot   \n",
       "1   0.738377          few-shot   \n",
       "2   0.755626  chain-of-thought   \n",
       "3   0.886178         zero-shot   \n",
       "4   0.768972          few-shot   \n",
       "5   0.835151  chain-of-thought   \n",
       "6   1.305808         zero-shot   \n",
       "7   1.756505          few-shot   \n",
       "8   1.183125  chain-of-thought   \n",
       "9   0.765095         zero-shot   \n",
       "10  1.487631          few-shot   \n",
       "11  0.902966  chain-of-thought   \n",
       "12  0.620902         zero-shot   \n",
       "13  0.601496          few-shot   \n",
       "14  0.588182  chain-of-thought   \n",
       "\n",
       "                                             question  \\\n",
       "0             What is OpenScholar and how is it used?   \n",
       "1             What is OpenScholar and how is it used?   \n",
       "2             What is OpenScholar and how is it used?   \n",
       "3   What is the ELI5 dataset and what is it used for?   \n",
       "4   What is the ELI5 dataset and what is it used for?   \n",
       "5   What is the ELI5 dataset and what is it used for?   \n",
       "6   What is LLAMA and how does it differ from othe...   \n",
       "7   What is LLAMA and how does it differ from othe...   \n",
       "8   What is LLAMA and how does it differ from othe...   \n",
       "9   How is citation generation performed for medic...   \n",
       "10  How is citation generation performed for medic...   \n",
       "11  How is citation generation performed for medic...   \n",
       "12         What kind of Questions does WebGPT answer?   \n",
       "13         What kind of Questions does WebGPT answer?   \n",
       "14         What kind of Questions does WebGPT answer?   \n",
       "\n",
       "                                     generated_answer  \\\n",
       "0   I'll review the provided context and ask quest...   \n",
       "1   I've read the context, which appears to be a p...   \n",
       "2   I'd be happy to help you verify the informatio...   \n",
       "3   There is no ELI5 dataset mentioned in the prov...   \n",
       "4   The paper does not mention the ELI5 dataset. T...   \n",
       "5   I'd be happy to help you understand the contex...   \n",
       "6   The text you provided appears to be a set of p...   \n",
       "7   Transformer architectures are a type of neural...   \n",
       "8   The provided text appears to be a collection o...   \n",
       "9   The paper being referenced is likely \"How well...   \n",
       "10  Based on the provided context, the answer is:\\...   \n",
       "11  I'll break down the steps to explain how citat...   \n",
       "12  WebGPT answers short-form questions, specifica...   \n",
       "13  According to the text, WebGPT was primarily tr...   \n",
       "14  WebGPT answers a variety of questions, but bas...   \n",
       "\n",
       "                                            reference  \n",
       "0   OpenScholar is a research platform designed to...  \n",
       "1   OpenScholar is a research platform designed to...  \n",
       "2   OpenScholar is a research platform designed to...  \n",
       "3   The ELI5 (Explain Like I'm 5) dataset contains...  \n",
       "4   The ELI5 (Explain Like I'm 5) dataset contains...  \n",
       "5   The ELI5 (Explain Like I'm 5) dataset contains...  \n",
       "6   LLAMA (Large Language Model Meta AI) is a fami...  \n",
       "7   LLAMA (Large Language Model Meta AI) is a fami...  \n",
       "8   LLAMA (Large Language Model Meta AI) is a fami...  \n",
       "9   Citation generation for medical data typically...  \n",
       "10  Citation generation for medical data typically...  \n",
       "11  Citation generation for medical data typically...  \n",
       "12                                Long-Form Questions  \n",
       "13                                Long-Form Questions  \n",
       "14                                Long-Form Questions  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = EvaluationPipeline(get_config())\n",
    "batch_size = 3\n",
    "pdf_folder = r\"C:\\Users\\Usman\\Desktop\\For Thesis\\Citation_Generation\\Papers\"\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch = questions[i:i + batch_size]\n",
    "    results_df, stats = pipeline.run_evaluation(batch, pdf_folder)\n",
    "    \n",
    "    \n",
    "    all_results.append(results_df)\n",
    "    \n",
    "   \n",
    "    gc.collect()\n",
    "    print(f\"Batch {i // batch_size + 1} processed.\")\n",
    "\n",
    "\n",
    "final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"\\nAll Results Summary:\")\n",
    "display(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
