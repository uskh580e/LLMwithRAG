{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cde9e38-13f4-4245-95f9-1c657b870d96",
   "metadata": {},
   "source": [
    "LitSearch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0f4ec4-8a90-4ee0-9190-19fabd6d9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from typing import List, Any, Tuple\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from enum import Enum\n",
    "import openai\n",
    "import pickle\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from gritlm import GritLM\n",
    "import sentence_transformers\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c3e3f0-6db4-4454-a87a-86b2a4e41edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6e605e-4915-4fce-b86b-5a622fe2fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = query_data.to_pandas()\n",
    "corpus_clean_df = corpus_clean_data.to_pandas()\n",
    "corpus_s2orc_df = corpus_s2orc_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920bb9fd-87c2-410f-991b-7c2bf60f204d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_set</th>\n",
       "      <th>query</th>\n",
       "      <th>specificity</th>\n",
       "      <th>quality</th>\n",
       "      <th>corpusids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any research papers on methods to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[202719327]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any resources available for translat...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[227231792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any studies that explore post-hoc te...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[226254579, 204976362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any tools or studies that have focus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[10961392, 12160022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there papers that propose contextualized c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[233296182]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_set                                              query  specificity  \\\n",
       "0  inline_acl  Are there any research papers on methods to co...            0   \n",
       "1  inline_acl  Are there any resources available for translat...            1   \n",
       "2  inline_acl  Are there any studies that explore post-hoc te...            0   \n",
       "3  inline_acl  Are there any tools or studies that have focus...            1   \n",
       "4  inline_acl  Are there papers that propose contextualized c...            1   \n",
       "\n",
       "   quality               corpusids  \n",
       "0        2             [202719327]  \n",
       "1        2             [227231792]  \n",
       "2        2  [226254579, 204976362]  \n",
       "3        2    [10961392, 12160022]  \n",
       "4        2             [233296182]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ee025c-f62d-42e3-a785-50069dc24932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?\n"
     ]
    }
   ],
   "source": [
    "print(query_df.loc[0, \"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59bf2b6-ed06-44d2-a37d-549fb022e513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>citations</th>\n",
       "      <th>full_paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252715594</td>\n",
       "      <td>PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...</td>\n",
       "      <td>We present Phenaki, a model capable of realist...</td>\n",
       "      <td>[6628106, 174802916, 238582653]</td>\n",
       "      <td>PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13002849</td>\n",
       "      <td>MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS</td>\n",
       "      <td>Although Generative Adversarial Networks achie...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MODE REGULARIZED GENERATIVE ADVERSARIAL NETWOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239998253</td>\n",
       "      <td>What Do We Mean by Generalization in Federated...</td>\n",
       "      <td>Federated learning data is drawn from a distri...</td>\n",
       "      <td>[235613568, 231924480, 211678094, 195798643, 4...</td>\n",
       "      <td>What Do We Mean by Generalization in Federated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62841605</td>\n",
       "      <td>SPREADING VECTORS FOR SIMILARITY SEARCH</td>\n",
       "      <td>Discretizing multi-dimensional data distributi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>SPREADING VECTORS FOR SIMILARITY SEARCH\\n\\n\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253237531</td>\n",
       "      <td>MACHINE UNLEARNING OF FEDERATED CLUSTERS</td>\n",
       "      <td>Federated clustering (FC) is an unsupervised l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MACHINE UNLEARNING OF FEDERATED CLUSTERS\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpusid                                              title  \\\n",
       "0  252715594  PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...   \n",
       "1   13002849   MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS   \n",
       "2  239998253  What Do We Mean by Generalization in Federated...   \n",
       "3   62841605            SPREADING VECTORS FOR SIMILARITY SEARCH   \n",
       "4  253237531           MACHINE UNLEARNING OF FEDERATED CLUSTERS   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We present Phenaki, a model capable of realist...   \n",
       "1  Although Generative Adversarial Networks achie...   \n",
       "2  Federated learning data is drawn from a distri...   \n",
       "3  Discretizing multi-dimensional data distributi...   \n",
       "4  Federated clustering (FC) is an unsupervised l...   \n",
       "\n",
       "                                           citations  \\\n",
       "0                    [6628106, 174802916, 238582653]   \n",
       "1                                                 []   \n",
       "2  [235613568, 231924480, 211678094, 195798643, 4...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          full_paper  \n",
       "0  PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...  \n",
       "1  MODE REGULARIZED GENERATIVE ADVERSARIAL NETWOR...  \n",
       "2  What Do We Mean by Generalization in Federated...  \n",
       "3  SPREADING VECTORS FOR SIMILARITY SEARCH\\n\\n\\nA...  \n",
       "4  MACHINE UNLEARNING OF FEDERATED CLUSTERS\\n\\n\\n...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc0f9b1-fdaf-4128-aa6e-074a788cb40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>externalids</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252715594</td>\n",
       "      <td>{'acl': None, 'arxiv': '2210.02399', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":2761,\"st...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13002849</td>\n",
       "      <td>{'acl': None, 'arxiv': '1612.02136', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1726,\"st...</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239998253</td>\n",
       "      <td>{'acl': None, 'arxiv': None, 'dblp': 'conf/icl...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1082,\"st...</td>\n",
       "      <td>2022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62841605</td>\n",
       "      <td>{'acl': None, 'arxiv': '1806.03198', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1503,\"st...</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253237531</td>\n",
       "      <td>{'acl': None, 'arxiv': '2210.16424', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":2886,\"st...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpusid                                        externalids  \\\n",
       "0  252715594  {'acl': None, 'arxiv': '2210.02399', 'dblp': '...   \n",
       "1   13002849  {'acl': None, 'arxiv': '1612.02136', 'dblp': '...   \n",
       "2  239998253  {'acl': None, 'arxiv': None, 'dblp': 'conf/icl...   \n",
       "3   62841605  {'acl': None, 'arxiv': '1806.03198', 'dblp': '...   \n",
       "4  253237531  {'acl': None, 'arxiv': '2210.16424', 'dblp': '...   \n",
       "\n",
       "                                             content    year  \n",
       "0  {'annotations': {'abstract': '[{\"end\":2761,\"st...  2023.0  \n",
       "1  {'annotations': {'abstract': '[{\"end\":1726,\"st...  2017.0  \n",
       "2  {'annotations': {'abstract': '[{\"end\":1082,\"st...  2022.0  \n",
       "3  {'annotations': {'abstract': '[{\"end\":1503,\"st...  2019.0  \n",
       "4  {'annotations': {'abstract': '[{\"end\":2886,\"st...  2023.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_s2orc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f29b9-7ad0-421f-9d90-00cc695c7776",
   "metadata": {},
   "source": [
    "Q1: To find the number of queries with more than one corpusid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f785ef04-392e-41ac-8663-be3ec3a3c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpusids(df, column_name=\"corpusids\"):\n",
    "    return df[column_name].apply(lambda x: len(eval(x)) if isinstance(x, str) else len(x))\n",
    "\n",
    "query_df[\"corpusid_count\"] = count_corpusids(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede5d5be-2052-4e3e-984a-b1867e5d0eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_set</th>\n",
       "      <th>query</th>\n",
       "      <th>specificity</th>\n",
       "      <th>quality</th>\n",
       "      <th>corpusids</th>\n",
       "      <th>corpusid_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any research papers on methods to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[202719327]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any resources available for translat...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[227231792]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any studies that explore post-hoc te...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[226254579, 204976362]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any tools or studies that have focus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[10961392, 12160022]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there papers that propose contextualized c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[233296182]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_set                                              query  specificity  \\\n",
       "0  inline_acl  Are there any research papers on methods to co...            0   \n",
       "1  inline_acl  Are there any resources available for translat...            1   \n",
       "2  inline_acl  Are there any studies that explore post-hoc te...            0   \n",
       "3  inline_acl  Are there any tools or studies that have focus...            1   \n",
       "4  inline_acl  Are there papers that propose contextualized c...            1   \n",
       "\n",
       "   quality               corpusids  corpusid_count  \n",
       "0        2             [202719327]               1  \n",
       "1        2             [227231792]               1  \n",
       "2        2  [226254579, 204976362]               2  \n",
       "3        2    [10961392, 12160022]               2  \n",
       "4        2             [233296182]               1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0279c7f9-e941-48da-bc48-2b313101dcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corpusid_distribution = query_df[\"corpusid_count\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=corpusid_distribution.index, y=corpusid_distribution.values, color=\"royalblue\")\n",
    "plt.xlabel(\"#Corpus IDs per Query\")\n",
    "plt.ylabel(\"#Queries\")\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be181a69-4d82-4a23-afec-e8da06e3cd2c",
   "metadata": {},
   "source": [
    "Q2: Is the data already preprocessed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5b1eee-d6db-4fb0-86cf-ad24615c29e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n",
      "\n",
      "\n",
      "Ruben Villegas \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Mohammad Babaeizadeh \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Pieter-Jan Kindermans \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Hernan Moraldo hmoraldo@google.com \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Han Zhang zhanghan@google.com \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Mohammad Taghi \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Saffar Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Santiago Castro sacastro@umich.edu \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Julius Kunze \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Dumitru Erhan dumitru@google.com \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "Google Brain \n",
      "University of Michigan\n",
      "University College London\n",
      "\n",
      "\n",
      "PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS\n",
      "\n",
      "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "It is now possible to generate realistic high resolution images given a description [34,35,32,38,59], but generating high quality videos from text remains challenging. In essence, videos are just a sequence of images, but this does not mean that generating a long coherent video is easy. In practice, it is a significantly harder task because there is much less high quality data available and the computational requirements are much more severe [9]. For image generation, there are datasets with billions of image-text pairs (such as LAION-5B [41] and JFT4B [60]) while the text-video datasets are substantially smaller e.g. WebVid [4] with ∼10M videos, which is not enough given the higher complexity of open domain videos. As for computation, training current state-of-theart image generation models is already pushing the state-of-the-art computational capabilities [59], leaving little to no room for generating videos, particularly videos of variable length.\n",
      "\n",
      "To make the matters worse, one can argue that a single short text prompt is not sufficient to provide a complete description of a video (except for short clips), and instead, a generated video must be conditioned on a sequence of prompts, or a story, which narrates what happens over time. Ideally, Figure 1. Time variable text (i.e. story) conditional video generation. The entire figure is one continuous video generated auto-regressively. We start by generating the video conditioned on the first prompt and then after a couple of frames we change the prompt to the next one. Each row contains a selected number of frames (from left to right in order) while the model was conditioned on that particular prompt. The model manages to preserve the temporal coherence of the video while adapting to the new prompt, usually taking the shortest path for the adaption (notice the morphing of the teddy bear to the panda). Please note that the generated video has complex visual features such as reflections, occlusions, interactions and scene transitions. Full video is available at phenaki.github.io. a video generation model must be able to generate videos of arbitrary length, all the while having the capability of conditioning the generated frames at time t on prompts at time t that can vary over time. Such capability can clearly distinguish the video from a \"moving image\" and open up the way to real-world creative applications in art, design and content creation. To the best our knowledge, story based conditional video generation has never been explored before and this is the first paper to take early steps towards that goal. A traditional deep learning approach of simply learning this task from data is not possible, since there is no story-based dataset to learn from. Instead, to achieve this we rely on a model that is designed specifically with this capability in mind.\n",
      "\n",
      "In this paper, we introduce Phenaki, a text to video model trained on both text to video and text to image data that can:\n",
      "\n",
      "-Generate temporally coherent and diverse videos conditioned on open domain prompts even when the prompt is a new composition of concepts (Fig. 3). The videos can be long (minutes) even though the model is trained on 1.4 seconds videos (at 8 fps).\n",
      "\n",
      "-Generate videos conditioned on a story (i.e. a sequence of prompts), e.g. Fig. 1   The embeddings of images and video patches from raw frames x are processed by a spatial and then a causal transformer (auto-regressive in time) to generate video tokens z. Center: MaskGiT is trained to reconstruct masked tokens z predicted by a frozen C-ViViT encoder and conditioned on T5X tokens of a given prompt p 0 . Right: How Phenaki can generate arbitrary long videos by freezing the past token and generating the future tokens. The prompt can change over time to enable time-variable prompt (i.e. story) conditional generation. The subscripts represent time (i.e. frame number).\n",
      "\n",
      "To enable these capabilities, we could not rely on current video encoders, because they either can only decode fixed size videos or they encode frames independently. Hence, we introduce C-ViViT , a novel encoder-decoder architecture that:\n",
      "\n",
      "-Exploits temporal redundancy in videos to improve reconstruction quality over a per frame model while compressing the number of video tokens by 40% or more.\n",
      "\n",
      "-Allows encoding and decoding of variable length videos given its causal structure.\n",
      "\n",
      "\n",
      "THE PHENAKI MODEL\n",
      "\n",
      "Inspired by the previous work in auto-regressive text to image [34,59,38] and text to video [54,53,18], Phenaki is designed with two main components (see Figure 2): an encoder-decoder model which compresses videos to discrete embeddings (i.e. tokens) and a transformer model to translate text embeddings to video tokens. To get the text embeddings, Phenaki uses a pre-trained language model, T5X [37]. We will discuss each one of these components in the following subsections.\n",
      "\n",
      "\n",
      "ENCODER-DECODER VIDEO MODEL: C-VIVIT\n",
      "\n",
      "One of the primary challenges for generating video from text, is to get a compressed representation of videos. Previous work on text to video either use per-frame image encoders [18,54,57] such as VQ-GAN [12] or fixed length video encoders [52] such as VideoVQVAE [49]. The former allows for generating videos of arbitrary length, however in practice, the videos have to be short because the encoder does not compress the videos in time and the tokens are highly redundant in consecutive frames. The latter is more efficient in the number of tokens but it does not allow to generate variable length videos. In Phenaki, our goal is to generate videos of variable length while keeping the number of video tokens to a minimum so they can be modeled with a transformer within current computational limitations. To do so, we introduce C-ViViT , a causal variation of ViViT [1] with additional architectural changes for video generation, which can compress the videos in temporal and spatial dimensions, while staying auto-regressive in time, This capability allows for generating videos of arbitrary length auto-regressively.\n",
      "\n",
      "Encoder architecture: As illustrated in Figure 2, we start with a video sequence of t x + 1 frames with a resolution of w x × h x and c x channels: x ∈ R (tx+1)×hx×wx×cx . This sequence will be compressed into a token representation of size (t z + 1) × w z × h z where the first w z × h z tokens represent the first frame independently from the rest of the video, and the remaining tokens represent spatio-temporal video tokens that auto-regressively depend on previous frames. To do so, we extract non-overlapping image patches of size w p × h p × c p from the first frame and video patches of size t p × w p × h p × c p from the rest of the video. We typically use all channels at once such that the number of patches equals the number of video tokens t z = tx tp , w z = wx wp and h z = hx hp . Each of these patches is flattened and linearly projected into a d z dimensional space. We combine the spatial dimensions to have a tensor of shape (t z +1)×w z * h z ×d z where the spatial and temporal dimensions are separated. Then multiple transformer layers are applied along the spatial dimensions with allto-all attention. This is followed by multiple transformer layers over the temporal dimension with causal attention such that each spatial token only observes spatial tokens from previous frames in an auto-regressive manner. The effect of this is that the first frame can be completely independently encoded. This opens up the possibility of text to image training to be embedded naturally into our video model. The second advantage is that we can condition the video generation process on a number of starting frames. The resulting patch embeddings z of shape t z × w z × h z × d z are then tokenized into learned codewords c z by vector quantization. The codebook learning will be discussed later together with the losses.\n",
      "\n",
      "Decoder architecture: The C-ViViT decoder is simply an upside down version of the encoder. First tokens are transformed into embeddings. This is followed by the temporal transformer, then the spatial transformer. After the output of the spatial transformer, we apply a single linear projection without activation to map the tokens back to pixel space.\n",
      "\n",
      "\n",
      "Quantization and Losses:\n",
      "\n",
      "To learn a discrete latent space, we quantize our encoder outputs into the entries of a learned codebook via the vector quantization (VQ) objective in VQVAEs [45],\n",
      "L VQ = sg(z) − e 2 2 + β z − sg(e) 2 2 ,(1)\n",
      "where sg(x) ≡ x, and d dx sg(x) ≡ 0 is the stop-gradient operator, β is the commitment loss weight, and e is a codebook vector from codebook E. The index to the codebook vector closest to z is found by i = argmin j z − E j 2 2 . In addition to the VQ objective, we adopt the factorized and 2normalized codes from ViT-VQGAN [58] to improve codebook usage and reconstruction quality.\n",
      "\n",
      "To train our model, we use a combination of L 2 loss, image perceptual loss L IP [20,61], video perceptual loss L VP by using the I3D network [6] as feature extractor, and adversarial loss L Adv with StyleGAN architecture [21]. As training objective, we use the following\n",
      "L = L VQ + 0.1 × L Adv + 0.1 × L IP + 1.0 × L VP + 1.0 × L 2 .(2)\n",
      "Novelty over the ViViT architecture: While our proposed C-ViViT architecture is inspired by the factorized encoder in ViViT [1], we modify their architecture to enable self-supervised learning from unlabeled videos. We first remove the [CLS] tokens in the spatial and the temporal transformers. Next, we apply temporal transformer for all spatial tokens computed by the spatial encoder, in contrast to single run of the temporal transformer over the [CLS] tokens in ViViT. Most importantly, the ViViT encoder requires a fixed length video input due to the all-to-all attention in time. Therefore, we apply causal attention instead such that our C-ViViT encoder becomes autoregressive and allows for a variable number of input frames which are necessary to learn from image datasets, and auto-regressively extrapolate video or single frames into the future.\n",
      "\n",
      "\n",
      "TEXT-TO-VIDEO GENERATION WITH BIDIRECTIONAL TRANSFORMERS\n",
      "\n",
      "In this stage, the text-to-video task can be formulated as a sequence-to-sequence problem to predict video tokens given the paired text embeddings. Most of recent methods [34,59,54,18] adopt a transformer model for these sequence-to-sequence tasks. In their models, they use an auto-regressive transformer which predicts the image or video tokens sequentially given the encoded text features. As a result, the sampling time scales linearly with the sequence length, even when caching is used. This becomes impractical for long video sequence generation.\n",
      "\n",
      "\n",
      "Masked bidirectional transformer:\n",
      "\n",
      "In this work, we aim to reduce the sampling time by having a small and fixed sampling step disregarding different video sequence lengths. Inspired by previous work for image generation [8], we use a bidirectional transformer since it can predict different video tokens simultaneously. For training step i, we first sample a mask ratio γ i from 0 to 1 and randomly replace γ i · N tokens with the special token [MASK], where N is the video sequence length. Then we learn the model parameters by minimizing the cross entropy loss on those masked tokens given the encoded text embeddings and unmasked video tokens. During inference, we first label all of the video tokens as the special token [MASK]. Then, at each inference step, we predict all the masked (unknown) video tokens in parallel conditioned on the text embeddings and unmasked (predicted) video tokens. We keep a ratio β i of the predicted tokens at sampling step i and the remaining tokens are re-masked and re-predicted in the next step.\n",
      "\n",
      "As discussed in MaskGIT [8], the masking schedule γ i and sampling schedule β i have a significant effect on the samples quality therefore we follow the same strategies. Compared to an autoregressive transformer, the number of sampling steps is an order-of-magnitude smaller (typically we use values in the range of 12 to 48). Generally speaking, more sampling steps improves the quality.\n",
      "\n",
      "Losses and training strategies: Given a pre-trained C-ViViT , videos are encoded into codebook ids a of shape (t z + 1) × w z × h z which are flattened into a long vector using the raster ordering from [58]. We then model the text-conditional video token distribution using Masked Visual Token Modeling (MVTM) [8]:\n",
      "L mask = − ∀i∈[1,N ],mi=1 log p(a i |aM , p),(3)\n",
      "where aM represents the masked version of a, m i is a binary variable indicating whether a i is masked or not, N is the number of video tokens, and p is the text condition embedding. In addition to the MVTM objective, we train using classifier-free guidance by dropping the text condition 10% of the time during training [16,59] . Finally, we dynamically adjust the MVTM objective during training to allow the use of image and video datasets as a single large dataset. We achieve this by only applying the masking ratio and objective on the first w z × h z tokens if only a single frame is given or over all video tokens if a full video is given. This mixed image and video dataset training strategy allows our models to learn concepts only present in image datasets, and transfer them to concepts present video datasets (e.g., the pencil drawing styled video of the panda in Figure.3).\n",
      "\n",
      "Inference and auto-regressive generation of long videos: At inference time, we sample videos tokens by the same iterative process used in [8] with classifier-free guidance scale λ to control alignment between the generation and the text condition. Once the first video is generated, we can extrapolate additional frames auto-regressively by encoding the last K generated frames in the last video using C-ViViT , initializing MaskGIT with the tokens computed by our C-ViViT encoder, and proceed to generate the remaining video tokens conditioned on a text input. During video extrapolation, the text condition can be the same or a different one which enables our model to dynamically create visual transitions between the previous and current text condition visual content, effective generating a visual story an described by the input text.\n",
      "\n",
      "\n",
      "EXPERIMENTS\n",
      "\n",
      "To evaluate Phenaki, we test it on the following tasks: 1) text conditional video generation, 2) textimage conditional video generation, 3) time variable text conditional video generation (i.e.) story mode, 4) video quantization and 5) image conditional video generation a.k.a. video prediction.\n",
      "\n",
      "To the best of our knowledge, 3) time variable text conditional video generation has not been explored in prior work. Given the dynamic nature of videos, we highly encourage readers to visit phenaki.github.io to check the generated videos. The website also includes qualitative comparisons to a subset of the prompts from the CogVideo paper [18]. While the focus is on the text to video generation tasks, it is remarkable that Phenaki is still competitive on the more traditional video tasks despite not being developed explicitly for these tasks. We implemented Phenaki in JAX [? ] using FLAX [? ] library. \n",
      "\n",
      "\n",
      "TEXT CONDITIONAL VIDEO GENERATION\n",
      "\n",
      "Currently there is no established benchmark for evaluating text to video methods. This makes comparing Phenaki to recent methods such as NUWA [54], CogVideo [18], NUWA-Infinity [53] and video diffusion models [17] difficult.\n",
      "\n",
      "Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came from the video dataset and each image dataset contributed 10%.\n",
      "\n",
      "Qualitative evaluation: Samples from this model can be seen in Figure 3 and additional samples are provided at phenaki.github.io. We observe that there is a high degree of control over both the actors and the background dynamics in the videos. The appearance of the actors and the video style can be adjusted by the text prompt as well (e.g. a regular video, a cartoon or a pencil drawing).\n",
      "\n",
      "On phenaki.github.io we provide examples from prompts that were provided in the CogVideo [18] demo. Since there are substantial differences between these methods it is hard to compare them on an equal footing. As an example, there are massive differences in scale: 9B parameters for CogVideo and 1.8B for our model. Additionally, the training data is different. Finally, we do not know how representative the prompts in the CogVideo demo are for the general performance of the CogVideo.\n",
      "\n",
      "Quantative comparison: The NUWA [54] paper provided a qualitative evaluation on Kinetics-400. Since the NUWA model is only 0.9B parameters we also use a model of the same size. Our model was trained on 50% video and 50% image data in this experiment. The NUWA model finetuned on Kinetics but the Phenaki model is not: it is evaluated in a zero shot setting. The results in Table 1 show that Phenaki achieves comparable generation quality, in a zero-shot setting, compared to previous text to video methods that were actually trained or finetuned on this dataset.\n",
      "\n",
      "On the importance of joint text-to-image and text-to-video training While there are some textvideo datasets, text-image datasets dominate the internet in terms of quality and quantity [30]. Consequently, there is simply not enough video data available to cover all the concepts present in textimage datasets. For example using only our video data, concepts such as pencil drawings or different painting styles cannot be learned. To be able to learn a model that can combine video dynamics with these additional concepts we have to combine training on image and video data. In Table 2, we evaluate the performance of using different ratios of video and images. We start with data splits of only video, and vary the ratio of image and video datasets up to using 50% image and 50% video datasets. In our results, we find that there is a trade-off in performance between models trained with only video video (i.e., significantly better FVD), and models trained with more image data (i.e., better text-video and text-image alignment, and significantly better FID in image datasets). On phenaki.github.io we show samples from different models side by side where this trade-off between control over the content and the quality of the dynamics can be seen. We believe that the tradeoff between concepts and dynamics will be improved as the quality and size of text-video datasets increases in the future.\n",
      "\n",
      "\n",
      "TEXT-IMAGE CONDITIONAL VIDEO GENERATION\n",
      "\n",
      "Given that Phenaki can be conditioned on both still images and text, an interesting setup is to animate existing images given a text prompt. For this experiment, we use the same model from Section 3.1 but conditioned on unseen pictures (captured with our phones from local subjects) and a related prompt. As it can be seen in Figure 4 the model can generate coherent videos starting from the given images, while following the given prompts. \n",
      "\n",
      "\n",
      "VISUAL STORY TELLING BY DYNAMIC TEXT INPUTS\n",
      "\n",
      "A notable and useful feature of Phenaki is that it is auto-regressive in time. This allows for generating long videos, while the prompt changes over time. Time variable prompts can be thought of as a story; a narration of the entire video where each prompt corresponds to a scene from the video. This allows for creating dynamically changing scenes. To the best our knowledge, this paper is the first work to generate such videos. An example of this can be seen in Fig. 1 and on phenaki.github.io. The way it works is that we generate a video with the first prompt and then extend it in time by conditioning a possibly new prompt and on the last N , typically 5, previously generated frames.\n",
      "\n",
      "\n",
      "VIDEO ENCODING\n",
      "\n",
      "To evaluate the video encoding and reconstruction performance of C-ViViT , we use the Momentsin-Time (MiT) [29] dataset. MiT contains ∼802K training, ∼33K validation and ∼67K test videos at 25 FPS. The MiT dataset, in contrast to other publicly available video datasets, is a high quality balanced dataset with high coverage and density of verbs depicting moments of a few seconds [29]. We compare C-ViViT against per-frame image based encoder-decoders that have been used as video quantizers for conditional video generation [57,54,18,54,18,52]: a ViT [58] and a convolutional VQ-GAN [12]. The experimental details can be found in the Appendix B.1.  As demonstrated in Table 3, we evaluate the video reconstruction quality using FID [15] and FVD [44]. Both FID and FVD compare the distribution of generated videos (or images) to the ground truth distribution. The FID ignores temporal coherency, while the FVD measures how well the spatio-temporal dynamics of the videos are reconstructed. Results in Table 3 show that perframe image based methods slightly outperform our video method (indicated by marginally higher FID of C-ViViT ), however, they do poorly at modeling the spatio-temporal dynamics in video (significantly lower FVD of C-ViViT ). This is expected as C-ViViT has spatio-temporal connections between patches in each frame, allowing space and time to be modeled together. In addition, C-ViViT compresses the video into fewer tokens per video compared to the image based baselines. This is crucial as the number of tokens drastically impacts the computational cost of the transformer in downstream tasks. Furthermore, C-ViViT tokens are auto-regressive in time which enables variable length videos to be modeled with the same encoder which is important for video extrapolation conditioned on previously generated frames.\n",
      "\n",
      "\n",
      "IMAGE CONDITIONAL VIDEO GENERATION A.K.A VIDEO PREDICTION\n",
      "\n",
      "To evaluate the learnt video representation of C-ViViT beyond reconstruction, we test it on the task of frame-conditioned video generation, also commonly known as video prediction [3]. In this experiment, we test Phenaki on BAIR Robot Pushing benchmark [11] where the task is to generate 15 frames conditioned on a given single frame. For open domain videos, we test Phenaki on Kinetics-600 [7] where the task is to predict 11 frames given 5 frames. More details about these experiments can be found in Appendix B.2. Tables 4 and 5 show the results of these experiments. Note that Table 4. Video prediction on Kinetics-600 [7]. While\n",
      "\n",
      "Phenaki is not designed for video prediction it achieves comparable results with SOTA video prediction models.\n",
      "\n",
      "Method FVD ↓ Video Transformer [51] 170.0 ± 5.00 CogVideo [18] 109.2 DVD-GAN-FP [9] 69.1 ± 0.78 Video VQ-VAE [49] 64.3 ± 2.04 CCVS [28] 55.0 ± 1.00 TrIVD-GAN-FP [27] 25.7 ± 0.66 Transframer [31] 25.4 RaMViD [19] 16.5 Video Diffusion [17] 16.2 ± 0.34 Phenaki (Ours) 36.4 ± 0.19 Table 5. Video prediction on BAIR [11].\n",
      "\n",
      "Method FVD ↓ DVD-GAN [9] 109.8 VideoGPT [55] 103.3 TrIVD-GAN [27] 103.3 Transframer [31] 100.0 HARP [57] 99.3 CCVS [28] 99.0 Video Transformer [51] 94.0 FitVid [3] 93.6 MCVD [47] 89.5 NUWA [54] 86.9 RaMViD [19] 84.2 Phenaki (Ours) 97.0\n",
      "\n",
      "Phenaki is not specifically designed for video prediction, therefore, it lacks components such as skip connections in U-Nets which are known to improve the performance for video prediction methods [10,46,3]. Nevertheless, our method is competitive on these benchmarks with SOTA video prediction methods. Overall, these experiments show that Phenaki is strong at modeling dynamics of the videos which is required for generating coherent videos from text.\n",
      "\n",
      "\n",
      "RELATED WORKS\n",
      "\n",
      "This paper is closely related to auto-regressive methods for text conditioned image and video generation. DALL-E [34] translates text tokens to discrete image embeddings learnt using a VQVAE [45]. Parti [59] has a similar architecture but can generate higher quality images by predicting tokens from a ViT-VQGAN [58] using a 21B parameters transformer. Similar architectures have been used for generating videos as well. GODIVA [52] uses a transformer to map text tokens to video tokens from a image based VQVAE. Given the large number of tokens from multiple frames, GODIVA relied on a local-attention mechanism. Similarly, NUWA [54] and NUWA-Infinity [53] both employ auto-regressive architectures to generate videos and images from text. NUWA generates fixed size outputs, while NUWA-Infinity introduces a second layer of auto-regressive computation to support variable size videos. Likewise, CogVideo [18] argues the main reason behind low quality video generation is the scarcity of good text-video data and tried to leverage pre-trained text to images models to generate high quality video.\n",
      "\n",
      "While Phenaki sticks to the same architecture principles, it has major differences with previous work. Most notably, NUWA, NUWA-Infinity and CogVideo treat videos as a sequence of independent images. This can lead to poor modeling of dynamics and generate motion artifacts. To combat this, NUWA-infinity used the previous frame during decoding to combat this. In Phenaki, we go further and treat videos as a temporal sequence of images which substantially decreases the number of video tokens given the redundancy in video generation, and results in a much lower training cost. The auto-regressive nature of the Phenaki also allows us to effectively condition on previous frames and generates longer videos as detailed in Section 2.\n",
      "\n",
      "Diffusion models are another class of models which recently have been used for conditional and unconditional video generation, which we call VDM [17]. In VDM, authors proposed replacing the conventional U-Net architectures for 2D image modeling with a 3D space-time model to run the diffusion process directly on pixels. While this approach provides an effective formulation for modeling videos, it is limited to fixed size videos. To address this issue, VDM provides an autoregressive extension, which allows the model to generate longer videos but it is typically impractical due to high sampling time of diffusion models.\n",
      "\n",
      "Text conditional video generation is a relatively new field of research, nonetheless, image conditional video generation, commonly known as video prediction, and unconditional video generation have been studied more comprehensively. These papers include deterministic methods using a combination of recurrent and convolutional networks [36,42,13,50], variational based stochastic methods [2,10,46,3] and more recently by learning a discrete representation [49,33,31], auto-regressive models [51,55,28,57], diffusion models [47,14,56,19] flow based models [24], and finally adversarial based methods [48,39,43,9,40,27]. These works mostly consider limited domain (e.g. robotic videos) prediction/generation, or short fixed size clips. Section 3 provides comparison with some of these models.\n",
      "\n",
      "\n",
      "CONCLUSION\n",
      "\n",
      "We introduced Phenaki, a model which is capable of generating variable length videos conditioned on a sequence of open domain text prompts. Phenaki uses C-ViViT as video encoder. C-ViViT is a new model which provides temporal-spatial compression while being auto-regressive in time. The C-ViViT model is a crucial part of Phenaki that allows it to generate variable length videos. We demonstrate how joint training on images and videos can improve the generation quality, and diversity, given the existence of much larger image-text dataset with order of magnitude more samples. The Phenaki model achieves good performance on video prediction, it can be used as to generate long videos conditioned on a text prompt. Additionally it is able to condition on both text and a starting frame. Finally, Phenaki is not limited to generating a video depicting a single concept or caption. It is actually able to generate longer coherent video stories based on a sequence of text prompts. The more complex narratives it can visualize demonstrate how this can become a great creative tool for story telling.\n",
      "\n",
      "\n",
      "ETHICS STATEMENT\n",
      "\n",
      "While we have not explored potential downstream applications of the generative models described in this work, we believe Phenaki can have a positive impact in a variety of creative settings. In general, many of the samples from the model will not perfectly correspond to the input caption or the user's intent; however, the end-user is likely to gain considerable time savings even if only one of the generated samples aligns with their intent. We thus foresee Phenaki being useful in eventually empowering users to accelerate their creativity, especially since the model can so quickly generate videos. Phenaki and similar models will be part of an ever-broad toolset for artists and non-artists alike, providing new and exciting ways to express creativity.\n",
      "\n",
      "The flip-side of this acceleration and ease-of-use is the potential for harmful impact, as with many of the prior or concurrent work in generative modeling. An easy-to-use system like Phenaki can be repurposed for generating maliciously fake content and enable spreading of such content much easier. While the quality of the videos generated by Phenaki is not yet indistinguishable from real videos, getting to that bar for a specific set of samples is within the realm of possibility, even today. This can be particularly harmful if Phenaki is to be used to generate videos of someone without their consent and knowledge.\n",
      "\n",
      "Like DALLE-2 [35], Imagen [38], Parti [59] and others, Phenaki is trained on a collection of datasets that is known to encode a number of undesirable biases. LAION-400M [41] specifically has a variety of issues regarding violence, pornography, gore. While our primary image and video datasets have minimal traits like this, we did incorporate LAION-400M into our training and observed better results. In a currently training version of Phenaki, we use a set of datasets that minimizes such problems.\n",
      "\n",
      "Taken together, these issues contribute to our decision not to release the underlying models, code, data or interactive demo at this time. Before we can do that, we want to focus our efforts on better understanding of data, prompt and output filtering. We would also like to more explicitly measure the biases encoded in the outputs of Phenaki, so that we can further mitigate them actively, either in the data, models or pre/post-processing steps.\n",
      "\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "We would like to thank Niki Parmar for initial discussions. Special thanks to Gabriel Bender and Thang Luong for reviewing the paper and providing constructive feedback. We appreciate the efforts of Kevin Murphy and David Fleet for advising the project and providing feedback throughout. We are grateful to Evan Rapoport, Douglas Eck and Zoubin Ghahramani for supporting this work in a variety of ways. The decoder architecture for all models is the same as the encoder but in reverse to put the latent embeddings back to image space. The VQ objective is trained with commitment loss of β = 0.25 and codebook size of 8192. The discriminator architecture is the StyleGAN [21] discriminator with blur resample, and channel multiplier of 1.\n",
      "\n",
      "\n",
      "B.1.2 TRAINING\n",
      "\n",
      "We train all encoder-decoder baselines and with StyleGAN [21] discriminators with a batch size of 128 using Adam optimizer [23] with β 1 = 0.9 and β 2 = 0.99. We use a linear learning rate warmup to a peak value of 1 × 10 −4 over 100, 000 steps and then decaying over the remaining 900, 000 steps with a cosine schedule, and use a decoupled weight decay [26]  We use a similar setup as in Section B.1, but the video tokenization step is done over 4 × 4 spatial patches on the first image and 2 × 4 × 4 spatio-temporal patches in the rest of the video. The spatial encoder consists of 8 layers and the temporal encoder consists of 6 layers.\n",
      "\n",
      "\n",
      "B.2.2 KINETICS-600 C-VIVIT ARCHITECTURE\n",
      "\n",
      "We use a similar setup as in Section B.2.1, but both the spatial encoder and temporal encoder consist of 8 layers.\n",
      "\n",
      "\n",
      "B.2.3 MASKGIT ARCHITECTURE\n",
      "\n",
      "To perform video prediction in latent space in the BAIR Robot Push and Kinetics-600 datasets, we use an unconditional transformer architecture consisting of 24 layers, 768 hidden units, 16 attention heads, dropout and attention dropout rate of 0.1, 3072 mlp hidden units.\n",
      "\n",
      "\n",
      "B.2.4 TRAINING AND INFERENCE\n",
      "\n",
      "As described in Table 7, we train C-ViViT with the same optimizer setup as in Sec B.1, but we do not downsample the FPS of any of the datasets in this section for fair comparison with the video prediction baselines. We train MaskGIT on the video tokens extracted using C-ViViT in an unconditional setting, that is, we do not assume frames or text inputs to be given. During training, we use the Adam [23] optimizer with β 1 = 0.9 and β 2 = 0.99. We use a linear learning rate warmup up to a peak value of 1 × 10 −4 over 10, 000 steps, and constant learning rate schedule for ∼2M steps. At inference time, we initialize MaskGIT given a number of input frames, and predict the rest of the frames depending on the dataset on which we evaluate.\n",
      "\n",
      "\n",
      "B.3 TEXT CONDITIONAL VIDEO GENERATION\n",
      "\n",
      "\n",
      "B.3.1 ARCHITECTURE\n",
      "\n",
      "In our text conditional video generation, we use the same C-ViViT architecture and training described in Section B.1. To train MaskGIT, we include a text conditioning in the form of T5X embeddings [37] which are used as input through the use of cross attention with the video tokens. We reduce the number of parameters of our base model for fairness in the quantitative comparisons against NUWA. We use λ = 12, 48 MaskGIT iterations, and temperature of 8.0. \n",
      "\n",
      "Figure 2 .\n",
      "2The architecture of Phenaki. Left: C-ViViT encoder architecture.\n",
      "\n",
      "Figure 3 .\n",
      "3Text conditional video generation. Each row shows selected frames from a video generated given the prompt. The model is trained on a mix of images and videos. The video dataset does not include any stylized videos such as pencil drawings, however, the image dataset does. The model can generalize from still images to videos. This figure also demonstrate the capability of the model in generating new unseen compositions. Full videos are available at phenaki.github.io.\n",
      "\n",
      "Figure 4 .\n",
      "4Animating images conditioned on a prompt. Each row demonstrates multiple frames of a generated video conditioned on a given first frame as well as a given text prompt. The first frames are new (captured by author's phone) and not observed during the training. The model animates the given image while following the prompt. Full videos are available at phenaki.github.io.\n",
      "\n",
      "Figure 5 .\n",
      "5Another example of story conditional video generation. Full videos are available at phenaki.github.io.\n",
      "\n",
      "\n",
      "and Fig. 5.Empty \n",
      "Tokens \n",
      "\n",
      "Tokens \n",
      "\n",
      "Patch \n",
      "Emb \n",
      "\n",
      "Patch \n",
      "Emb \n",
      "\n",
      "Patch \n",
      "Emb \n",
      "\n",
      "Spatial \n",
      "Transformer \n",
      "\n",
      "Spatial \n",
      "Transformer \n",
      "\n",
      "Spatial \n",
      "Transformer \n",
      "\n",
      "Causal \n",
      "Transformer \n",
      "\n",
      "Causal \n",
      "Transformer \n",
      "\n",
      "Causal \n",
      "Transformer \n",
      "\n",
      "... \n",
      "\n",
      "... \n",
      "\n",
      "... \n",
      "\n",
      "... \n",
      "\n",
      "C-ViViT \n",
      "Encoder \n",
      "T5X \n",
      "\n",
      "... \n",
      "\n",
      "Transformer \n",
      "\n",
      "Random Masking \n",
      "\n",
      "... \n",
      "\n",
      "... \n",
      "\n",
      "Video \n",
      "\n",
      "Tokens \n",
      "\n",
      "Tokens \n",
      "\n",
      "Masked \n",
      "\n",
      "Reconstructed \n",
      "\n",
      "... \n",
      "\n",
      "Transformer \n",
      "\n",
      "... \n",
      "\n",
      "Shift Time \n",
      "\n",
      "... \n",
      "... \n",
      "\n",
      "Transformer \n",
      "\n",
      "... \n",
      "\n",
      "T5X \n",
      "\n",
      "T5X \n",
      "\n",
      "... \n",
      "\n",
      "\"Next Prompt\" \n",
      "\n",
      "Tokens \n",
      "\n",
      "Tokens \n",
      "\n",
      "Predicted \n",
      "\n",
      "Frozne Past \n",
      "\n",
      "Predicted \n",
      "\n",
      "Future Tokens \n",
      "\n",
      "C-ViViT Encoder \n",
      "Training Transformer \n",
      "Video Generation \n",
      "\n",
      "Token \n",
      "Masked/Empty \n",
      "Token \n",
      "Transformer \n",
      "Frozen Model \n",
      "Linear \n",
      "Embedding \n",
      "Operation \n",
      "\n",
      "\"1st Prompt\" \n",
      "\n",
      "\"Prompt\" \n",
      "\n",
      "Discretize \n",
      "Discretize \n",
      "Discretize \n",
      "\n",
      "... \n",
      "\n",
      "\n",
      "\n",
      "Table 1 .\n",
      "1Text to video comparisons on Kinetics-400 [22].Table 2. Text to video and text to image results highlighting the importance of image datasets in video models. Text-to-image evaluation is done on ∼40K images of LAION-400M [41]. Data Split Text to Video Text to Image Vid% / Img% CLIP ↑ FID ↓ FVD ↓ CLIP ↑ FID ↓ 100% / 0% 0.298 19.2 168.9 0.240 53.9 80% / 20% 0.303 21.4 198.4 0.289 29.4 50% / 50% 0.302 21.4 239.7 0.287 30.5Method \n",
      "FID \n",
      "Image \n",
      "↓ \n",
      "FID \n",
      "Video \n",
      "↓ \n",
      "\n",
      "T2V [25] \n",
      "82.13 14.65 \n",
      "SC [5] \n",
      "33.51 \n",
      "7.34 \n",
      "TFGAN [5] \n",
      "31.76 \n",
      "7.19 \n",
      "NUWA \n",
      "28.46 \n",
      "7.05 \n",
      "Phenaki [0-Shot] 37.74 \n",
      "3.84 \n",
      "\n",
      "\n",
      "\n",
      "Table 3 .\n",
      "3Video reconstruction results on Moments-in-Time. The number of tokens is computed for 10 frames with the exception of C-ViViT which is for 11, due to the isolated initial frame.Method \n",
      "FID ↓ FVD ↓ Number of Tokens ↓ \n",
      "Conv VQ-GAN [12] \n",
      "7.5 \n",
      "306.1 \n",
      "2560 \n",
      "Conv VQ-GAN + Video loss \n",
      "13.7 \n",
      "346.5 \n",
      "2560 \n",
      "ViT VQ-GAN [58] \n",
      "3.4 \n",
      "166.6 \n",
      "2560 \n",
      "ViT VQ-GAN + Video loss \n",
      "3.8 \n",
      "173.1 \n",
      "2560 \n",
      "C-ViViT VQ-GAN (Ours) \n",
      "4.5 \n",
      "65.78 \n",
      "1536 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tim Salimans and Chitwan Saharia helped us with brainstorming and coming up with shared benchmarks. Jason Baldridge was instrumental for bouncing ideas. Alex Rizkowsky was very helpful in keeping things organized, while Erica Moreira and Victor Gomes ensured smooth resourcing for the project. Sarah Laszlo and Kathy Meier-Hellstern have greatly helped us incorporate important responsible AI practices into this project, which we are immensely grateful for. Finally, Blake Hechtman and Anselm Levskaya were generous in helping us debug a number of JAX issues.A HYPER-PARAMETERS Symbol Value Description t x , w x , h x , c x 11, 128, 128, 3 Video dimensions t p , w p , h p , c pTable 6. Hyperparamters used for C-ViViT architecture and optimizer.Table 7. Hyperparamters used for MaskGIT architecture and optimizer. B DETAILS OF EXPERIMENTS B.1 VIDEO QUANTIZATION B.1.1 NETWORK ARCHITECTURE All encoder-decoder baselines have approximately 50M parameters. The Convolutional baseline encoder architecture consists of 5 convolutional blocks with channel multipliers of [1, 1, 2, 2, 4], 2 residual layers and 128 hidden units per block, and embedding dimension of 256. The ViT baseline encoder architecture consists of an image patchification step over non-overlapping 8 × 8 spatial patches which are linearly transformed into image tokens. Next, we follow with 8 transformer layers with 512 hidden units, 8 attention heads, 2048 mlp units, and embedding dimension of 32. C-ViViT encoder architecture patches the first frame to non-overlapping 8 × 8 patches, and then the rest of the frames to non-overlapping 2 × 8 × 8 spatio-temporal patches which are linearly transformed into video embeddings. Next, C-ViViT encoder architecture consists of 4 spatial and 4 temporal transformer layers with 512 hidden units, 8 attention heads, 2048 mlp hidden units, and embedding dimension of 32.2, 8, 8, 3 \n",
      "Patches dimensions (all frames except the first one) \n",
      "t z , w z , h z \n",
      "6, 16, 16 \n",
      "Video tokens dimension (before linear projection) \n",
      "h z \n",
      "512 \n",
      "Hidden size in the transformer layer \n",
      "d z \n",
      "32 \n",
      "Embedding dimension (after linear projection) \n",
      "− \n",
      "4 \n",
      "Number of layers for spatial transformer \n",
      "− \n",
      "4 \n",
      "Number of layers for temporal transformer \n",
      "− \n",
      "2048 \n",
      "MLP size \n",
      "|E| \n",
      "8192 \n",
      "Codebook size \n",
      "-\n",
      "AdamW \n",
      "Optimizer \n",
      "β 1 \n",
      "0.9 \n",
      "first moment of gradient \n",
      "β 2 \n",
      "0.99 \n",
      "second moment of gradient \n",
      "-\n",
      "1e-4 \n",
      "Learning rate \n",
      "-\n",
      "1e-4 \n",
      "Weight decay \n",
      "-\n",
      "Cosine decay Learning rate scheduler \n",
      "-\n",
      "1M \n",
      "Target number of training steps for learning rate scheduler \n",
      "-\n",
      "100K \n",
      "Warmup steps \n",
      "-\n",
      "10 \n",
      "Gradient clipping magnitude \n",
      "-\n",
      "1028 \n",
      "Batch size \n",
      "\n",
      "Symbol \n",
      "Value \n",
      "Description \n",
      "|z| \n",
      "1536 \n",
      "Sequence Length \n",
      "-\n",
      "24 \n",
      "Number of layer \n",
      "-\n",
      "2048 \n",
      "Embedding dimension \n",
      "-\n",
      "8192 \n",
      "MLP dimension \n",
      "-\n",
      "32 \n",
      "Number of heads \n",
      "-\n",
      "AdamW \n",
      "Optimizer \n",
      "β 1 \n",
      "0.9 \n",
      "first moment of gradient \n",
      "β 2 \n",
      "0.99 \n",
      "second moment of gradient \n",
      "-\n",
      "1e-4 \n",
      "Learning rate \n",
      "-\n",
      "1e-4 \n",
      "Weight decay \n",
      "-\n",
      "Cosine decay Learning rate scheduler \n",
      "-\n",
      "4M \n",
      "Target number of training steps for learning rate scheduler \n",
      "-\n",
      "10K \n",
      "Warmup steps \n",
      "-\n",
      "10 \n",
      "Gradient clipping magnitude \n",
      "-\n",
      "512 \n",
      "Batch size \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of 1 × 10 −4 for the encoder-decoder and discriminator. To capture longer time horizons during training and better evaluate temporal coherence, we downsample the MiT dataset from 25 FPS to 6 FPS and evaluate on videos of 10 frames at spatial resolution of 128 × 128. B.2 IMAGE CONDITIONAL VIDEO GENERATION B.2.1 BAIR ROBOT PUSH C-VIVIT ARCHITECTURE\n",
      "\n",
      "\n",
      "The MaskGIT architecture used against NUWA consists of 20 transformer layers with 1536 hidden units, 24 attention heads, and 6144 MLP hidden units, resulting in 0.9B parameters similar to NUWA. For the main experiments in this paper, we use a larger architecture that consists of consists of 24 transformer layers with 2048 hidden units, 32 attention heads, and 8192 mlp hidden units, resulting in 1.8B parameters.B.3.2 TRAINING AND INFERENCEFor all our text-conditional video generation, we use the training parametersTable 7. B.3.3 INFERENCE PARAMETERS AGAINST NUWA We use λ = 0.1, 12 MaskGIT iterations, and temperature of 4.0. B.3.4 INFERENCE PARAMETERS FOR ABLATION OF IMAGE AND VIDEO DATA FOR TRAINING. We use λ = 6, 24 MaskGIT iterations, and temperature of 4.0. B.3.5 INFERENCE PARAMETERS FOR ALL VIDEOS IN THE PAPER.\n",
      "\n",
      "Vivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid, ICCV. 2021Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vision transformer. In ICCV, 2021.\n",
      "\n",
      "Stochastic variational video prediction. ICLR. Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, H Roy, Sergey Campbell, Levine, Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. ICLR, 2018.\n",
      "\n",
      "Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, Dumitru Erhan, Fitvid, arXiv:2106.13195Overfitting in pixel-level video prediction. arXiv preprintMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2020.\n",
      "\n",
      "Frozen in time: A joint video and image encoder for end-to-end retrieval. Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728-1738, 2021.\n",
      "\n",
      "Conditional gan with discriminative filter generation for text-to-video synthesis. Yogesh Balaji, Bing Martin Renqiang Min, Rama Bai, Hans Peter Chellappa, Graf, IJCAI. Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Con- ditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, 2019.\n",
      "\n",
      "Quo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, CVPR. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017.\n",
      "\n",
      "A short note about kinetics-600. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, Andrew Zisserman, Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600, 2018.\n",
      "\n",
      "Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T Freeman, Maskgit, arXiv:2202.04200Masked generative image transformer. arXiv preprintHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. arXiv preprint arXiv:2202.04200, 2022.\n",
      "\n",
      "Adversarial video generation on complex datasets. Aidan Clark, Jeff Donahue, Karen Simonyan, arXiv:1907.06571arXiv preprintAidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019.\n",
      "\n",
      "Stochastic video generation with a learned prior. Emily Denton, Rob Fergus, Proceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine Learning80Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1174-1183, 2018.\n",
      "\n",
      "Self-supervised visual planning with temporal skip connections. Frederik Ebert, Chelsea Finn, Alex X Lee, Sergey Levine, Frederik Ebert, Chelsea Finn, Alex X. Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections, 2017.\n",
      "\n",
      "Taming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Björn Ommer, Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020.\n",
      "\n",
      "Unsupervised learning for physical interaction through video prediction. Chelsea Finn, Ian Goodfellow, Sergey Levine, Advances in neural information processing systems. Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical inter- action through video prediction. In Advances in neural information processing systems, pages 64-72, 2016.\n",
      "\n",
      "Flexible diffusion modeling of long videos. William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood, arXiv:2205.11495arXiv preprintWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022.\n",
      "\n",
      "Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, 30Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n",
      "\n",
      "Classifier-free diffusion guidance. Jonathan Ho, Tim Salimans, Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2021.\n",
      "\n",
      ". Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.03458Video diffusion models. arXiv preprintJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.\n",
      "\n",
      "Cogvideo: Large-scale pretraining for text-to-video generation via transformers. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang, arXiv:2205.15868arXiv preprintWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\n",
      "\n",
      "Diffusion models for video prediction and infilling. Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi, arXiv:2206.07696arXiv preprintTobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022.\n",
      "\n",
      "Justin Johnson, Alexandre Alahi, Li Fei-Fei, arXiv:1603.08155Perceptual losses for real-time style transfer and super-resolution. arXiv preprintJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. arXiv preprint arXiv:1603.08155, 2016.\n",
      "\n",
      "Analyzing and improving the image quality of stylegan. Jtero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, CVPR. JTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020.\n",
      "\n",
      "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, The kinetics human action video dataset. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya- narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and An- drew Zisserman. The kinetics human action video dataset, 2017.\n",
      "\n",
      "Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "\n",
      "Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, arXiv:1903.01434Laurent Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprintManoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Lau- rent Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprint arXiv:1903.01434, 2019.\n",
      "\n",
      "Video generation from text. Yitong Li, Martin Min, Dinghan Shen, David Carlson, Lawrence Carin, AAAI. Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In AAAI, 2018.\n",
      "\n",
      "Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n",
      "\n",
      "Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. Pauline Luc, Aidan Clark, Sander Dieleman, arXiv:2003.04035Yotam DoronarXiv preprintDiego de Las CasasPauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cas- sirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. arXiv preprint arXiv:2003.04035, 2019.\n",
      "\n",
      "CCVS: Context-aware controllable video synthesis. Guillaume Le Moing, Jean Ponce, Cordelia Schmid, NeurIPS. 2021Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.\n",
      "\n",
      "Moments in time dataset: one million videos for event understanding. Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, IEEE Transactions on Pattern Analysis and Machine Intelligence. Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n",
      "\n",
      "Learning audio-video modalities from image captions. Arsha Nagrani, Paul Hongsuck Seo, Bryan Andrew Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid, ECCV. 2022Arsha Nagrani, Paul Hongsuck Seo, Bryan Andrew Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities from image captions. In ECCV, 2022.\n",
      "\n",
      "Transframer: Arbitrary frame prediction with generative models. Charlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, Peter Battaglia, arXiv:2203.09494arXiv preprintCharlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv preprint arXiv:2203.09494, 2019.\n",
      "\n",
      "Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc-Grew, Ilya Sutskever, Mark Chen, arXiv:2112.10741arXiv preprintAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mc- Grew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n",
      "\n",
      ". Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, Evgeny Burnaev, arXiv:2006.10704Latent video transformer. arXiv preprintRuslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. arXiv preprint arXiv:2006.10704, 2020.\n",
      "\n",
      "Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLRAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021.\n",
      "\n",
      "Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125arXiv preprintAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n",
      "\n",
      "Video (language) modeling: a baseline for generative models of natural videos. Marcaurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, Sumit Chopra, arXiv:1412.6604arXiv preprintMarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.\n",
      "\n",
      "Afroz Mohiuddin, et al. Scaling up models and data with t5x and seqio. Adam Roberts, Hyung Won, Anselm Chung, Gaurav Levskaya, James Mishra, Daniel Bradbury, Sharan Andor, Brian Narang, Colin Lester, Gaffney, arXiv:2203.17189arXiv preprintAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scal- ing up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.\n",
      "\n",
      "Photorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, ; S Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.11487Burcu Karagol Ayan. arXiv preprintSeyed Kamyar Seyed GhasemipourChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n",
      "\n",
      "Temporal generative adversarial nets with singular value clipping. Masaki Saito, Eiichi Matsumoto, Shunta Saito, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMasaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 2830-2839, 2017.\n",
      "\n",
      "Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan. Masaki Saito, Shunta Saito, Masanori Koyama, Sosuke Kobayashi, International Journal of Computer Vision. 12810Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, gener- ate densely: Memory-efficient unsupervised training of high-resolution temporal gan. Interna- tional Journal of Computer Vision, 128(10):2586-2606, 2020.\n",
      "\n",
      "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114arXiv preprintChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n",
      "\n",
      "Unsupervised learning of video representations using lstms. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov, International Conference on Machine Learning. Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International Conference on Machine Learning, 2015.\n",
      "\n",
      "Mocogan: Decomposing motion and content for video generation. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526-1535, 2018.\n",
      "\n",
      "Towards accurate generative models of video: A new metric & challenges. Thomas Unterthiner, Karol Sjoerd Van Steenkiste, Raphael Kurach, Marinier, arXiv:1812.01717arXiv preprintMarcin Michalski, and Sylvain GellyThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michal- ski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & chal- lenges. arXiv preprint arXiv:1812.01717, 2018.\n",
      "\n",
      "Neural discrete representation learning. Aaron Van Den Oord, Oriol Vinyals, Koray Kavukcuoglu, NeurIPS. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2018.\n",
      "\n",
      "High fidelity video prediction with large stochastic recurrent neural networks. Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, V Quoc, Honglak Le, Lee, Advances in Neural Information Processing Systems. Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V Le, and Honglak Lee. High fidelity video prediction with large stochastic recurrent neural networks. In Ad- vances in Neural Information Processing Systems, pages 81-91, 2019.\n",
      "\n",
      "Mcvd: Masked conditional video diffusion for prediction, generation, and interpolation. Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal, arXiv:2205.09853arXiv preprintVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Mcvd: Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\n",
      "\n",
      "Generating videos with scene dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, arXiv:1609.02612arXiv preprintCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dy- namics. arXiv preprint arXiv:1609.02612, 2016.\n",
      "\n",
      ". Jacob Walker, Ali Razavi, Aäron Van Den Oord, arXiv:2103.01950Predicting video with vqvae. arXiv preprintJacob Walker, Ali Razavi, and Aäron van den Oord. Predicting video with vqvae. arXiv preprint arXiv:2103.01950, 2019.\n",
      "\n",
      "Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. Advances in neural information processing systems. Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, Philip S Yu, 30Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Re- current neural networks for predictive learning using spatiotemporal lstms. Advances in neural information processing systems, 30, 2017.\n",
      "\n",
      "Scaling autoregressive video models. Dirk Weissenborn, Oscar Täckström, Jakob Uszkoreit, ICLR. Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video mod- els. In ICLR, 2020.\n",
      "\n",
      "Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, Nan Duan Godiva, arXiv:2104.14806Generating open-domain videos from natural descriptions. arXiv preprintChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.\n",
      "\n",
      "Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, arXiv:2207.09814Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. arXiv preprintChenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. arXiv preprint arXiv:2207.09814, 2022.\n",
      "\n",
      "NÜwa: Visual synthesis pre-training for neural visual world creation. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan, ECCV. 2022Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. NÜwa: Visual synthesis pre-training for neural visual world creation. In ECCV, 2022.\n",
      "\n",
      "Videogpt: Video generation using vq-vae and transformers. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas, arXiv:2104.10157arXiv preprintWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2019.\n",
      "\n",
      "Diffusion probabilistic modeling for video generation. Ruihan Yang, Prakhar Srivastava, Stephan Mandt, arXiv:2203.09481arXiv preprintRuihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481, 2022.\n",
      "\n",
      "Harp: Autoregressive latent video prediction with high-fidelity image generator. Fangchen Liu Stephen James Pieter Abbeel Younggyo Seo, Kimin Lee, arXiv:2209.07143arXiv preprintFangchen Liu Stephen James Pieter Abbeel Younggyo Seo, Kimin Lee. Harp: Autoregressive latent video prediction with high-fidelity image generator. arXiv preprint arXiv:2209.07143, 2022.\n",
      "\n",
      "Vector-quantized image modeling with improved vqgan. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu, ICLR. 2022Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022.\n",
      "\n",
      "Scaling autoregressive models for content-rich text-to-image generation. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Ben Burcu Karagol Ayan, Wei Hutchinson, Zarana Han, Xin Parekh, Han Li, Jason Zhang, Yonghui Baldridge, Wu, arXiv:2206.10789arXiv preprintJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Va- sudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n",
      "\n",
      "Scaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision trans- formers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 12104-12113, 2022.\n",
      "\n",
      "The unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPRRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, , and Oliver Wang. The unrea- sonable effectiveness of deep features as a perceptual metric. CVPR, 2018.\n"
     ]
    }
   ],
   "source": [
    "print(corpus_clean_df.loc[0, \"full_paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e597b6fa-0e0d-496a-ba17-ebb132936d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>citations</th>\n",
       "      <th>full_paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>174802916</td>\n",
       "      <td>Published as a conference paper at ICLR 2020 S...</td>\n",
       "      <td>Due to the statistical complexity of video, th...</td>\n",
       "      <td>[54458552, 205514, 52892477, 8495258]</td>\n",
       "      <td>Published as a conference paper at ICLR 2020 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>238582653</td>\n",
       "      <td>VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVE...</td>\n",
       "      <td>Pretraining language models with next-token pr...</td>\n",
       "      <td>[227127234, 52967399, 3568073, 84591, 4009713,...</td>\n",
       "      <td>VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>6628106</td>\n",
       "      <td>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</td>\n",
       "      <td>We introduce Adam, an algorithm for first-orde...</td>\n",
       "      <td>[1428702]</td>\n",
       "      <td>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       corpusid                                              title  \\\n",
       "3460  174802916  Published as a conference paper at ICLR 2020 S...   \n",
       "4009  238582653  VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVE...   \n",
       "4156    6628106         ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION   \n",
       "\n",
       "                                               abstract  \\\n",
       "3460  Due to the statistical complexity of video, th...   \n",
       "4009  Pretraining language models with next-token pr...   \n",
       "4156  We introduce Adam, an algorithm for first-orde...   \n",
       "\n",
       "                                              citations  \\\n",
       "3460              [54458552, 205514, 52892477, 8495258]   \n",
       "4009  [227127234, 52967399, 3568073, 84591, 4009713,...   \n",
       "4156                                          [1428702]   \n",
       "\n",
       "                                             full_paper  \n",
       "3460  Published as a conference paper at ICLR 2020 S...  \n",
       "4009  VECTOR-QUANTIZED IMAGE MODELING WITH IM- PROVE...  \n",
       "4156  ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\\n\\n...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean_df[corpus_clean_df[\"corpusid\"].isin([6628106, 174802916, 238582653])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a21880-46e2-4bd0-b6e6-c814fcf6a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       query_set  count\n",
      "0     inline_acl     98\n",
      "1  inline_nonacl    253\n",
      "2     manual_acl    155\n",
      "3    manual_iclr     91\n"
     ]
    }
   ],
   "source": [
    "query_counts = query_df.groupby('query_set').size().reset_index(name='count')\n",
    "print(query_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e22abb6b-a9a0-4926-9d1e-34f04305e437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpusid      64183\n",
       "title         56717\n",
       "abstract      55909\n",
       "citations     44680\n",
       "full_paper    63725\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean_df.astype(str).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b886f-5b3f-479c-9793-80bd19f24e14",
   "metadata": {},
   "source": [
    "ALCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "437065a0-3995-441a-877d-e047cd8819fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/uskh580e-llm_citation_ws/ALCE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/uskh580e-llm_citation_ws/myenv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ALCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dacd6c5-ea76-474a-98ab-7132bd2ff6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t\t  eval.py     post_hoc_cite.py\trun.py\n",
      "configs\t\t  human_eval  prompts\t\tsearcher.py\n",
      "data\t\t  LICENSE     README.md\t\ttools\n",
      "download_data.sh  paper       retrieval.py\tutils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863376a5-2b26-44a3-b9a4-ed743055a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-07 20:30:31--  https://huggingface.co/datasets/princeton-nlp/ALCE-data/resolve/main/ALCE-data.tar\n",
      "Resolving huggingface.co (huggingface.co)... 3.160.150.7, 3.160.150.119, 3.160.150.2, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.160.150.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/repos/56/20/5620fb830225268cf35bc951e315c64ab64bc534c5c9ea4f4fcf278da5389466/eda837bf659a91b3648dc6e7ab6b17197664d93593857e8fdf3800b6aa6a98f0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ALCE-data.tar%3B+filename%3D%22ALCE-data.tar%22%3B&response-content-type=application%2Fx-tar&Expires=1738960232&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODk2MDIzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy81Ni8yMC81NjIwZmI4MzAyMjUyNjhjZjM1YmM5NTFlMzE1YzY0YWI2NGJjNTM0YzVjOWVhNGY0ZmNmMjc4ZGE1Mzg5NDY2L2VkYTgzN2JmNjU5YTkxYjM2NDhkYzZlN2FiNmIxNzE5NzY2NGQ5MzU5Mzg1N2U4ZmRmMzgwMGI2YWE2YTk4ZjA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=bqnKHgDthRe7BuL36TjtWks9m49ZSSrpXzaJiYjuGQ8yOhOVZa6EGyGiFHM9eMu2SgQN98Ly1XQ1yKJL6mLMA1X%7EYUkAbHpRUg%7Eymdv7JtnLAier%7EzxzoLhY22EwwHiqcYPTaZrD5H4aShcOkkmtOJRTQen3UKFoyoj6mhHrR7FYXvJ9go3pWXSunaAhz%7E%7ERd2T8Dz73BzZjX06MddreSJ9SNOg1QUKPP5rx7ljxP8yg-Tbd2TyRLeh-qjzbkKMy4p5SANIxyD17BmnZAiKlbI-N6sUm0rqxSGMxTtiE34pRe9YVKpZzFfovUkyqr5PuFAhTqnl%7EEX-Z5an0SCoQMQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2025-02-07 20:30:32--  https://cdn-lfs.hf.co/repos/56/20/5620fb830225268cf35bc951e315c64ab64bc534c5c9ea4f4fcf278da5389466/eda837bf659a91b3648dc6e7ab6b17197664d93593857e8fdf3800b6aa6a98f0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ALCE-data.tar%3B+filename%3D%22ALCE-data.tar%22%3B&response-content-type=application%2Fx-tar&Expires=1738960232&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODk2MDIzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy81Ni8yMC81NjIwZmI4MzAyMjUyNjhjZjM1YmM5NTFlMzE1YzY0YWI2NGJjNTM0YzVjOWVhNGY0ZmNmMjc4ZGE1Mzg5NDY2L2VkYTgzN2JmNjU5YTkxYjM2NDhkYzZlN2FiNmIxNzE5NzY2NGQ5MzU5Mzg1N2U4ZmRmMzgwMGI2YWE2YTk4ZjA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=bqnKHgDthRe7BuL36TjtWks9m49ZSSrpXzaJiYjuGQ8yOhOVZa6EGyGiFHM9eMu2SgQN98Ly1XQ1yKJL6mLMA1X%7EYUkAbHpRUg%7Eymdv7JtnLAier%7EzxzoLhY22EwwHiqcYPTaZrD5H4aShcOkkmtOJRTQen3UKFoyoj6mhHrR7FYXvJ9go3pWXSunaAhz%7E%7ERd2T8Dz73BzZjX06MddreSJ9SNOg1QUKPP5rx7ljxP8yg-Tbd2TyRLeh-qjzbkKMy4p5SANIxyD17BmnZAiKlbI-N6sUm0rqxSGMxTtiE34pRe9YVKpZzFfovUkyqr5PuFAhTqnl%7EEX-Z5an0SCoQMQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "18.172.112.14, 18.172.112.124, 18.172.112.90, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.172.112.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451297280 (430M) [application/x-tar]\n",
      "Saving to: ‘ALCE-data.tar’\n",
      "\n",
      "ALCE-data.tar       100%[===================>] 430.39M  54.6MB/s    in 7.9s    \n",
      "\n",
      "2025-02-07 20:30:40 (54.4 MB/s) - ‘ALCE-data.tar’ saved [451297280/451297280]\n",
      "\n",
      "ALCE-data/\n",
      "ALCE-data/qampari_eval_gtr_top100.json\n",
      "ALCE-data/asqa_eval_gtr_top100.json\n",
      "ALCE-data/eli5_eval_bm25_top100.json\n",
      "ALCE-data/eli5_eval_bm25_top100_reranked_oracle.json\n",
      "ALCE-data/asqa_eval_gtr_top100_reranked_oracle.json\n",
      "ALCE-data/qampari_eval_gtr_top100_reranked_oracle.json\n",
      "ALCE-data/qampari_eval_dpr_top100.json\n",
      "ALCE-data/asqa_eval_dpr_top100.json\n",
      "mv: cannot move 'ALCE-data' to 'data/ALCE-data': Directory not empty\n",
      "deleting tar file...\n"
     ]
    }
   ],
   "source": [
    "!bash download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98f42fb8-759b-4c53-ac8c-92bc7f23cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x. 3 uskh580e p_scads_llm_secrets 4096 Feb  7 20:28 ALCE-data\n",
      "drwxr-xr-x. 2 uskh580e p_scads_llm_secrets 4096 Jan 31 23:38 asqa\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "509277cb-e448-40c2-b684-01690ef9263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 440744\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 83625528 May 23  2023 asqa_eval_dpr_top100.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 84762794 May 23  2023 asqa_eval_gtr_top100.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 10350811 May 23  2023 asqa_eval_gtr_top100_reranked_oracle.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 83469180 May 23  2023 eli5_eval_bm25_top100.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets  7631357 May 23  2023 eli5_eval_bm25_top100_reranked_oracle.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 85769271 May 23  2023 qampari_eval_dpr_top100.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets 87241220 May 23  2023 qampari_eval_gtr_top100.json\n",
      "-rw-r--r--. 1 uskh580e p_scads_llm_secrets  8434999 May 23  2023 qampari_eval_gtr_top100_reranked_oracle.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/ALCE-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b4137b0-5f8b-4342-90bf-de543babed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for eli5_eval_bm25_top100_reranked_oracle.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_ctx</th>\n",
       "      <th>answer</th>\n",
       "      <th>claims</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are firms like snapchat, uber etc valued s...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Yes. Did you watch The Social Network? They we...</td>\n",
       "      <td>[Firms like Snapchat and Uber need to establis...</td>\n",
       "      <td>[{'title': 'Is Snapchat really worth $19 billi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it bad to eat cookie dough for risk of ...</td>\n",
       "      <td></td>\n",
       "      <td>Any edible \"cookie dough\" products are made wi...</td>\n",
       "      <td>[Cookie Dough Bites are safe to eat since they...</td>\n",
       "      <td>[{'title': 'It's Probably OK to Eat Raw Cookie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how does so much of our trash end up in the oc...</td>\n",
       "      <td>I read that something like 80% of plastic ends...</td>\n",
       "      <td>Because water flows downhill and very often en...</td>\n",
       "      <td>[When it rains, trash is washed downhill and i...</td>\n",
       "      <td>[{'title': 'ENVIRONMENTAL THREATS', 'text': 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please explain American equivalents for Britis...</td>\n",
       "      <td>I have always loved British comedy shows. Pyth...</td>\n",
       "      <td>**Education:**\\n\\nProgression is measured in Y...</td>\n",
       "      <td>[In the UK, primary school is for Key Stages 1...</td>\n",
       "      <td>[{'title': 'Integration hub | Education', 'tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are quantifiers, and the difference betwe...</td>\n",
       "      <td></td>\n",
       "      <td>Okay so there are two types of common quantifi...</td>\n",
       "      <td>[Quantifiers are symbols that are used to repr...</td>\n",
       "      <td>[{'title': 'The cognitive functions of languag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How are firms like snapchat, uber etc valued s...   \n",
       "1  Why is it bad to eat cookie dough for risk of ...   \n",
       "2  how does so much of our trash end up in the oc...   \n",
       "3  please explain American equivalents for Britis...   \n",
       "4  What are quantifiers, and the difference betwe...   \n",
       "\n",
       "                                        question_ctx  \\\n",
       "0                                          [removed]   \n",
       "1                                                      \n",
       "2  I read that something like 80% of plastic ends...   \n",
       "3  I have always loved British comedy shows. Pyth...   \n",
       "4                                                      \n",
       "\n",
       "                                              answer  \\\n",
       "0  Yes. Did you watch The Social Network? They we...   \n",
       "1  Any edible \"cookie dough\" products are made wi...   \n",
       "2  Because water flows downhill and very often en...   \n",
       "3  **Education:**\\n\\nProgression is measured in Y...   \n",
       "4  Okay so there are two types of common quantifi...   \n",
       "\n",
       "                                              claims  \\\n",
       "0  [Firms like Snapchat and Uber need to establis...   \n",
       "1  [Cookie Dough Bites are safe to eat since they...   \n",
       "2  [When it rains, trash is washed downhill and i...   \n",
       "3  [In the UK, primary school is for Key Stages 1...   \n",
       "4  [Quantifiers are symbols that are used to repr...   \n",
       "\n",
       "                                                docs  \n",
       "0  [{'title': 'Is Snapchat really worth $19 billi...  \n",
       "1  [{'title': 'It's Probably OK to Eat Raw Cookie...  \n",
       "2  [{'title': 'ENVIRONMENTAL THREATS', 'text': 't...  \n",
       "3  [{'title': 'Integration hub | Education', 'tex...  \n",
       "4  [{'title': 'The cognitive functions of languag...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for qampari_eval_gtr_top100.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>docs</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799__wikidata_simple__dev</td>\n",
       "      <td>What manga was drawn by Ryoichi Ikegami?</td>\n",
       "      <td>[[Heat], [Mai, the Psychic Girl], [Wounded Man...</td>\n",
       "      <td>[{'id': '8377518', 'title': 'Ryu Fujisaki', 't...</td>\n",
       "      <td>Heat, Mai, the Psychic Girl, Wounded Man, Sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136__wikidata_intersection__dev</td>\n",
       "      <td>Harmony Korine was both screenwriter and direc...</td>\n",
       "      <td>[[Spring Breakers], [Trash Humpers], [Julien D...</td>\n",
       "      <td>[{'id': '3518783', 'title': 'Harmony Korine', ...</td>\n",
       "      <td>Spring Breakers, Trash Humpers, Julien Donkey-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248__wikidata_comp__dev</td>\n",
       "      <td>Where did administrators of the UN Development...</td>\n",
       "      <td>[[University of Auckland], [Yale Law School], ...</td>\n",
       "      <td>[{'id': '5654454', 'title': 'United Nations In...</td>\n",
       "      <td>University of Auckland, Yale Law School, Marlb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366__wikidata_comp__dev</td>\n",
       "      <td>Who directed a film that had P. Balachandran a...</td>\n",
       "      <td>[[Kamal], [P. Balachandran], [T. K. Rajeev Kum...</td>\n",
       "      <td>[{'id': '12727772', 'title': 'Server Sundaram'...</td>\n",
       "      <td>Kamal, P. Balachandran, T. K. Rajeev Kumar, V....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>663__wikidata_simple__dev</td>\n",
       "      <td>The Russian Empire has what ships registered t...</td>\n",
       "      <td>[[Russian gunboat Sivuch], [Russian cruiser Ad...</td>\n",
       "      <td>[{'id': '13872170', 'title': 'Russian Maritime...</td>\n",
       "      <td>Russian gunboat Sivuch, Russian cruiser Admira...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0        799__wikidata_simple__dev   \n",
       "1  136__wikidata_intersection__dev   \n",
       "2          248__wikidata_comp__dev   \n",
       "3          366__wikidata_comp__dev   \n",
       "4        663__wikidata_simple__dev   \n",
       "\n",
       "                                            question  \\\n",
       "0           What manga was drawn by Ryoichi Ikegami?   \n",
       "1  Harmony Korine was both screenwriter and direc...   \n",
       "2  Where did administrators of the UN Development...   \n",
       "3  Who directed a film that had P. Balachandran a...   \n",
       "4  The Russian Empire has what ships registered t...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [[Heat], [Mai, the Psychic Girl], [Wounded Man...   \n",
       "1  [[Spring Breakers], [Trash Humpers], [Julien D...   \n",
       "2  [[University of Auckland], [Yale Law School], ...   \n",
       "3  [[Kamal], [P. Balachandran], [T. K. Rajeev Kum...   \n",
       "4  [[Russian gunboat Sivuch], [Russian cruiser Ad...   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [{'id': '8377518', 'title': 'Ryu Fujisaki', 't...   \n",
       "1  [{'id': '3518783', 'title': 'Harmony Korine', ...   \n",
       "2  [{'id': '5654454', 'title': 'United Nations In...   \n",
       "3  [{'id': '12727772', 'title': 'Server Sundaram'...   \n",
       "4  [{'id': '13872170', 'title': 'Russian Maritime...   \n",
       "\n",
       "                                              answer  \n",
       "0  Heat, Mai, the Psychic Girl, Wounded Man, Sanc...  \n",
       "1  Spring Breakers, Trash Humpers, Julien Donkey-...  \n",
       "2  University of Auckland, Yale Law School, Marlb...  \n",
       "3  Kamal, P. Balachandran, T. K. Rajeev Kumar, V....  \n",
       "4  Russian gunboat Sivuch, Russian cruiser Admira...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for asqa_eval_gtr_top100.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_pairs</th>\n",
       "      <th>wikipages</th>\n",
       "      <th>annotations</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>question</th>\n",
       "      <th>docs</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'International Federation of Footba...</td>\n",
       "      <td>[{'knowledge': [], 'long_answer': 'Ali Dael ha...</td>\n",
       "      <td>-7013890438520559398</td>\n",
       "      <td>Who has the highest goals in world football?</td>\n",
       "      <td>[{'id': '6669150', 'title': 'Argentina–Brazil ...</td>\n",
       "      <td>The players with the highest all-time goals an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'context': 'Sounds of Silence is the second ...</td>\n",
       "      <td>[{'title': 'The Sound of Silence', 'url': 'htt...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Wednesday Morning...</td>\n",
       "      <td>7089015503030534342</td>\n",
       "      <td>Who is the original artist of sound of silence?</td>\n",
       "      <td>[{'id': '2627084', 'title': 'The Sound of Sile...</td>\n",
       "      <td>There are several songs with the title \"Sound ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'context': 'On January 9, 2007, Steve Jobs a...</td>\n",
       "      <td>[{'title': 'iPhone (1st generation)', 'url': '...</td>\n",
       "      <td>[{'knowledge': [{'content': 'The iPhone was re...</td>\n",
       "      <td>8793099883447006698</td>\n",
       "      <td>When was the first apple i phone made?</td>\n",
       "      <td>[{'id': '14664751', 'title': 'IPhone (1st gene...</td>\n",
       "      <td>There were several Apple iPhones, including th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'context': 'Richard Fish appeared as Bill br...</td>\n",
       "      <td>[{'title': 'List of Harry Potter characters', ...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Dozens of actors ...</td>\n",
       "      <td>-881464876144297194</td>\n",
       "      <td>Who played the weasley brothers in harry potter?</td>\n",
       "      <td>[{'id': '5621126', 'title': 'James and Oliver ...</td>\n",
       "      <td>The Weasely family is a family of wizards incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'List of Virginia state parks', 'ur...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Virginia opened i...</td>\n",
       "      <td>1650309494326541834</td>\n",
       "      <td>How many state parks are there in virginia?</td>\n",
       "      <td>[{'id': '444081', 'title': 'Virginia', 'text':...</td>\n",
       "      <td>Virginia opened its entire state park system o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            qa_pairs  \\\n",
       "0  [{'context': 'No context provided', 'question'...   \n",
       "1  [{'context': 'Sounds of Silence is the second ...   \n",
       "2  [{'context': 'On January 9, 2007, Steve Jobs a...   \n",
       "3  [{'context': 'Richard Fish appeared as Bill br...   \n",
       "4  [{'context': 'No context provided', 'question'...   \n",
       "\n",
       "                                           wikipages  \\\n",
       "0  [{'title': 'International Federation of Footba...   \n",
       "1  [{'title': 'The Sound of Silence', 'url': 'htt...   \n",
       "2  [{'title': 'iPhone (1st generation)', 'url': '...   \n",
       "3  [{'title': 'List of Harry Potter characters', ...   \n",
       "4  [{'title': 'List of Virginia state parks', 'ur...   \n",
       "\n",
       "                                         annotations             sample_id  \\\n",
       "0  [{'knowledge': [], 'long_answer': 'Ali Dael ha...  -7013890438520559398   \n",
       "1  [{'knowledge': [{'content': 'Wednesday Morning...   7089015503030534342   \n",
       "2  [{'knowledge': [{'content': 'The iPhone was re...   8793099883447006698   \n",
       "3  [{'knowledge': [{'content': 'Dozens of actors ...   -881464876144297194   \n",
       "4  [{'knowledge': [{'content': 'Virginia opened i...   1650309494326541834   \n",
       "\n",
       "                                           question  \\\n",
       "0      Who has the highest goals in world football?   \n",
       "1   Who is the original artist of sound of silence?   \n",
       "2            When was the first apple i phone made?   \n",
       "3  Who played the weasley brothers in harry potter?   \n",
       "4       How many state parks are there in virginia?   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [{'id': '6669150', 'title': 'Argentina–Brazil ...   \n",
       "1  [{'id': '2627084', 'title': 'The Sound of Sile...   \n",
       "2  [{'id': '14664751', 'title': 'IPhone (1st gene...   \n",
       "3  [{'id': '5621126', 'title': 'James and Oliver ...   \n",
       "4  [{'id': '444081', 'title': 'Virginia', 'text':...   \n",
       "\n",
       "                                              answer  \n",
       "0  The players with the highest all-time goals an...  \n",
       "1  There are several songs with the title \"Sound ...  \n",
       "2  There were several Apple iPhones, including th...  \n",
       "3  The Weasely family is a family of wizards incl...  \n",
       "4  Virginia opened its entire state park system o...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for qampari_eval_dpr_top100.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>docs</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799__wikidata_simple__dev</td>\n",
       "      <td>What manga was drawn by Ryoichi Ikegami?</td>\n",
       "      <td>[[Heat], [Mai, the Psychic Girl], [Wounded Man...</td>\n",
       "      <td>[{'id': 'wiki:8377518', 'title': 'Ryu Fujisaki...</td>\n",
       "      <td>Heat, Mai, the Psychic Girl, Wounded Man, Sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136__wikidata_intersection__dev</td>\n",
       "      <td>Harmony Korine was both screenwriter and direc...</td>\n",
       "      <td>[[Spring Breakers], [Trash Humpers], [Julien D...</td>\n",
       "      <td>[{'id': 'wiki:3518795', 'title': 'Harmony Kori...</td>\n",
       "      <td>Spring Breakers, Trash Humpers, Julien Donkey-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248__wikidata_comp__dev</td>\n",
       "      <td>Where did administrators of the UN Development...</td>\n",
       "      <td>[[University of Auckland], [Yale Law School], ...</td>\n",
       "      <td>[{'id': 'wiki:5655643', 'title': 'United Natio...</td>\n",
       "      <td>University of Auckland, Yale Law School, Marlb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366__wikidata_comp__dev</td>\n",
       "      <td>Who directed a film that had P. Balachandran a...</td>\n",
       "      <td>[[Kamal], [P. Balachandran], [T. K. Rajeev Kum...</td>\n",
       "      <td>[{'id': 'wiki:16710085', 'title': 'P. Balachan...</td>\n",
       "      <td>Kamal, P. Balachandran, T. K. Rajeev Kumar, V....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>663__wikidata_simple__dev</td>\n",
       "      <td>The Russian Empire has what ships registered t...</td>\n",
       "      <td>[[Russian gunboat Sivuch], [Russian cruiser Ad...</td>\n",
       "      <td>[{'id': 'wiki:2353570', 'title': 'Russian Navy...</td>\n",
       "      <td>Russian gunboat Sivuch, Russian cruiser Admira...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0        799__wikidata_simple__dev   \n",
       "1  136__wikidata_intersection__dev   \n",
       "2          248__wikidata_comp__dev   \n",
       "3          366__wikidata_comp__dev   \n",
       "4        663__wikidata_simple__dev   \n",
       "\n",
       "                                            question  \\\n",
       "0           What manga was drawn by Ryoichi Ikegami?   \n",
       "1  Harmony Korine was both screenwriter and direc...   \n",
       "2  Where did administrators of the UN Development...   \n",
       "3  Who directed a film that had P. Balachandran a...   \n",
       "4  The Russian Empire has what ships registered t...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [[Heat], [Mai, the Psychic Girl], [Wounded Man...   \n",
       "1  [[Spring Breakers], [Trash Humpers], [Julien D...   \n",
       "2  [[University of Auckland], [Yale Law School], ...   \n",
       "3  [[Kamal], [P. Balachandran], [T. K. Rajeev Kum...   \n",
       "4  [[Russian gunboat Sivuch], [Russian cruiser Ad...   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [{'id': 'wiki:8377518', 'title': 'Ryu Fujisaki...   \n",
       "1  [{'id': 'wiki:3518795', 'title': 'Harmony Kori...   \n",
       "2  [{'id': 'wiki:5655643', 'title': 'United Natio...   \n",
       "3  [{'id': 'wiki:16710085', 'title': 'P. Balachan...   \n",
       "4  [{'id': 'wiki:2353570', 'title': 'Russian Navy...   \n",
       "\n",
       "                                              answer  \n",
       "0  Heat, Mai, the Psychic Girl, Wounded Man, Sanc...  \n",
       "1  Spring Breakers, Trash Humpers, Julien Donkey-...  \n",
       "2  University of Auckland, Yale Law School, Marlb...  \n",
       "3  Kamal, P. Balachandran, T. K. Rajeev Kumar, V....  \n",
       "4  Russian gunboat Sivuch, Russian cruiser Admira...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for asqa_eval_gtr_top100_reranked_oracle.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_pairs</th>\n",
       "      <th>wikipages</th>\n",
       "      <th>annotations</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>question</th>\n",
       "      <th>docs</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'International Federation of Footba...</td>\n",
       "      <td>[{'knowledge': [], 'long_answer': 'Ali Dael ha...</td>\n",
       "      <td>-7013890438520559398</td>\n",
       "      <td>Who has the highest goals in world football?</td>\n",
       "      <td>[{'id': '6669150', 'title': 'Argentina–Brazil ...</td>\n",
       "      <td>The players with the highest all-time goals an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'context': 'Sounds of Silence is the second ...</td>\n",
       "      <td>[{'title': 'The Sound of Silence', 'url': 'htt...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Wednesday Morning...</td>\n",
       "      <td>7089015503030534342</td>\n",
       "      <td>Who is the original artist of sound of silence?</td>\n",
       "      <td>[{'id': '2627084', 'title': 'The Sound of Sile...</td>\n",
       "      <td>There are several songs with the title \"Sound ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'context': 'On January 9, 2007, Steve Jobs a...</td>\n",
       "      <td>[{'title': 'iPhone (1st generation)', 'url': '...</td>\n",
       "      <td>[{'knowledge': [{'content': 'The iPhone was re...</td>\n",
       "      <td>8793099883447006698</td>\n",
       "      <td>When was the first apple i phone made?</td>\n",
       "      <td>[{'id': '14664751', 'title': 'IPhone (1st gene...</td>\n",
       "      <td>There were several Apple iPhones, including th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'context': 'Richard Fish appeared as Bill br...</td>\n",
       "      <td>[{'title': 'List of Harry Potter characters', ...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Dozens of actors ...</td>\n",
       "      <td>-881464876144297194</td>\n",
       "      <td>Who played the weasley brothers in harry potter?</td>\n",
       "      <td>[{'id': '1884113', 'title': 'Order of the Phoe...</td>\n",
       "      <td>The Weasely family is a family of wizards incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'List of Virginia state parks', 'ur...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Virginia opened i...</td>\n",
       "      <td>1650309494326541834</td>\n",
       "      <td>How many state parks are there in virginia?</td>\n",
       "      <td>[{'id': '12057784', 'title': 'Virginia Departm...</td>\n",
       "      <td>Virginia opened its entire state park system o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            qa_pairs  \\\n",
       "0  [{'context': 'No context provided', 'question'...   \n",
       "1  [{'context': 'Sounds of Silence is the second ...   \n",
       "2  [{'context': 'On January 9, 2007, Steve Jobs a...   \n",
       "3  [{'context': 'Richard Fish appeared as Bill br...   \n",
       "4  [{'context': 'No context provided', 'question'...   \n",
       "\n",
       "                                           wikipages  \\\n",
       "0  [{'title': 'International Federation of Footba...   \n",
       "1  [{'title': 'The Sound of Silence', 'url': 'htt...   \n",
       "2  [{'title': 'iPhone (1st generation)', 'url': '...   \n",
       "3  [{'title': 'List of Harry Potter characters', ...   \n",
       "4  [{'title': 'List of Virginia state parks', 'ur...   \n",
       "\n",
       "                                         annotations             sample_id  \\\n",
       "0  [{'knowledge': [], 'long_answer': 'Ali Dael ha...  -7013890438520559398   \n",
       "1  [{'knowledge': [{'content': 'Wednesday Morning...   7089015503030534342   \n",
       "2  [{'knowledge': [{'content': 'The iPhone was re...   8793099883447006698   \n",
       "3  [{'knowledge': [{'content': 'Dozens of actors ...   -881464876144297194   \n",
       "4  [{'knowledge': [{'content': 'Virginia opened i...   1650309494326541834   \n",
       "\n",
       "                                           question  \\\n",
       "0      Who has the highest goals in world football?   \n",
       "1   Who is the original artist of sound of silence?   \n",
       "2            When was the first apple i phone made?   \n",
       "3  Who played the weasley brothers in harry potter?   \n",
       "4       How many state parks are there in virginia?   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [{'id': '6669150', 'title': 'Argentina–Brazil ...   \n",
       "1  [{'id': '2627084', 'title': 'The Sound of Sile...   \n",
       "2  [{'id': '14664751', 'title': 'IPhone (1st gene...   \n",
       "3  [{'id': '1884113', 'title': 'Order of the Phoe...   \n",
       "4  [{'id': '12057784', 'title': 'Virginia Departm...   \n",
       "\n",
       "                                              answer  \n",
       "0  The players with the highest all-time goals an...  \n",
       "1  There are several songs with the title \"Sound ...  \n",
       "2  There were several Apple iPhones, including th...  \n",
       "3  The Weasely family is a family of wizards incl...  \n",
       "4  Virginia opened its entire state park system o...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for eli5_eval_bm25_top100.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_ctx</th>\n",
       "      <th>answer</th>\n",
       "      <th>claims</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are firms like snapchat, uber etc valued s...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Yes. Did you watch The Social Network? They we...</td>\n",
       "      <td>[Firms like Snapchat and Uber need to establis...</td>\n",
       "      <td>[{'title': 'Is Snapchat really worth $19 billi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it bad to eat cookie dough for risk of ...</td>\n",
       "      <td></td>\n",
       "      <td>Any edible \"cookie dough\" products are made wi...</td>\n",
       "      <td>[Cookie Dough Bites are safe to eat since they...</td>\n",
       "      <td>[{'title': 'How to Treat and Prevent Food Pois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how does so much of our trash end up in the oc...</td>\n",
       "      <td>I read that something like 80% of plastic ends...</td>\n",
       "      <td>Because water flows downhill and very often en...</td>\n",
       "      <td>[When it rains, trash is washed downhill and i...</td>\n",
       "      <td>[{'title': 'Teachers', 'text': 'The programs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please explain American equivalents for Britis...</td>\n",
       "      <td>I have always loved British comedy shows. Pyth...</td>\n",
       "      <td>**Education:**\\n\\nProgression is measured in Y...</td>\n",
       "      <td>[In the UK, primary school is for Key Stages 1...</td>\n",
       "      <td>[{'title': 'Integration hub | Education', 'tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are quantifiers, and the difference betwe...</td>\n",
       "      <td></td>\n",
       "      <td>Okay so there are two types of common quantifi...</td>\n",
       "      <td>[Quantifiers are symbols that are used to repr...</td>\n",
       "      <td>[{'title': 'The completeness and compactness t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How are firms like snapchat, uber etc valued s...   \n",
       "1  Why is it bad to eat cookie dough for risk of ...   \n",
       "2  how does so much of our trash end up in the oc...   \n",
       "3  please explain American equivalents for Britis...   \n",
       "4  What are quantifiers, and the difference betwe...   \n",
       "\n",
       "                                        question_ctx  \\\n",
       "0                                          [removed]   \n",
       "1                                                      \n",
       "2  I read that something like 80% of plastic ends...   \n",
       "3  I have always loved British comedy shows. Pyth...   \n",
       "4                                                      \n",
       "\n",
       "                                              answer  \\\n",
       "0  Yes. Did you watch The Social Network? They we...   \n",
       "1  Any edible \"cookie dough\" products are made wi...   \n",
       "2  Because water flows downhill and very often en...   \n",
       "3  **Education:**\\n\\nProgression is measured in Y...   \n",
       "4  Okay so there are two types of common quantifi...   \n",
       "\n",
       "                                              claims  \\\n",
       "0  [Firms like Snapchat and Uber need to establis...   \n",
       "1  [Cookie Dough Bites are safe to eat since they...   \n",
       "2  [When it rains, trash is washed downhill and i...   \n",
       "3  [In the UK, primary school is for Key Stages 1...   \n",
       "4  [Quantifiers are symbols that are used to repr...   \n",
       "\n",
       "                                                docs  \n",
       "0  [{'title': 'Is Snapchat really worth $19 billi...  \n",
       "1  [{'title': 'How to Treat and Prevent Food Pois...  \n",
       "2  [{'title': 'Teachers', 'text': 'The programs a...  \n",
       "3  [{'title': 'Integration hub | Education', 'tex...  \n",
       "4  [{'title': 'The completeness and compactness t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for qampari_eval_gtr_top100_reranked_oracle.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>docs</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799__wikidata_simple__dev</td>\n",
       "      <td>What manga was drawn by Ryoichi Ikegami?</td>\n",
       "      <td>[[Heat], [Mai, the Psychic Girl], [Wounded Man...</td>\n",
       "      <td>[{'id': '7329293', 'title': 'History of manga'...</td>\n",
       "      <td>Heat, Mai, the Psychic Girl, Wounded Man, Sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136__wikidata_intersection__dev</td>\n",
       "      <td>Harmony Korine was both screenwriter and direc...</td>\n",
       "      <td>[[Spring Breakers], [Trash Humpers], [Julien D...</td>\n",
       "      <td>[{'id': '3518783', 'title': 'Harmony Korine', ...</td>\n",
       "      <td>Spring Breakers, Trash Humpers, Julien Donkey-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248__wikidata_comp__dev</td>\n",
       "      <td>Where did administrators of the UN Development...</td>\n",
       "      <td>[[University of Auckland], [Yale Law School], ...</td>\n",
       "      <td>[{'id': '5654454', 'title': 'United Nations In...</td>\n",
       "      <td>University of Auckland, Yale Law School, Marlb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366__wikidata_comp__dev</td>\n",
       "      <td>Who directed a film that had P. Balachandran a...</td>\n",
       "      <td>[[Kamal], [P. Balachandran], [T. K. Rajeev Kum...</td>\n",
       "      <td>[{'id': '15097804', 'title': 'Ivan Megharoopan...</td>\n",
       "      <td>Kamal, P. Balachandran, T. K. Rajeev Kumar, V....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>663__wikidata_simple__dev</td>\n",
       "      <td>The Russian Empire has what ships registered t...</td>\n",
       "      <td>[[Russian gunboat Sivuch], [Russian cruiser Ad...</td>\n",
       "      <td>[{'id': '16901181', 'title': 'Russian ship of ...</td>\n",
       "      <td>Russian gunboat Sivuch, Russian cruiser Admira...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0        799__wikidata_simple__dev   \n",
       "1  136__wikidata_intersection__dev   \n",
       "2          248__wikidata_comp__dev   \n",
       "3          366__wikidata_comp__dev   \n",
       "4        663__wikidata_simple__dev   \n",
       "\n",
       "                                            question  \\\n",
       "0           What manga was drawn by Ryoichi Ikegami?   \n",
       "1  Harmony Korine was both screenwriter and direc...   \n",
       "2  Where did administrators of the UN Development...   \n",
       "3  Who directed a film that had P. Balachandran a...   \n",
       "4  The Russian Empire has what ships registered t...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [[Heat], [Mai, the Psychic Girl], [Wounded Man...   \n",
       "1  [[Spring Breakers], [Trash Humpers], [Julien D...   \n",
       "2  [[University of Auckland], [Yale Law School], ...   \n",
       "3  [[Kamal], [P. Balachandran], [T. K. Rajeev Kum...   \n",
       "4  [[Russian gunboat Sivuch], [Russian cruiser Ad...   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [{'id': '7329293', 'title': 'History of manga'...   \n",
       "1  [{'id': '3518783', 'title': 'Harmony Korine', ...   \n",
       "2  [{'id': '5654454', 'title': 'United Nations In...   \n",
       "3  [{'id': '15097804', 'title': 'Ivan Megharoopan...   \n",
       "4  [{'id': '16901181', 'title': 'Russian ship of ...   \n",
       "\n",
       "                                              answer  \n",
       "0  Heat, Mai, the Psychic Girl, Wounded Man, Sanc...  \n",
       "1  Spring Breakers, Trash Humpers, Julien Donkey-...  \n",
       "2  University of Auckland, Yale Law School, Marlb...  \n",
       "3  Kamal, P. Balachandran, T. K. Rajeev Kumar, V....  \n",
       "4  Russian gunboat Sivuch, Russian cruiser Admira...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame for asqa_eval_dpr_top100.json:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_pairs</th>\n",
       "      <th>wikipages</th>\n",
       "      <th>annotations</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'International Federation of Footba...</td>\n",
       "      <td>[{'knowledge': [], 'long_answer': 'Ali Dael ha...</td>\n",
       "      <td>-7013890438520559398</td>\n",
       "      <td>The players with the highest all-time goals an...</td>\n",
       "      <td>Who has the highest goals in world football?</td>\n",
       "      <td>[{'id': 'wiki:3262915', 'title': 'FIFA World R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'context': 'Sounds of Silence is the second ...</td>\n",
       "      <td>[{'title': 'The Sound of Silence', 'url': 'htt...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Wednesday Morning...</td>\n",
       "      <td>7089015503030534342</td>\n",
       "      <td>There are several songs with the title \"Sound ...</td>\n",
       "      <td>Who is the original artist of sound of silence?</td>\n",
       "      <td>[{'id': 'wiki:2627084', 'title': 'The Sound of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'context': 'On January 9, 2007, Steve Jobs a...</td>\n",
       "      <td>[{'title': 'iPhone (1st generation)', 'url': '...</td>\n",
       "      <td>[{'knowledge': [{'content': 'The iPhone was re...</td>\n",
       "      <td>8793099883447006698</td>\n",
       "      <td>There were several Apple iPhones, including th...</td>\n",
       "      <td>When was the first apple i phone made?</td>\n",
       "      <td>[{'id': 'wiki:5974', 'title': 'Apple I', 'text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'context': 'Richard Fish appeared as Bill br...</td>\n",
       "      <td>[{'title': 'List of Harry Potter characters', ...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Dozens of actors ...</td>\n",
       "      <td>-881464876144297194</td>\n",
       "      <td>The Weasely family is a family of wizards incl...</td>\n",
       "      <td>Who played the weasley brothers in harry potter?</td>\n",
       "      <td>[{'id': 'wiki:7276354', 'title': 'Oh No, It's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'context': 'No context provided', 'question'...</td>\n",
       "      <td>[{'title': 'List of Virginia state parks', 'ur...</td>\n",
       "      <td>[{'knowledge': [{'content': 'Virginia opened i...</td>\n",
       "      <td>1650309494326541834</td>\n",
       "      <td>Virginia opened its entire state park system o...</td>\n",
       "      <td>How many state parks are there in virginia?</td>\n",
       "      <td>[{'id': 'wiki:444081', 'title': 'Virginia', 't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            qa_pairs  \\\n",
       "0  [{'context': 'No context provided', 'question'...   \n",
       "1  [{'context': 'Sounds of Silence is the second ...   \n",
       "2  [{'context': 'On January 9, 2007, Steve Jobs a...   \n",
       "3  [{'context': 'Richard Fish appeared as Bill br...   \n",
       "4  [{'context': 'No context provided', 'question'...   \n",
       "\n",
       "                                           wikipages  \\\n",
       "0  [{'title': 'International Federation of Footba...   \n",
       "1  [{'title': 'The Sound of Silence', 'url': 'htt...   \n",
       "2  [{'title': 'iPhone (1st generation)', 'url': '...   \n",
       "3  [{'title': 'List of Harry Potter characters', ...   \n",
       "4  [{'title': 'List of Virginia state parks', 'ur...   \n",
       "\n",
       "                                         annotations             sample_id  \\\n",
       "0  [{'knowledge': [], 'long_answer': 'Ali Dael ha...  -7013890438520559398   \n",
       "1  [{'knowledge': [{'content': 'Wednesday Morning...   7089015503030534342   \n",
       "2  [{'knowledge': [{'content': 'The iPhone was re...   8793099883447006698   \n",
       "3  [{'knowledge': [{'content': 'Dozens of actors ...   -881464876144297194   \n",
       "4  [{'knowledge': [{'content': 'Virginia opened i...   1650309494326541834   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The players with the highest all-time goals an...   \n",
       "1  There are several songs with the title \"Sound ...   \n",
       "2  There were several Apple iPhones, including th...   \n",
       "3  The Weasely family is a family of wizards incl...   \n",
       "4  Virginia opened its entire state park system o...   \n",
       "\n",
       "                                           question  \\\n",
       "0      Who has the highest goals in world football?   \n",
       "1   Who is the original artist of sound of silence?   \n",
       "2            When was the first apple i phone made?   \n",
       "3  Who played the weasley brothers in harry potter?   \n",
       "4       How many state parks are there in virginia?   \n",
       "\n",
       "                                                docs  \n",
       "0  [{'id': 'wiki:3262915', 'title': 'FIFA World R...  \n",
       "1  [{'id': 'wiki:2627084', 'title': 'The Sound of...  \n",
       "2  [{'id': 'wiki:5974', 'title': 'Apple I', 'text...  \n",
       "3  [{'id': 'wiki:7276354', 'title': 'Oh No, It's ...  \n",
       "4  [{'id': 'wiki:444081', 'title': 'Virginia', 't...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"data/ALCE-data\"\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "df_dict = {}\n",
    "\n",
    "# Loop through each JSON file and convert to DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    \n",
    "    # Load JSON file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Store DataFrame in a dictionary with the filename (without .json)\n",
    "    df_dict[file.replace(\".json\", \"\")] = df\n",
    "\n",
    "    # Display the first few rows\n",
    "    print(f\"\\nDataFrame for {file}:\")\n",
    "    display(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3b9aa-db4b-4e55-a0bd-a4c84076eded",
   "metadata": {},
   "source": [
    "QASPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eaf10b4-244a-4e82-83e8-a855d2a7df53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f5337aed3d4f928563a44a8837b012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607b5079694744539e3b7ad6c484c17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.75M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fa0ed67c06405c9f285178ae71de79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f47685cec149268c864b0c2bc7ca62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d480b82e58b446b59a222dac722514fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6eb7ed8a5d4f2a87c34dca84bb18f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qasper_dataset = load_dataset(\"allenai/qasper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc81a56-cb17-4cd9-a330-c28dd8e10e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
      "        num_rows: 888\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
      "        num_rows: 281\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
      "        num_rows: 416\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(qasper_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59df73f3-6d68-4713-a5c9-33f4276cfb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = qasper_dataset[\"train\"]\n",
    "validation_data = qasper_dataset[\"validation\"]\n",
    "test_data = qasper_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71713d2c-4b62-4dc8-9810-d53e4fa7c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "validation_df = pd.DataFrame(validation_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116ba033-0b92-41c1-9ea0-952ffd6d5759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': {30: '2003.08553'},\n",
       " 'title': {30: 'QnAMaker: Data to Bot in 2 Minutes'},\n",
       " 'abstract': {30: 'Having a bot for seamless conversations is a much-desired feature that products and services today seek for their websites and mobile apps. These bots help reduce traffic received by human support significantly by handling frequent and directly answerable known questions. Many such services have huge reference documents such as FAQ pages, which makes it hard for users to browse through this data. A conversation layer over such raw data can lower traffic to human support by a great margin. We demonstrate QnAMaker, a service that creates a conversational layer over semi-structured data such as FAQ pages, product manuals, and support documents. QnAMaker is the popular choice for Extraction and Question-Answering as a service and is used by over 15,000 bots in production. It is also used by search interfaces and not just bots.'},\n",
       " 'full_text': {30: {'section_name': ['Introduction',\n",
       "    'System description ::: Architecture',\n",
       "    'System description ::: Bot Development Process',\n",
       "    'System description ::: Extraction',\n",
       "    'System description ::: Retrieval And Ranking',\n",
       "    'System description ::: Retrieval And Ranking ::: Pre-Processing',\n",
       "    'System description ::: Retrieval And Ranking ::: Features',\n",
       "    'System description ::: Retrieval And Ranking ::: Contextual Features',\n",
       "    'System description ::: Retrieval And Ranking ::: Modeling and Training',\n",
       "    'System description ::: Persona Based Chit-Chat',\n",
       "    'System description ::: Active Learning',\n",
       "    'Evaluation and Insights',\n",
       "    'Demonstration',\n",
       "    'Future Work'],\n",
       "   'paragraphs': [[\"QnAMaker aims to simplify the process of bot creation by extracting Question-Answer (QA) pairs from data given by users into a Knowledge Base (KB) and providing a conversational layer over it. KB here refers to one instance of azure search index, where the extracted QA are stored. Whenever a developer creates a KB using QnAMaker, they automatically get all NLP capabilities required to answer user's queries. There are other systems such as Google's Dialogflow, IBM's Watson Discovery which tries to solve this problem. QnAMaker provides unique features for the ease of development such as the ability to add a persona-based chit-chat layer on top of the bot. Additionally, bot developers get automatic feedback from the system based on end-user traffic and interaction which helps them in enriching the KB; we call this feature active-learning. Our system also allows user to add Multi-Turn structure to KB using hierarchical extraction and contextual ranking. QnAMaker today supports over 35 languages, and is the only system among its competitors to follow a Server-Client architecture; all the KB data rests only in the client's subscription, giving users total control over their data. QnAMaker is part of Microsoft Cognitive Service and currently runs using the Microsoft Azure Stack.\"],\n",
       "    ['As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:',\n",
       "     'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.',\n",
       "     'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.',\n",
       "     'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.',\n",
       "     'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.',\n",
       "     \"Bot: Calls the WebApp with the User's query to get results.\"],\n",
       "    ['Creating a bot is a 3-step process for a bot developer:',\n",
       "     'Create a QnaMaker Resource in Azure: This creates a WebApp with binaries required to run QnAMaker. It also creates an Azure Search Service for populating the index with any given knowledge base, extracted from user data',\n",
       "     'Use Management APIs to Create/Update/Delete your KB: The Create API automatically extracts the QA pairs and sends the Content to WebApp, which indexes it in Azure Search Index. Developers can also add persona-based chat content and synonyms while creating and updating their KBs.',\n",
       "     'Bot Creation: Create a bot using any framework and call the WebApp hosted in Azure to get your queries answered. There are Bot-Framework templates provided for the same.'],\n",
       "    ['The Extraction component is responsible for understanding a given document and extracting potential QA pairs. These QA pairs are in turn used to create a KB to be consumed later on by the QnAMaker WebApp to answer user queries. First, the basic blocks from given documents such as text, lines are extracted. Then the layout of the document such as columns, tables, lists, paragraphs, etc is extracted. This is done using Recursive X-Y cut BIBREF0. Following Layout Understanding, each element is tagged as headers, footers, table of content, index, watermark, table, image, table caption, image caption, heading, heading level, and answers. Agglomerative clustering BIBREF1 is used to identify heading and hierarchy to form an intent tree. Leaf nodes from the hierarchy are considered as QA pairs. In the end, the intent tree is further augmented with entities using CRF-based sequence labeling. Intents that are repeated in and across documents are further augmented with their parent intent, adding more context to resolve potential ambiguity.'],\n",
       "    [\"QnAMaker uses Azure Search Index as it's retrieval layer, followed by re-ranking on top of retrieved results (Figure FIGREF21). Azure Search is based on inverted indexing and TF-IDF scores. Azure Search provides fuzzy matching based on edit-distance, thus making retrieval robust to spelling mistakes. It also incorporates lemmatization and normalization. These indexes can scale up to millions of documents, lowering the burden on QnAMaker WebApp which gets less than 100 results to re-rank.\",\n",
       "     'Different customers may use QnAMaker for different scenarios such as banking task completion, answering FAQs on company policies, or fun and engagement. The number of QAs, length of questions and answers, number of alternate questions per QA can vary significantly across different types of content. Thus, the ranker model needs to use features that are generic enough to be relevant across all use cases.'],\n",
       "    [\"The pre-processing layer uses components such as Language Detection, Lemmatization, Speller, and Word Breaker to normalize user queries. It also removes junk characters and stop-words from the user's query.\"],\n",
       "    ['Going into granular features and the exact empirical formulas used is out of the scope of this paper. The broad level features used while ranking are:',\n",
       "     'WordNet: There are various features generated using WordNet BIBREF2 matching with questions and answers. This takes care of word-level semantics. For instance, if there is information about “price of furniture\" in a KB and the end-user asks about “price of table\", the user will likely get a relevant answer. The scores of these WordNet features are calculated as a function of:',\n",
       "     'Distance of 2 words in the WordNet graph',\n",
       "     'Distance of Lowest Common Hypernym from the root',\n",
       "     'Knowledge-Base word importance (Local IDFs)',\n",
       "     'Global word importance (Global IDFs)',\n",
       "     'This is the most important feature in our model as it has the highest relative feature gain.',\n",
       "     'CDSSM: Convolutional Deep Structured Semantic Models BIBREF3 are used for sentence-level semantic matching. This is a dual encoder model that converts text strings (sentences, queries, predicates, entity mentions, etc) into their vector representations. These models are trained using millions of Bing Query Title Click-Through data. Using the source-model for vectorizing user query and target-model for vectorizing answer, we compute the cosine similarity between these two vectors, giving the relevance of answer corresponding to the query.',\n",
       "     'TF-IDF: Though sentence-to-vector models are trained on huge datasets, they fail to effectively disambiguate KB specific data. This is where a standard TF-IDF BIBREF4 featurizer with local and global IDFs helps.'],\n",
       "    ['We extend the features for contextual ranking by modifying the candidate QAs and user query in these ways:',\n",
       "     '$Query_{modified}$ = Query + Previous Answer; For instance, if user query is “yes\" and the previous answer is “do you want to know about XYZ\", the current query becomes “do you want to know about XYZ yes\".',\n",
       "     'Candidate QnA pairs are appended with its parent Questions and Answers; no contextual information is used from the user\\'s query. For instance, if a candidate QnA has a question “benefits\" and its parent question was “know about XYZ\", the candidate QA\\'s question is changed to “know about XYZ benefits\".',\n",
       "     'The features mentioned in Section SECREF20 are calculated for the above combinations also. These features carry contextual information.'],\n",
       "    ['We use gradient-boosted decision trees as our ranking model to combine all the features. Early stopping BIBREF5 based on Generality-to-Progress ratio is used to decide the number of step trees and Tolerant Pruning BIBREF6 helps prevent overfitting. We follow incremental training if there is small changes in features or training data so that the score distribution is not changed drastically.'],\n",
       "    ['We add support for bot-developers to directly enable handling chit-chat queries like “hi\", “thank you\", “what\\'s up\" in their QnAMaker bots. In addition to chit-chat, we also give bot developers the flexibility to ground responses for such queries in a specific personality: professional, witty, friendly, caring, or enthusiastic. For example, the “Humorous\" personality can be used for a casual bot, whereas a “Professional\" personality is more suited in case of banking FAQs or task-completion bots. There is a list of 100+ predefined intents BIBREF7. There is a curated list of queries for each of these intents, along with a separate query understanding layer for ranking these intents. The arbitration between chit-chat answers and user\\'s knowledge base answers is handled by using a chat-domain classifier BIBREF8.'],\n",
       "    [\"The majority of the KBs are created using existing FAQ pages or manuals but to improve the quality it requires effort from the developers. Active learning generates suggestions based on end-user feedback as well as ranker's implicit signals. For instance, if for a query, CDSSM feature was confident that one QnA should be ranked higher whereas wordnet feature thought other QnA should be ranked higher, active learning system will try to disambiguate it by showing this as a suggestion to the bot developer. To avoid showing similar suggestions to developers, DB-Scan clustering is done which optimizes the number of suggestions shown.\"],\n",
       "    [\"QnAMaker is not domain-specific and can be used for any type of data. To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). Hybrid of deep learning(CDSSM) and machine learning features give our ranking model low computation cost, high explainability and significant F1/AUC score. Based on QnAMaker usage, we observed these trends:\",\n",
       "     'Around 27% of the knowledge bases created use pre-built persona-based chitchat, out of which, $\\\\sim $4% of the knowledge bases are created for chit-chat alone. The highest used personality is Professional which is used in 9% knowledge bases.',\n",
       "     'Around $\\\\sim $25% developers have enabled active learning suggestions. The acceptance to reject ratio for active learning suggestions is 0.31.',\n",
       "     '25.5% of the knowledge bases use one URL as a source while creation. $\\\\sim $41% of the knowledge bases created use different sources like multiple URLs. 15.19% of the knowledge bases use both URL and editorial content as sources. Rest use just editorial content.'],\n",
       "    ['We demonstrate QnAMaker: a service to add a conversational layer over semi-structured user data. In addition to query-answering, we support novel features like personality-grounded chit-chat, active learning based on user-interaction feedback (Figure FIGREF40), and hierarchical extraction for multi-turn conversations (Figure FIGREF41). The goal of the demonstration will be to show how easy it is to create an intelligent bot using QnAMaker. All the demonstrations will be done on the production website Demo Video can be seen here.'],\n",
       "    [\"The system currently doesn't highlight the answer span and does not generate answers taking the KB as grounding. We will be soon supporting Answer Span BIBREF9 and KB-grounded response generation BIBREF10 in QnAMaker. We are also working on user-defined personas for chit-chat (automatically learned from user-documents). We aim to enhance our extraction to be able to work for any unstructured document as well as images. We are also experimenting on improving our ranking system by using semantic vector-based search as our retrieval and transformer-based models for re-ranking.\"]]}},\n",
       " 'qas': {30: {'question': ['What experiments do the authors present to validate their system?',\n",
       "    'How does the conversation layer work?',\n",
       "    'What components is the QnAMaker composed of?'],\n",
       "   'question_id': ['fd0ef5a7b6f62d07776bf672579a99c67e61a568',\n",
       "    '071bcb4b054215054f17db64bfd21f17fd9e1a80',\n",
       "    'f399d5a8dbeec777a858f81dc4dd33a83ba341a2'],\n",
       "   'nlp_background': ['five', 'five', 'five'],\n",
       "   'topic_background': ['', '', ''],\n",
       "   'paper_read': ['', '', ''],\n",
       "   'search_query': ['', '', ''],\n",
       "   'question_writer': ['e8b24c3133e0bec0a6465e1f13acd3a5ed816b37',\n",
       "    'e8b24c3133e0bec0a6465e1f13acd3a5ed816b37',\n",
       "    'e8b24c3133e0bec0a6465e1f13acd3a5ed816b37'],\n",
       "   'answers': [{'answer': [{'unanswerable': False,\n",
       "       'extractive_spans': [\" we measure our system's performance for datasets across various domains\",\n",
       "        'evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs'],\n",
       "       'yes_no': None,\n",
       "       'free_form_answer': '',\n",
       "       'evidence': [\"QnAMaker is not domain-specific and can be used for any type of data. To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). Hybrid of deep learning(CDSSM) and machine learning features give our ranking model low computation cost, high explainability and significant F1/AUC score. Based on QnAMaker usage, we observed these trends:\"],\n",
       "       'highlighted_evidence': [\" To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19).\"]}],\n",
       "     'annotation_id': ['c6aac397b3bf27942363d5b4be00bf094654d366'],\n",
       "     'worker_id': ['258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       "    {'answer': [{'unanswerable': True,\n",
       "       'extractive_spans': [],\n",
       "       'yes_no': None,\n",
       "       'free_form_answer': '',\n",
       "       'evidence': [],\n",
       "       'highlighted_evidence': []}],\n",
       "     'annotation_id': ['3c069b65ef0117a5d5c4ee9ac49ab6709cfbe124'],\n",
       "     'worker_id': ['258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       "    {'answer': [{'unanswerable': False,\n",
       "       'extractive_spans': ['QnAMaker Portal',\n",
       "        'QnaMaker Management APIs',\n",
       "        'Azure Search Index',\n",
       "        'QnaMaker WebApp',\n",
       "        'Bot'],\n",
       "       'yes_no': None,\n",
       "       'free_form_answer': '',\n",
       "       'evidence': ['System description ::: Architecture',\n",
       "        'As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:',\n",
       "        'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.',\n",
       "        'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.',\n",
       "        'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.',\n",
       "        'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.',\n",
       "        \"Bot: Calls the WebApp with the User's query to get results.\"],\n",
       "       'highlighted_evidence': ['System description ::: Architecture',\n",
       "        'The components involved in the process are:',\n",
       "        'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. ',\n",
       "        'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. ',\n",
       "        'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.',\n",
       "        'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. ',\n",
       "        \"Bot: Calls the WebApp with the User's query to get results.\"]},\n",
       "      {'unanswerable': False,\n",
       "       'extractive_spans': ['QnAMaker Portal',\n",
       "        'QnaMaker Management APIs',\n",
       "        'Azure Search Index',\n",
       "        'QnaMaker WebApp',\n",
       "        'Bot'],\n",
       "       'yes_no': None,\n",
       "       'free_form_answer': '',\n",
       "       'evidence': ['As shown in Figure FIGREF4, humans can have two different kinds of roles in the system: Bot-Developers who want to create a bot using the data they have, and End-Users who will chat with the bot(s) created by bot-developers. The components involved in the process are:',\n",
       "        'QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.',\n",
       "        'QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.',\n",
       "        'Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.',\n",
       "        'QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.',\n",
       "        \"Bot: Calls the WebApp with the User's query to get results.\"],\n",
       "       'highlighted_evidence': [\"The components involved in the process are:\\n\\nQnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker. This website is designed to ease the use of management APIs. It also provides a test pane.\\n\\nQnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content. It then passes these QA pairs to the web app to create the Knowledge Base Index.\\n\\nAzure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer.\\n\\nQnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index. WebApp does ranking on top of retrieved results. WebApp also handles feedback management for active learning.\\n\\nBot: Calls the WebApp with the User's query to get results.\"]}],\n",
       "     'annotation_id': ['443426bf61950f89af016a359cbdb0f5f3680d81',\n",
       "      'cc3663b4c97c95bfda1e9a6d64172abea619da01'],\n",
       "     'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "      '258ee4069f740c400c0049a2580945a1cc7f044c']}]}},\n",
       " 'figures_and_tables': {30: {'caption': ['Figure 1: Interactions between various components of QnaMaker, along with their scopes: server-side and client-side',\n",
       "    'Table 1: Retrieval And Ranking Measurements',\n",
       "    'Figure 2: QnAMaker Runtime Pipeline',\n",
       "    'Figure 3: Active Learning Suggestions',\n",
       "    'Figure 4: Multi-Turn Knowledge Base'],\n",
       "   'file': ['2-Figure1-1.png',\n",
       "    '3-Table1-1.png',\n",
       "    '3-Figure2-1.png',\n",
       "    '4-Figure3-1.png',\n",
       "    '4-Figure4-1.png']}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86b2731-19e7-43be-9c3f-5e1f2ec1f0fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>qas</th>\n",
       "      <th>figures_and_tables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1911.10742</td>\n",
       "      <td>End-to-End Trainable Non-Collaborative Dialog ...</td>\n",
       "      <td>End-to-end task-oriented dialog models have ac...</td>\n",
       "      <td>{'section_name': ['Introduction', 'Related Wor...</td>\n",
       "      <td>{'question': ['How big is the ANTISCAM dataset...</td>\n",
       "      <td>{'caption': ['Table 1: Hierarchical intent ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1904.09131</td>\n",
       "      <td>OpenTapioca: Lightweight Entity Linking for Wi...</td>\n",
       "      <td>We propose a simple Named Entity Linking syste...</td>\n",
       "      <td>{'section_name': ['Introduction', 'Particulari...</td>\n",
       "      <td>{'question': ['What is the accuracy of this mo...</td>\n",
       "      <td>{'caption': ['Figure 1: Example of an annotate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1611.06322</td>\n",
       "      <td>Spotting Rumors via Novelty Detection</td>\n",
       "      <td>Rumour detection is hard because the most accu...</td>\n",
       "      <td>{'section_name': ['Introduction', 'Related Wor...</td>\n",
       "      <td>{'question': ['What previous methods do they c...</td>\n",
       "      <td>{'caption': ['Table 1: Excerpt of topics with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1604.02038</td>\n",
       "      <td>Sentence Level Recurrent Topic Model: Letting ...</td>\n",
       "      <td>We propose Sentence Level Recurrent Topic Mode...</td>\n",
       "      <td>{'section_name': ['Introduction', 'Related Wor...</td>\n",
       "      <td>{'question': ['What baselines did they compare...</td>\n",
       "      <td>{'caption': ['Figure 1: The illustration of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1911.04474</td>\n",
       "      <td>TENER: Adapting Transformer Encoder for Named ...</td>\n",
       "      <td>The Bidirectional long short-term memory netwo...</td>\n",
       "      <td>{'section_name': ['Introduction', 'Related Wor...</td>\n",
       "      <td>{'question': ['Which NER dataset do they use?'...</td>\n",
       "      <td>{'caption': ['Figure 1: An example for NER. Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  1911.10742  End-to-End Trainable Non-Collaborative Dialog ...   \n",
       "1  1904.09131  OpenTapioca: Lightweight Entity Linking for Wi...   \n",
       "2  1611.06322              Spotting Rumors via Novelty Detection   \n",
       "3  1604.02038  Sentence Level Recurrent Topic Model: Letting ...   \n",
       "4  1911.04474  TENER: Adapting Transformer Encoder for Named ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  End-to-end task-oriented dialog models have ac...   \n",
       "1  We propose a simple Named Entity Linking syste...   \n",
       "2  Rumour detection is hard because the most accu...   \n",
       "3  We propose Sentence Level Recurrent Topic Mode...   \n",
       "4  The Bidirectional long short-term memory netwo...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  {'section_name': ['Introduction', 'Related Wor...   \n",
       "1  {'section_name': ['Introduction', 'Particulari...   \n",
       "2  {'section_name': ['Introduction', 'Related Wor...   \n",
       "3  {'section_name': ['Introduction', 'Related Wor...   \n",
       "4  {'section_name': ['Introduction', 'Related Wor...   \n",
       "\n",
       "                                                 qas  \\\n",
       "0  {'question': ['How big is the ANTISCAM dataset...   \n",
       "1  {'question': ['What is the accuracy of this mo...   \n",
       "2  {'question': ['What previous methods do they c...   \n",
       "3  {'question': ['What baselines did they compare...   \n",
       "4  {'question': ['Which NER dataset do they use?'...   \n",
       "\n",
       "                                  figures_and_tables  \n",
       "0  {'caption': ['Table 1: Hierarchical intent ann...  \n",
       "1  {'caption': ['Figure 1: Example of an annotate...  \n",
       "2  {'caption': ['Table 1: Excerpt of topics with ...  \n",
       "3  {'caption': ['Figure 1: The illustration of th...  \n",
       "4  {'caption': ['Figure 1: An example for NER. Th...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c4a9f2-c028-44d9-a384-26a6357a9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section_name': ['Introduction', 'Related Work', 'Proposed Method', 'Proposed Method ::: Polarity Function', 'Proposed Method ::: Discourse Relation-Based Event Pairs', 'Proposed Method ::: Discourse Relation-Based Event Pairs ::: AL (Automatically Labeled Pairs)', 'Proposed Method ::: Discourse Relation-Based Event Pairs ::: CA (Cause Pairs)', 'Proposed Method ::: Discourse Relation-Based Event Pairs ::: CO (Concession Pairs)', 'Proposed Method ::: Loss Functions', 'Experiments', 'Experiments ::: Dataset', 'Experiments ::: Dataset ::: AL, CA, and CO', 'Experiments ::: Dataset ::: ACP (ACP Corpus)', 'Experiments ::: Model Configurations', 'Experiments ::: Results and Discussion', 'Conclusion', 'Acknowledgments', 'Appendices ::: Seed Lexicon ::: Positive Words', 'Appendices ::: Seed Lexicon ::: Negative Words', 'Appendices ::: Settings of Encoder ::: BiGRU', 'Appendices ::: Settings of Encoder ::: BERT'], 'paragraphs': [[\"Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).\", 'Learning affective events is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data.', \"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'We trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small.'], ['Learning affective events is closely related to sentiment analysis. Whereas sentiment analysis usually focuses on the polarity of what are described (e.g., movies), we work on how people are typically affected by events. In sentiment analysis, much attention has been paid to compositionality. Word-level polarity BIBREF5, BIBREF6, BIBREF7 and the roles of negation and intensification BIBREF8, BIBREF6, BIBREF9 are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold).', 'Label propagation from seed instances is a common approach to inducing sentiment polarities. While BIBREF5 and BIBREF10 worked on word- and phrase-level polarities, BIBREF0 dealt with event-level polarities. BIBREF5 and BIBREF10 linked instances using co-occurrence information and/or phrase-level coordinations (e.g., “$A$ and $B$” and “$A$ but $B$”). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as event-level counterparts of phrase-level conjunctions.', 'BIBREF0 constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive.', 'Some previous studies made use of document structure to understand the sentiment. BIBREF11 proposed a sentiment-specific pre-training strategy using unlabeled dialog data (tweet-reply pairs). BIBREF12 proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability.', ''], [''], ['', 'Our goal is to learn the polarity function $p(x)$, which predicts the sentiment polarity score of an event $x$. We approximate $p(x)$ by a neural network with the following form:', '${\\\\rm Encoder}$ outputs a vector representation of the event $x$. ${\\\\rm Linear}$ is a fully-connected layer and transforms the representation into a scalar. ${\\\\rm tanh}$ is the hyperbolic tangent and transforms the scalar into a score ranging from $-1$ to 1. In Section SECREF21, we consider two specific implementations of ${\\\\rm Encoder}$.', ''], ['Our method requires a very small seed lexicon and a large raw corpus. We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\\\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession.', 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.', ''], [\"The seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if the discourse relation type is Concession, the former event is given the opposite of the latter's score. They are used as reference scores during training.\", ''], ['The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Cause. We assume the two events have the same polarities.', ''], ['The seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities.', ''], ['Using AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions.', 'We use mean squared error to construct loss functions. For the AL data, the loss function is defined as:', 'where $x_{i1}$ and $x_{i2}$ are the $i$-th pair of the AL data. $r_{i1}$ and $r_{i2}$ are the automatically-assigned scores of $x_{i1}$ and $x_{i2}$, respectively. $N_{\\\\rm AL}$ is the total number of AL pairs, and $\\\\lambda _{\\\\rm AL}$ is a hyperparameter.', 'For the CA data, the loss function is defined as:', '$y_{i1}$ and $y_{i2}$ are the $i$-th pair of the CA pairs. $N_{\\\\rm CA}$ is the total number of CA pairs. $\\\\lambda _{\\\\rm CA}$ and $\\\\mu $ are hyperparameters. The first term makes the scores of the two events closer while the second term prevents the scores from shrinking to zero.', 'The loss function for the CO data is defined analogously:', 'The difference is that the first term makes the scores of the two events distant from each other.', ''], [''], [''], ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', '. 重大な失敗を犯したので、仕事をクビになった。', 'Because [I] made a serious mistake, [I] got fired.', 'From this sentence, we extracted the event pair of “重大な失敗を犯す” ([I] make a serious mistake) and “仕事をクビになる” ([I] get fired), and tagged it with Cause.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], ['We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:', '. 作業が楽だ。', 'The work is easy.', '. 駐車場がない。', 'There is no parking lot.', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'The objective function for supervised training is:', '', 'where $v_i$ is the $i$-th event, $R_i$ is the reference score of $v_i$, and $N_{\\\\rm ACP}$ is the number of the events of the ACP Corpus.', 'To optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\\\le 0$.', ''], ['As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'BERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\\\rm Encoder}$, see Sections SECREF30.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.', ''], ['', \"Table TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event's predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction.\", 'The models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation.', 'Comparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO.', 'Contrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.', 'The result of hyperparameter optimization for the BiGRU encoder was as follows:', 'As the CA and CO pairs were equal in size (Table TABREF16), $\\\\lambda _{\\\\rm CA}$ and $\\\\lambda _{\\\\rm CO}$ were comparable values. $\\\\lambda _{\\\\rm CA}$ was about one-third of $\\\\lambda _{\\\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assumption was in the form of “$\\\\textit {problem}_{\\\\text{negative}}$ causes $\\\\textit {solution}_{\\\\text{positive}}$”:', '. (悪いところがある, よくなるように努力する)', '(there is a bad point, [I] try to improve [it])', 'The polarities of the two events were reversed in spite of the Cause relation, and this lowered the value of $\\\\lambda _{\\\\rm CA}$.', 'Some examples of model outputs are shown in Table TABREF26. The first two examples suggest that our model successfully learned negation without explicit supervision. Similarly, the next two examples differ only in voice but the model correctly recognized that they had opposite polarities. The last two examples share the predicate “落とす\" (drop) and only the objects are different. The second event “肩を落とす\" (lit. drop one\\'s shoulders) is an idiom that expresses a disappointed feeling. The examples demonstrate that our model correctly learned non-compositional expressions.', ''], ['In this paper, we proposed to use discourse relations to effectively propagate polarities of affective events from seeds. Experiments show that, even with a minimal amount of supervision, the proposed method performed well.', 'Although event pairs linked by discourse analysis are shown to be useful, they nevertheless contain noises. Adding linguistically-motivated filtering rules would help improve the performance.'], ['We thank Nobuhiro Kaji for providing the ACP Corpus and Hirokazu Kiyomaru and Yudai Kishimoto for their help in extracting event pairs. This work was partially supported by Yahoo! Japan Corporation.'], ['喜ぶ (rejoice), 嬉しい (be glad), 楽しい (be pleasant), 幸せ (be happy), 感動 (be impressed), 興奮 (be excited), 懐かしい (feel nostalgic), 好き (like), 尊敬 (respect), 安心 (be relieved), 感心 (admire), 落ち着く (be calm), 満足 (be satisfied), 癒される (be healed), and スッキリ (be refreshed).'], ['怒る (get angry), 悲しい (be sad), 寂しい (be lonely), 怖い (be scared), 不安 (feel anxious), 恥ずかしい (be embarrassed), 嫌 (hate), 落ち込む (feel down), 退屈 (be bored), 絶望 (feel hopeless), 辛い (have a hard time), 困る (have trouble), 憂鬱 (be depressed), 心配 (be worried), and 情けない (be sorry).'], ['The dimension of the embedding layer was 256. The embedding layer was initialized with the word embeddings pretrained using the Web corpus. The input sentences were segmented into words by the morphological analyzer Juman++. The vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set.'], ['We used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.']]}\n"
     ]
    }
   ],
   "source": [
    "print(train_df.loc[0, \"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdc1de7-fb0b-48a2-b2af-4af0ebeb89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caption': ['Figure 1: An overview of our method. We focus on pairs of events, the former events and the latter events, which are connected with a discourse relation, CAUSE or CONCESSION. Dropped pronouns are indicated by brackets in English translations. We divide the event pairs into three types: AL, CA, and CO. In AL, the polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). We propagate the latter event’s polarity to the former event. The same polarity as the latter event is used for the discourse relation CAUSE, and the reversed polarity for CONCESSION. In CA and CO, the latter event’s polarity is not known. Depending on the discourse relation, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2.', 'Table 1: Statistics of the AL, CA, and CO datasets.', 'Table 2: Details of the ACP dataset.', 'Table 5: Examples of polarity scores predicted by the BiGRU model trained with AL+CA+CO.', 'Table 3: Performance of various models on the ACP test set.', 'Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'file': ['2-Figure1-1.png', '4-Table1-1.png', '4-Table2-1.png', '5-Table5-1.png', '5-Table3-1.png', '5-Table4-1.png']}\n"
     ]
    }
   ],
   "source": [
    "print(train_df.loc[0, \"figures_and_tables\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ed012c-421a-42f5-b84e-0104222c413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['What is the seed lexicon?', 'What are the results?', 'How are relations used to propagate polarity?', 'How big is the Japanese data?', 'What are labels available in dataset for supervision?', 'How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?', 'How does their model learn using mostly raw data?', 'How big is seed lexicon used for training?', 'How large is raw corpus used for training?'], 'question_id': ['753990d0b621d390ed58f20c4d9e4f065f0dc672', '9d578ddccc27dd849244d632dd0f6bf27348ad81', '02e4bf719b1a504e385c35c6186742e720bcb281', '44c4bd6decc86f1091b5fc0728873d9324cdde4e', '86abeff85f3db79cf87a8c993e5e5aa61226dc98', 'c029deb7f99756d2669abad0a349d917428e9c12', '39f8db10d949c6b477fa4b51e7c184016505884f', 'd0bc782961567dc1dd7e074b621a6d6be44bb5b4', 'a592498ba2fac994cd6fad7372836f0adb37e22a'], 'nlp_background': ['two', 'two', 'two', 'two', 'zero', 'zero', 'zero', 'zero', 'zero'], 'topic_background': ['unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar'], 'paper_read': ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no'], 'search_query': ['', '', '', '', '', '', '', '', ''], 'question_writer': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c'], 'answers': [{'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}, {'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}], 'annotation_id': ['31e85022a847f37c15fd0415f3c450c74c8e4755', '95da0a6e1b08db74a405c6a71067c9b272a50ff5'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.']}], 'annotation_id': ['1e5e867244ea656c4b7632628086209cf9bae5fa'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.']}], 'annotation_id': ['49a78a07d2eed545556a835ccf2eb40e5eee9801', 'acd6d15bd67f4b1496ee8af1c93c33e7d59c89e1'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well.', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.']}], 'annotation_id': ['36926a4c9e14352c91111150aa4c6edcc5c0770f', '75b6dd28ccab20a70087635d89c2b22d0e99095c'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['negative', 'positive'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).\"], 'highlighted_evidence': ['In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).']}], 'annotation_id': ['2d8c7df145c37aad905e48f64d8caa69e54434d4'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '3%', 'evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.']}], 'annotation_id': ['df4372b2e8d9bb2039a5582f192768953b01d904'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive).\"]}], 'annotation_id': ['5c5bbc8af91c16af89b4ddd57ee6834be018e4e7'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '30 words', 'evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. ']}], 'annotation_id': ['0206f2131f64a3e02498cedad1250971b78ffd0c'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['100 million sentences'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.']}], 'annotation_id': ['c36bad2758c4f9866d64c357c475d370595d937f'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}]}\n"
     ]
    }
   ],
   "source": [
    "print(train_df.loc[0, \"qas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978804f-1de6-4a8b-9b74-77e5220c3336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
