{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102535b3-91fe-4541-9ba6-4004b9a494de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76ffb27-29fe-4221-84ca-15622c82a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Any, Tuple\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from enum import Enum\n",
    "import openai\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194536e1-8482-44eb-a0d3-e9d15f7b0172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970144baca774702bee637dc623bb50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from gritlm import GritLM\n",
    "import sentence_transformers\n",
    "from datasets import load_dataset\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e78a96-9f08-46c5-8089-e80084025c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = query_data.to_pandas()\n",
    "corpus_clean_df = corpus_clean_data.to_pandas()\n",
    "corpus_s2orc_df = corpus_s2orc_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dab5b0d-1a67-420f-ba22-60a6625c305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_set</th>\n",
       "      <th>query</th>\n",
       "      <th>specificity</th>\n",
       "      <th>quality</th>\n",
       "      <th>corpusids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any research papers on methods to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[202719327]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any resources available for translat...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[227231792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any studies that explore post-hoc te...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[226254579, 204976362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any tools or studies that have focus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[10961392, 12160022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there papers that propose contextualized c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[233296182]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_set                                              query  specificity  \\\n",
       "0  inline_acl  Are there any research papers on methods to co...            0   \n",
       "1  inline_acl  Are there any resources available for translat...            1   \n",
       "2  inline_acl  Are there any studies that explore post-hoc te...            0   \n",
       "3  inline_acl  Are there any tools or studies that have focus...            1   \n",
       "4  inline_acl  Are there papers that propose contextualized c...            1   \n",
       "\n",
       "   quality               corpusids  \n",
       "0        2             [202719327]  \n",
       "1        2             [227231792]  \n",
       "2        2  [226254579, 204976362]  \n",
       "3        2    [10961392, 12160022]  \n",
       "4        2             [233296182]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3caf1a-fe54-42f6-9330-8d35998540e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>citations</th>\n",
       "      <th>full_paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252715594</td>\n",
       "      <td>PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...</td>\n",
       "      <td>We present Phenaki, a model capable of realist...</td>\n",
       "      <td>[6628106, 174802916, 238582653]</td>\n",
       "      <td>PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13002849</td>\n",
       "      <td>MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS</td>\n",
       "      <td>Although Generative Adversarial Networks achie...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MODE REGULARIZED GENERATIVE ADVERSARIAL NETWOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239998253</td>\n",
       "      <td>What Do We Mean by Generalization in Federated...</td>\n",
       "      <td>Federated learning data is drawn from a distri...</td>\n",
       "      <td>[235613568, 231924480, 211678094, 195798643, 4...</td>\n",
       "      <td>What Do We Mean by Generalization in Federated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62841605</td>\n",
       "      <td>SPREADING VECTORS FOR SIMILARITY SEARCH</td>\n",
       "      <td>Discretizing multi-dimensional data distributi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>SPREADING VECTORS FOR SIMILARITY SEARCH\\n\\n\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253237531</td>\n",
       "      <td>MACHINE UNLEARNING OF FEDERATED CLUSTERS</td>\n",
       "      <td>Federated clustering (FC) is an unsupervised l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MACHINE UNLEARNING OF FEDERATED CLUSTERS\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpusid                                              title  \\\n",
       "0  252715594  PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...   \n",
       "1   13002849   MODE REGULARIZED GENERATIVE ADVERSARIAL NETWORKS   \n",
       "2  239998253  What Do We Mean by Generalization in Federated...   \n",
       "3   62841605            SPREADING VECTORS FOR SIMILARITY SEARCH   \n",
       "4  253237531           MACHINE UNLEARNING OF FEDERATED CLUSTERS   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We present Phenaki, a model capable of realist...   \n",
       "1  Although Generative Adversarial Networks achie...   \n",
       "2  Federated learning data is drawn from a distri...   \n",
       "3  Discretizing multi-dimensional data distributi...   \n",
       "4  Federated clustering (FC) is an unsupervised l...   \n",
       "\n",
       "                                           citations  \\\n",
       "0                    [6628106, 174802916, 238582653]   \n",
       "1                                                 []   \n",
       "2  [235613568, 231924480, 211678094, 195798643, 4...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                          full_paper  \n",
       "0  PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM...  \n",
       "1  MODE REGULARIZED GENERATIVE ADVERSARIAL NETWOR...  \n",
       "2  What Do We Mean by Generalization in Federated...  \n",
       "3  SPREADING VECTORS FOR SIMILARITY SEARCH\\n\\n\\nA...  \n",
       "4  MACHINE UNLEARNING OF FEDERATED CLUSTERS\\n\\n\\n...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d23cdc0-b3f4-444a-84a2-669a35d9f202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>externalids</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252715594</td>\n",
       "      <td>{'acl': None, 'arxiv': '2210.02399', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":2761,\"st...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13002849</td>\n",
       "      <td>{'acl': None, 'arxiv': '1612.02136', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1726,\"st...</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239998253</td>\n",
       "      <td>{'acl': None, 'arxiv': None, 'dblp': 'conf/icl...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1082,\"st...</td>\n",
       "      <td>2022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62841605</td>\n",
       "      <td>{'acl': None, 'arxiv': '1806.03198', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":1503,\"st...</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253237531</td>\n",
       "      <td>{'acl': None, 'arxiv': '2210.16424', 'dblp': '...</td>\n",
       "      <td>{'annotations': {'abstract': '[{\"end\":2886,\"st...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpusid                                        externalids  \\\n",
       "0  252715594  {'acl': None, 'arxiv': '2210.02399', 'dblp': '...   \n",
       "1   13002849  {'acl': None, 'arxiv': '1612.02136', 'dblp': '...   \n",
       "2  239998253  {'acl': None, 'arxiv': None, 'dblp': 'conf/icl...   \n",
       "3   62841605  {'acl': None, 'arxiv': '1806.03198', 'dblp': '...   \n",
       "4  253237531  {'acl': None, 'arxiv': '2210.16424', 'dblp': '...   \n",
       "\n",
       "                                             content    year  \n",
       "0  {'annotations': {'abstract': '[{\"end\":2761,\"st...  2023.0  \n",
       "1  {'annotations': {'abstract': '[{\"end\":1726,\"st...  2017.0  \n",
       "2  {'annotations': {'abstract': '[{\"end\":1082,\"st...  2022.0  \n",
       "3  {'annotations': {'abstract': '[{\"end\":1503,\"st...  2019.0  \n",
       "4  {'annotations': {'abstract': '[{\"end\":2886,\"st...  2023.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_s2orc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b3f63c-27ae-4271-8068-7fd96682e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading json files into 'data'\n",
    "def read_json(filename: str, silent: bool = False) -> List[Any]:\n",
    "    with open(filename, 'r') as file:\n",
    "        if filename.endswith(\".json\"):\n",
    "            data = json.load(file)\n",
    "        elif filename.endswith(\".jsonl\"):\n",
    "            data = [json.loads(line) for line in file]\n",
    "        else:\n",
    "            raise ValueError(\"Input file must be either a .json or .jsonl file\")\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"Loaded {len(data)} records from {filename}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833729b6-7dbd-4667-b7b0-11c9061bbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for writing data into json file format\n",
    "def write_json(data: List[Any], filename: str, silent: bool = False) -> None:\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "    elif filename.endswith(\".jsonl\"):\n",
    "        with open(filename, 'w') as file:\n",
    "            for item in data:\n",
    "                file.write(json.dumps(item) + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(\"Output file must be either a .json or .jsonl file\")\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"Saved {len(data)} records to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5df46df-2b98-44ad-8ceb-da259226b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved: List[int], relevant_docs: List[int]) -> float:\n",
    "    \"\"\"Calculate recall metric.\"\"\"\n",
    "    num_relevant_retrieved = len(set(retrieved).intersection(set(relevant_docs)))\n",
    "    num_relevant = len(relevant_docs)\n",
    "    return num_relevant_retrieved / num_relevant if num_relevant > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2ee7d2-7b43-42c9-a85a-6b4bb511d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(retrieved: List[int], relevant_docs: List[int]) -> float:\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain.\"\"\"\n",
    "    dcg = sum(1 / (idx + 1) for idx, docid in enumerate(retrieved) if docid in relevant_docs)\n",
    "    idcg = sum(1 / (idx + 1) for idx in range(len(relevant_docs)))\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d632c794-610e-440f-a551-c1c95d84ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TextType(Enum):\n",
    "    KEY = 1\n",
    "    QUERY = 2\n",
    "\n",
    "# KVStore Class\n",
    "class KVStore:\n",
    "    \"\"\"Key-Value store base class for retrieval systems.\"\"\"\n",
    "    def __init__(self, index_name: str, index_type: str) -> None:\n",
    "        self.index_name = index_name\n",
    "        self.index_type = index_type\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.keys)\n",
    "\n",
    "    def _encode(self, text: str, type: TextType) -> Any:\n",
    "        return self._encode_batch([text], type, show_progress_bar=False)[0]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.keys = []\n",
    "        self.encoded_keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        if len(self.keys) > 0:\n",
    "            raise ValueError(\"Index is not empty. Please create a new index or clear the existing one.\")\n",
    "\n",
    "        for key, value in tqdm(key_value_pairs.items(), desc=f\"Creating {self.index_name} index\"):\n",
    "            self.keys.append(key)\n",
    "            self.values.append(value)\n",
    "        self.encoded_keys = self._encode_batch(self.keys, TextType.KEY)\n",
    "\n",
    "    def save(self, dir_name: str) -> None:\n",
    "        save_dict = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key[0] != \"_\":  # Ignore private attributes\n",
    "                save_dict[key] = value\n",
    "\n",
    "        print(f\"Saving index to {os.path.join(dir_name, f'{self.index_name}.{self.index_type}')}\")\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        with open(os.path.join(dir_name, f\"{self.index_name}.{self.index_type}\"), 'wb') as file:\n",
    "            pickle.dump(save_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def query(self, query_text: str, n: int, return_keys: bool = False) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Query the index and return the top-N results.\n",
    "\n",
    "        :param query_text: The query text.\n",
    "        :param n: The number of top results to return.\n",
    "        :param return_keys: Whether to return keys with values.\n",
    "        :return: A list of results.\n",
    "        \"\"\"\n",
    "        encoded_query = self._encode(query_text, TextType.QUERY)\n",
    "        indices = self._query(encoded_query, n)\n",
    "        if return_keys:\n",
    "            results = [(self.keys[i], self.values[i]) for i in indices]\n",
    "        else:\n",
    "            results = [self.values[i] for i in indices]\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55e903b-40c6-4188-bf02-31eacdcb4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTR(KVStore):\n",
    "    def __init__(self, index_name: str, model_path: str = \"sentence-transformers/gtr-t5-large\"):\n",
    "        super().__init__(index_name, 'gtr')\n",
    "        self.model_path = model_path\n",
    "        self._model = sentence_transformers.SentenceTransformer(model_path, device=\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]:\n",
    "        return self._model.encode(texts, batch_size=256, show_progress_bar=show_progress_bar).astype(np.float16)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        super().load(path)\n",
    "        self._model = sentence_transformers.SentenceTransformer(model_path, device=\"cpu\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "      cosine_similarities = cosine_similarity([encoded_query], self.encoded_keys)[0]\n",
    "      top_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "      return top_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "477216c1-dad6-4703-b6a8-c3103678b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "\n",
    "class BM25(KVStore):\n",
    "    def __init__(self, index_name: str):\n",
    "        super().__init__(index_name, 'bm25')\n",
    "\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        self.index = None   # BM25 index\n",
    "\n",
    "    def _encode_batch(self, texts: str, type: TextType, show_progress_bar: bool = True) -> List[str]:\n",
    "        # lowercase, tokenize, remove stopwords, and stem\n",
    "        tokens_list = []\n",
    "        for text in tqdm(texts, disable=not show_progress_bar):\n",
    "            tokens = self._tokenizer(text.lower())\n",
    "            tokens = [token for token in tokens if token not in self._stop_words]\n",
    "            tokens = [self._stemmer(token) for token in tokens]\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "\n",
    "    def _query(self, encoded_query: List[str], n: int) -> List[int]:\n",
    "        top_indices = np.argsort(self.index.get_scores(encoded_query))[::-1][:n].tolist()\n",
    "        return top_indices\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        super().clear()\n",
    "        self.index = None\n",
    "    \n",
    "    def create_index(self, key_value_pairs: List[Tuple[str, Any]]) -> None:\n",
    "        super().create_index(key_value_pairs)\n",
    "        self.index = BM25Okapi(self.encoded_keys)\n",
    "    \n",
    "    def load(self, dir_name: str) -> None:\n",
    "        super().load(dir_name)\n",
    "        self._tokenizer = nltk.word_tokenize\n",
    "        self._stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        self._stemmer = nltk.stem.PorterStemmer().stem\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "658b3993-ab9f-41d7-b9e3-06815dc26bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5(KVStore):\n",
    "    def __init__(self, index_name: str, model_path: str = \"intfloat/e5-large-v2\"):\n",
    "        super().__init__(index_name, 'e5')\n",
    "        self.model_path = model_path\n",
    "        self._model = sentence_transformers.SentenceTransformer(model_path, device=\"cuda\", cache_folder=utils.get_cache_dir()).bfloat16()\n",
    "    \n",
    "    def _format_text(self, text: str, type: TextType) -> str:\n",
    "        if type == TextType.KEY:\n",
    "            text = \"passage: \" + text\n",
    "        elif type == TextType.QUERY:\n",
    "            text = \"query: \" + text\n",
    "        else:\n",
    "            raise ValueError(\"Invalid TextType\")\n",
    "        return text\n",
    "    \n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]:\n",
    "        texts = [self._format_text(text, type) for text in texts]\n",
    "        return self._model.encode(texts, batch_size=256, normalize_embeddings=True, show_progress_bar=show_progress_bar).astype(np.float16)\n",
    "    \n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "        cosine_similarities = cosine_similarity([encoded_query], self.encoded_keys)[0]\n",
    "        top_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        return top_indices\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        super().load(path)\n",
    "        self._model = sentence_transformers.SentenceTransformer(self.model_path, device=\"cuda\", cache_folder=utils.get_cache_dir())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "651c078d-e3fc-4175-b2e7-5c561a0401dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRIT(KVStore):\n",
    "    def __init__(self, index_name: str, raw_instruction: str, model_path: str = \"GritLM/GritLM-7B\"):\n",
    "        super().__init__(index_name, 'grit')\n",
    "        self.model_path = model_path\n",
    "        self.raw_instruction = raw_instruction\n",
    "        self._model = GritLM(model_path, torch_dtype=\"auto\", device_map=\"auto\", mode=\"embedding\")\n",
    "    \n",
    "    def _get_instruction(self, type: TextType) -> str:\n",
    "        if type == TextType.KEY:\n",
    "            return \"<|embed|>\\n\"\n",
    "        elif type == TextType.QUERY:\n",
    "            return \"<|user|>\\n\" + self.raw_instruction + \"\\n<|embed|>\\n\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid TextType\")\n",
    "    \n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]:\n",
    "        return self._model.encode(texts, batch_size=256, instruction=self._get_instruction(type), show_progress_bar=show_progress_bar).astype(np.float16)\n",
    "    \n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "        try:\n",
    "            cosine_similarities = cosine_similarity([encoded_query], self.encoded_keys)[0]\n",
    "        except:\n",
    "            for i, encoded_key in enumerate(self.encoded_keys):\n",
    "                if np.any(np.isnan(encoded_key)):\n",
    "                    self.encoded_keys[i] = np.zeros_like(encoded_key)\n",
    "            cosine_similarities = cosine_similarity([encoded_query], self.encoded_keys)[0]\n",
    "        top_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        return top_indices\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        super().load(path)\n",
    "        self._model = GritLM(self.model_path, torch_dtype=\"auto\", device_map=\"auto\", mode=\"embedding\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc217406-30a0-4d20-a373-66a8b8c54ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instructor(KVStore):\n",
    "    def __init__(self, index_name: str, key_instruction: str, query_instruction: str, model_path: str = \"hkunlp/instructor-xl\"):\n",
    "        super().__init__(index_name, 'instructor')\n",
    "        self.model_path = model_path\n",
    "        self.key_instruction = key_instruction\n",
    "        self.query_instruction = query_instruction\n",
    "        self._model = INSTRUCTOR(model_path, device=\"cuda\", cache_folder=utils.get_cache_dir())\n",
    "    \n",
    "    def _format_text(self, text: str, type: TextType) -> List[str]:\n",
    "        if type == TextType.KEY:\n",
    "            return [self.key_instruction, text]\n",
    "        elif type == TextType.QUERY:\n",
    "            return [self.query_instruction, text]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid TextType\")\n",
    "    \n",
    "    def _encode_batch(self, texts: List[str], type: TextType, show_progress_bar: bool = True) -> List[Any]:\n",
    "        texts = [self._format_text(text, type) for text in texts]\n",
    "        return self._model.encode(texts, batch_size=128, normalize_embeddings=True, show_progress_bar=show_progress_bar).astype(np.float16)\n",
    "    \n",
    "    def _query(self, encoded_query: Any, n: int) -> List[int]:\n",
    "        cosine_similarities = cosine_similarity([encoded_query], self.encoded_keys)[0]\n",
    "        top_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        return top_indices\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        super().load(path)\n",
    "        self._model = INSTRUCTOR(self.model_path, device=\"cuda\", cache_folder=utils.get_cache_dir())\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acf875-54ee-4b72-914a-341a1928798c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a6f66-ea56-4820-9ab5-07afc08614e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d4fcf-3d59-4e5b-9115-fc6d36e58fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c284406-a179-49cb-b3ac-5bf2910e87e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f10d3-6b09-4ea4-b86b-cf4b9e1cb5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10eeb788-ce26-4c1d-b545-5b80f0af04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_index(index_name: str, dataset_path: str, top_k: int = 200):\n",
    "    query_set = [query for query in datasets.load_dataset(dataset_path, \"query\", split=\"full\")]\n",
    "    index = load_index(index_name)\n",
    "    for query in tqdm(query_set):\n",
    "        query_text = query[\"query\"]\n",
    "        top_k_results = index.query(query_text, top_k)\n",
    "        query[\"retrieved\"] = top_k_results\n",
    "    return query_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db3f58a8-69d9-47ba-b623-b0184bb4fb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a74283-6e05-4ce3-b93c-7866833e1f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving index to retrieval_indices/test_index.gtr\n"
     ]
    }
   ],
   "source": [
    "# Test KVStore save method\n",
    "kv_store = KVStore(\"test_index\", \"gtr\")\n",
    "kv_store.keys = [\"Document 1\", \"Document 2\"]\n",
    "kv_store.values = [{\"content\": \"This is the first document.\"}, {\"content\": \"This is the second document.\"}]\n",
    "kv_store.save(\"retrieval_indices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cff16686-e33c-438b-b3d1-2010a3f61872",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'nunique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_set\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnunique\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'nunique'"
     ]
    }
   ],
   "source": [
    "query_data['query_set'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e75eb574-40fc-4835-8f31-64405c3739ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data=query_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc76171a-a42c-417f-a956-55ebfbb4acd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data['query_set'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b689cf-8908-483f-af33-231e76ad04d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
