{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d82a9ab-395a-4e31-a628-c88b72720b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32eeb882-226b-4899-a33f-1b1234e5a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24fc29-1308-4786-a68e-ee2ef06ebf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b937dea-09ec-4559-ba7d-3e5f7b3a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key=\"\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fcee4-9a84-474d-97f2-4d2a973d9f15",
   "metadata": {},
   "source": [
    "Option 1: Two-Stage (Start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50319a-3a32-4f13-b04f-77f6eabad783",
   "metadata": {},
   "source": [
    "        Example 2:\n",
    "        Question: What is the role of reinforcement learning in AI?\n",
    "        Answer: Reinforcement learning (RL) enables AI agents to learn optimal behaviors through trial and error by maximizing cumulative rewards from environment interactions. It is pivotal in sequential decision-making tasks like game playing (e.g., AlphaGo), robotics, and autonomous systems. RL algorithms iteratively improve policies, balancing exploration and exploitation [Sutton & Barto, 2018; corpus: 2259]. Recent advances integrate deep learning for complex environments [Arulkumaran et al., 2020; corpus: 8403].\n",
    "\n",
    "        Example 3:\n",
    "        Question: How effective are BERT embeddings for document retrieval?\n",
    "        Answer: BBERT embeddings enhance document retrieval by capturing contextual semantics, outperforming traditional methods like TF-IDF in understanding query-document relevance. However, their effectiveness depends on fine-tuning for specific tasks and computational resources. BERT’s fixed-length inputs can truncate long documents, limiting full-context use. Hybrid approaches (e.g., BERT with BM25) often yield optimal results [Nogueira et al., 2019; corpus: 8596]. Domain adaptation further boosts performance [Zhan et al., 2021; corpus: 23884].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5fa8b-cd4a-474b-adc9-690b5d5cf9dc",
   "metadata": {},
   "source": [
    "Since you have a fervour for well-cited answers, I trust your ability to do this job well and look forward to reading your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1b85d7-399d-45a0-8bf8-bd599510f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query \n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "\n",
    "\n",
    "        Now, process this query\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"few-shot + Reward\":\n",
    "        return f\"\"\"\n",
    "        You are well-known for answering questions based on verifiable sources.\n",
    "        If you continue to do so, your reputation would be elevated. \n",
    "        Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "\n",
    "\n",
    "        Now, process this query\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"few-shot + Threat\":\n",
    "        return f\"\"\"\n",
    "        You need to prove your ability to answer questions based on verifiable sources.\n",
    "        If you fail to do so, your reputation will deteriorate. \n",
    "        Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "\n",
    "\n",
    "        Now, process this query\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard)\":\n",
    "        return f\"\"\"Let's break down the question step by step:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard) + Reward\":\n",
    "        return f\"\"\"\n",
    "        You are well-known for answering questions based on verifiable sources.\n",
    "        If you continue to do so, your reputation would be elevated. \n",
    "        Let's break down the question step by step:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard) + Threat\":\n",
    "        return f\"\"\"\n",
    "        You need to prove your ability to answer questions based on verifiable sources.\n",
    "        If you fail to do so, your reputation will deteriorate. \n",
    "        Let's break down the question step by step:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Role\":\n",
    "        return f\"\"\"You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Now, process this query:\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.        \n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        Please roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Instruction + Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Positive) + Example\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "        The following is an example of how to answer:\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Negative) + Example\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "        The following is an example of how to answer:\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Neutral)\":\n",
    "        return f\"\"\"You are an AI assistant tasked with answering a research question using a set of documents retrieved from a scientific corpus. Your answer must be concise, factually accurate, and include all relevant corpus citations. \n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers and excellent citation accuracy. Citing the correct sources is one of the most important ways you demonstrate trustworthiness, attention to detail, and helpfulness. Providing accurate, complete citations helps users understand and trust your response. Be sure to cite every relevant document, and include all sources that support your answer.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated for factual accuracy and proper attribution. Providing incomplete, incorrect, or hallucinated citations will result in penalization and reduce the trustworthiness of your response. Only cite sources that are directly relevant and verifiable from the retrieved documents. Do not fabricate or omit citations.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Academic Aspiration)\":\n",
    "        return f\"\"\"You are a rising academic in the field of artificial intelligence. Your work is beginning to attract attention, and every paper you write contributes to your growing reputation. It’s essential that your answers are accurate, well-supported, and ethically cited. Misattributed or missing citations can damage your credibility and compromise the trust of the research community. Cite with care, and write like your future depends on it.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification\":\n",
    "        return f\"\"\"\n",
    "    Generate an answer to the question using the documents. Then:\n",
    "    1. List each factual claim and its citation (e.g., \"Claim: ... [corpus ID]\").\n",
    "    2. Verify if the citation supports the claim. If not, state: \"Unsupported: [claim]\".\n",
    "    3. Revise the answer to remove unsupported claims.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "    elif strategy == \"self-verification + Reward\":\n",
    "        return f\"\"\"\n",
    "    You are well-known for answering questions based on verifiable sources.\n",
    "    If you continue to do so, your reputation would be elevated. \n",
    "    Generate an answer to the question using the documents. Then:\n",
    "    1. List each factual claim and its citation (e.g., \"Claim: ... [corpus ID]\").\n",
    "    2. Verify if the citation supports the claim. If not, state: \"Unsupported: [claim]\".\n",
    "    3. Revise the answer to remove unsupported claims.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification + Threat\":\n",
    "        return f\"\"\"\n",
    "    You need to prove your ability to answer questions based on verifiable sources.\n",
    "    If you fail to do so, your reputation will deteriorate.  \n",
    "    Generate an answer to the question using the documents. Then:\n",
    "    1. List each factual claim and its citation (e.g., \"Claim: ... [corpus ID]\").\n",
    "    2. Verify if the citation supports the claim. If not, state: \"Unsupported: [claim]\".\n",
    "    3. Revise the answer to remove unsupported claims.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification\":\n",
    "        return f\"\"\"\n",
    "        Answer the question as follows:\n",
    "        1. **Draft**: Write an initial answer with citations (e.g., [corpus ID]).\n",
    "        2. **Verify**: For each claim, check if the cited document supports it. If not, find a valid citation.\n",
    "        3. **Revise**: Rewrite the answer using only verified citations. Highlight changes.\n",
    "        4. **Finalize**: Output the revised answer and list verified citations.\n",
    "\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification + Reward\":\n",
    "        return f\"\"\"\n",
    "        You are well-known for answering questions based on verifiable sources.\n",
    "        If you continue to do so, your reputation would be elevated. \n",
    "        Answer the question as follows:\n",
    "        1. **Draft**: Write an initial answer with citations (e.g., [corpus ID]).\n",
    "        2. **Verify**: For each claim, check if the cited document supports it. If not, find a valid citation.\n",
    "        3. **Revise**: Rewrite the answer using only verified citations. Highlight changes.\n",
    "        4. **Finalize**: Output the revised answer and list verified citations.\n",
    "\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification + Threat\":\n",
    "        return f\"\"\"\n",
    "        You need to prove your ability to answer questions based on verifiable sources.\n",
    "        If you fail to do so, your reputation will deteriorate. \n",
    "        Answer the question as follows:\n",
    "        1. **Draft**: Write an initial answer with citations (e.g., [corpus ID]).\n",
    "        2. **Verify**: For each claim, check if the cited document supports it. If not, find a valid citation.\n",
    "        3. **Revise**: Rewrite the answer using only verified citations. Highlight changes.\n",
    "        4. **Finalize**: Output the revised answer and list verified citations.\n",
    "\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation\":\n",
    "        return f\"\"\"Answer the question step by step using only the provided documents (some may be irrelevant). \n",
    "        Cite the relevant document ID for each factual claim.\n",
    "        Conclude with 'The answer is: [answer]'. \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation + Reward\":\n",
    "        return f\"\"\"\n",
    "        You are well-known for answering questions based on verifiable sources.\n",
    "        If you continue to do so, your reputation would be elevated. \n",
    "        Answer the question step by step using only the provided documents (some may be irrelevant). \n",
    "        Cite the relevant document ID for each factual claim.\n",
    "        Conclude with 'The answer is: [answer]'. \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation + Threat\":\n",
    "        return f\"\"\"\n",
    "        You need to prove your ability to answer questions based on verifiable sources.\n",
    "        If you fail to do so, your reputation will deteriorate. \n",
    "        Answer the question step by step using only the provided documents (some may be irrelevant). \n",
    "        Cite the relevant document ID for each factual claim.\n",
    "        Conclude with 'The answer is: [answer]'. \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote\":\n",
    "        return f\"\"\"Answer the question step by step using only the provided documents (some may be irrelevant).\n",
    "        For each step, include an exact quote from the document and cite it like ‘quote’\n",
    "        Conclude with \"The answer is: [answer]\".\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote + Reward\":\n",
    "        return f\"\"\"\n",
    "        You are well-known for answering questions based on verifiable sources.\n",
    "        If you continue to do so, your reputation would be elevated. \n",
    "        Answer the question step by step using only the provided documents (some may be irrelevant).\n",
    "        For each step, include an exact quote from the document and cite it like ‘quote’\n",
    "        Conclude with \"The answer is: [answer]\".\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote + Threat\":\n",
    "        return f\"\"\"\n",
    "        You need to prove your ability to answer questions based on verifiable sources.\n",
    "        If you fail to do so, your reputation will deteriorate. \n",
    "        Answer the question step by step using only the provided documents (some may be irrelevant).\n",
    "        For each step, include an exact quote from the document and cite it like ‘quote’\n",
    "        Conclude with \"The answer is: [answer]\".\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Academic Aspiration (2-stage)\":\n",
    "        return f\"\"\"You are a rising academic in the field of artificial intelligence. Your work is beginning to attract attention, and every paper you write contributes to your growing reputation. It’s essential that your answers are accurate, well-supported, and ethically cited. Misattributed or missing citations can damage your credibility and compromise the trust of the research community. Cite with care, and write like your future depends on it. \n",
    "    Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated for factual accuracy and proper attribution. Providing incomplete, incorrect, or hallucinated citations will result in penalization and reduce the trustworthiness of your response. Only cite sources that are directly relevant and verifiable from the retrieved documents.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers and excellent citation accuracy. Citing the correct sources is one of the most important ways you demonstrate trustworthiness, attention to detail, and helpfulness. Providing accurate, complete citations helps users understand and trust your response.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (role-gen)\":\n",
    "        return f\"\"\"You are an expert at role selection for problem solving. Generate a professional role description following this format:\n",
    "\n",
    "A: This question is a [DOMAIN] problem involving [KEY CONCEPTS]. To better solve it, I will act as a [SPECIFIC ROLE] who [EXPERTISE DESCRIPTION].\n",
    "\n",
    "Examples:\n",
    "Q: Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
    "A: This question is a neuroscience problem involving cell biology and migration. To better solve it, I will act as a neuroscientist who specializes in the study of the brain and its cellular behaviors.\n",
    "\n",
    "Q: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "A: This question is a computer vision problem involving architectural comparisons. To better solve it, I will act as a deep learning researcher who focuses on model architecture analysis and performance benchmarking.\n",
    "\n",
    "Now generate a role description for:\n",
    "Q: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (answer)\":\n",
    "        # This will be populated dynamically with the generated role\n",
    "        return f\"\"\"{query['generated_role']}\n",
    "\n",
    "Using this expertise and the retrieved documents below, provide a comprehensive answer:\n",
    "{query}\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage1)\":\n",
    "        return f\"\"\"You are an expert research assistant. Paraphrase specific questions into fundamental scientific inquiries:\n",
    "\n",
    "        Examples:\n",
    "        Original: How does contrastive learning improve representation learning in vision transformers?\n",
    "        Stepback: What are the fundamental mechanisms by which self-supervised learning techniques enhance representation learning in transformer architectures?\n",
    "\n",
    "        Original: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "        Stepback: What architectural differences between convolutional networks and transformer models affect computer vision performance metrics?\n",
    "\n",
    "        Original: How effective is chain-of-thought prompting for mathematical reasoning in PaLM-2?\n",
    "        Stepback: What cognitive science principles underlie the effectiveness of decomposition strategies in large language model reasoning?\n",
    "\n",
    "        Original: What causes gradient instability in 4-bit quantized LLM fine-tuning?\n",
    "        Stepback: What numerical precision challenges emerge during post-training quantization of neural network parameters?\n",
    "\n",
    "        Original: How does Med-PaLM 2 achieve state-of-the-art on medical QA benchmarks?\n",
    "        Stepback: What architectural adaptations and training strategies improve language model performance in domain-specific knowledge tasks?\n",
    "\n",
    "        Now generate a step-back question for:\n",
    "        Original: {query}\n",
    "        Stepback:\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage2)\":\n",
    "        return f\"\"\"You are an expert at world knowledge. Use both contexts to answer:\n",
    "        \n",
    "        Original Context:\n",
    "        {query['original_context']}\n",
    "        \n",
    "        Stepback Context:\n",
    "        {query['stepback_context']}\n",
    "        \n",
    "        Original Question: {query['original_question']}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aca0ca3-b4e0-4a57-93d8-58867750b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "def generate_without_context(\n",
    "    original_query: str,\n",
    "    modified_query: str,\n",
    "    strategy: str,\n",
    "    client: Optional[object] = None,\n",
    "    use_llm: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer given only the query and prompting strategy.\n",
    "    If use_llm is False or no client is provided, returns a placeholder answer.\n",
    "    \"\"\"\n",
    "    if use_llm and client is not None:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": modified_query}],\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback: simple placeholder\n",
    "    return f\"Answer: {original_query}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56459227-2671-4ba5-b17b-ee8bafb6439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_citation_metrics(\n",
    "    cited_ids: List[str],\n",
    "    ground_truth_ids: List[str],\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall and F1 for citation overlap.\n",
    "    \"\"\"\n",
    "    cited_set = set(str(x) for x in cited_ids)\n",
    "    ground_set = set(str(x) for x in ground_truth_ids)\n",
    "    if not cited_set:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    true_pos = cited_set & ground_set\n",
    "    precision = len(true_pos) / len(cited_set)\n",
    "    recall = len(true_pos) / len(ground_set) if ground_set else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def bootstrap_confidence_interval(\n",
    "    data: List[float],\n",
    "    num_samples: int = 1000,\n",
    "    confidence_level: float = 0.95,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute a bootstrap confidence interval for the mean of the data.\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return (0.0, 0.0)\n",
    "    means = []\n",
    "    n = len(data)\n",
    "    for _ in range(num_samples):\n",
    "        sample = [data[random.randint(0, n - 1)] for _ in range(n)]\n",
    "        means.append(np.mean(sample))\n",
    "    lower = np.percentile(means, (1 - confidence_level) / 2 * 100)\n",
    "    upper = np.percentile(means, (1 + confidence_level) / 2 * 100)\n",
    "    return float(lower), float(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f87e85b9-d564-4aa5-9794-30c3ea6d1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    \n",
    "    parts = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "    return parts or [text]\n",
    "\n",
    "def grit_encode_texts(model, texts):\n",
    "    vecs = model.encode([f\"<|embed|>\\n{t}\" for t in texts], convert_to_numpy=True)\n",
    "    vecs = vecs.astype(np.float32)\n",
    "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
    "    return vecs / norms\n",
    "\n",
    "def build_id2idx_from_retriever(retriever) -> dict:\n",
    "    \n",
    "    cid_series = retriever.corpus_df[\"corpusid\"].astype(str).reset_index(drop=True)\n",
    "    return {cid: i for i, cid in enumerate(cid_series)}\n",
    "\n",
    "def select_with_grit_preencoded(\n",
    "    retriever,\n",
    "    id2idx: dict,\n",
    "    context_ids: list,\n",
    "    ans_vec: np.ndarray = None,\n",
    "    part_mat: np.ndarray = None,\n",
    "    top_m: int = 3,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Re-rank context_ids using cosine similarity between candidate doc embeddings\n",
    "    and either a single answer vector (ans_vec) or a matrix of per-sentence vectors (part_mat).\n",
    "    \"\"\"\n",
    "    # map IDs -> embedding rows\n",
    "    pairs = [(str(cid), id2idx.get(str(cid))) for cid in context_ids]\n",
    "    pairs = [(cid, idx) for cid, idx in pairs if idx is not None]\n",
    "    if not pairs:\n",
    "        return []\n",
    "\n",
    "    cand_mat = retriever.embeddings[[idx for _, idx in pairs]]\n",
    "    cand_mat = cand_mat.astype(np.float32)\n",
    "    cand_mat /= (np.linalg.norm(cand_mat, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    picked = []\n",
    "    if part_mat is not None:\n",
    "        \n",
    "        sims = part_mat @ cand_mat.T \n",
    "        for r in sims:\n",
    "            order = np.argsort(-r)[:min(top_m, len(pairs))]\n",
    "            picked.extend(pairs[i][0] for i in order)\n",
    "    else:\n",
    "        \n",
    "        sims = cand_mat @ ans_vec      \n",
    "        order = np.argsort(-sims)[:min(top_m, len(pairs))]\n",
    "        picked.extend(pairs[i][0] for i in order)\n",
    "\n",
    "    \n",
    "    seen, uniq = set(), []\n",
    "    for cid in picked:\n",
    "        if cid not in seen:\n",
    "            uniq.append(cid); seen.add(cid)\n",
    "    return uniq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c732f3e-bfda-47e4-b170-15f0c4d8cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@dataclass\n",
    "class RetrievalSets:\n",
    "    baseline: List[str]\n",
    "    gold_injected: List[str]\n",
    "    oracle: List[str]\n",
    "\n",
    "def run_post_generation_attribution(\n",
    "    query_df: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    retriever,                              \n",
    "    strategies: List[str],\n",
    "    use_llm: bool = False,\n",
    "    llm_client: Optional[object] = None,\n",
    "    sample_fraction: float = 0.1,\n",
    "    random_state: int = 41,\n",
    "    k_retrieval: int = 20,\n",
    "    max_context: int = 5,\n",
    "    bootstrap_samples: int = 1000,\n",
    "    show_progress: bool = True,\n",
    "    # --- new options ---\n",
    "    content_selector: str = \"grit\",         \n",
    "    top_m_citations: int = 3,               \n",
    "    per_sentence: bool = True,              \n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Post-generation attribution with optional GRIT-based content-aware citation selection.\n",
    "    - content_selector=\"all\": cite all context_ids.\n",
    "    - content_selector=\"grit\": re-rank context_ids by similarity to the generated answer,\n",
    "      then keep top_m_citations (de-duplicated, order-preserving).\n",
    "    \"\"\"\n",
    "    def _ensure_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except Exception:\n",
    "                return [x]\n",
    "        return []\n",
    "\n",
    "    \n",
    "    df = query_df.copy().reset_index(drop=True)\n",
    "    df[\"corpusids\"] = df[\"corpusids\"].apply(_ensure_list)\n",
    "    df = df[df[\"corpusids\"].apply(len) > 0].reset_index(drop=True)\n",
    "    if 0 < sample_fraction < 1:\n",
    "        df = df.sample(frac=sample_fraction, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    id2idx = build_id2idx_from_retriever(retriever)\n",
    "\n",
    "    records: List[Dict[str, object]] = []\n",
    "\n",
    "    total_modes = 3  \n",
    "    total_steps = len(df) * max(len(strategies), 1) * total_modes\n",
    "    pbar = tqdm(total=total_steps, desc=\"Post-gen attribution\", leave=True, mininterval=0.2) if show_progress else None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        original_query = row[\"query\"]\n",
    "        ground_truth = [str(x) for x in row[\"corpusids\"]]\n",
    "\n",
    "        retrieved_ids = retriever.retrieve(original_query, k=k_retrieval)\n",
    "        retrieved_ids = [str(x) for x in retrieved_ids]\n",
    "\n",
    "        baseline_ids = retrieved_ids[:max_context]\n",
    "        missing = [gt for gt in ground_truth if gt not in baseline_ids]\n",
    "        num_gold = min(len(missing), max_context)\n",
    "        gold_add = missing[:num_gold]\n",
    "        num_keep = max_context - len(gold_add)\n",
    "        gold_injected_ids = baseline_ids[:num_keep] + gold_add\n",
    "        oracle_ids = ground_truth[:max_context]\n",
    "\n",
    "        for strategy in strategies:\n",
    "            modified_query = reformulate_query(original_query, strategy)\n",
    "            answer_text = generate_without_context(\n",
    "                original_query=original_query,\n",
    "                modified_query=modified_query,\n",
    "                strategy=strategy,\n",
    "                client=llm_client,\n",
    "                use_llm=use_llm,\n",
    "            )\n",
    "\n",
    "            \n",
    "            ans_vec, part_mat = None, None\n",
    "            if content_selector == \"grit\":\n",
    "                if per_sentence:\n",
    "                    parts = split_sentences(answer_text)\n",
    "                    part_mat = grit_encode_texts(retriever.model, parts)\n",
    "                else:\n",
    "                    ans_vec = grit_encode_texts(retriever.model, [answer_text])[0]\n",
    "\n",
    "            for mode_name, context_ids in [\n",
    "                (\"baseline\", baseline_ids),\n",
    "                (\"gold_injected\", gold_injected_ids),\n",
    "                (\"oracle\", oracle_ids),\n",
    "            ]:\n",
    "                if content_selector == \"grit\":\n",
    "                    picked_ids = select_with_grit_preencoded(\n",
    "                        retriever=retriever,\n",
    "                        id2idx=id2idx,\n",
    "                        context_ids=context_ids,\n",
    "                        ans_vec=ans_vec,\n",
    "                        part_mat=part_mat,\n",
    "                        top_m=top_m_citations,\n",
    "                    )\n",
    "                else:\n",
    "                    picked_ids = list(map(str, context_ids))  \n",
    "\n",
    "                citations = \" \".join(f\"[{cid}]\" for cid in picked_ids)\n",
    "                attributed_answer = f\"{answer_text} {citations}\".strip()\n",
    "\n",
    "                precision, recall, f1 = compute_citation_metrics(picked_ids, ground_truth)\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"Query\": original_query,\n",
    "                        \"Strategy\": strategy,\n",
    "                        \"Mode\": mode_name,\n",
    "                        \"Generated Answer\": attributed_answer,\n",
    "                        \"Context IDs\": context_ids,\n",
    "                        \"Cited IDs\": picked_ids,\n",
    "                        \"Ground Truth\": ground_truth,\n",
    "                        \"Citation Precision\": precision,\n",
    "                        \"Citation Recall\": recall,\n",
    "                        \"Citation F1\": f1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if pbar:\n",
    "                    pbar.set_postfix({\"strategy\": strategy, \"mode\": mode_name}, refresh=False)\n",
    "                    pbar.update(1)\n",
    "\n",
    "    if pbar:\n",
    "        pbar.close()\n",
    "\n",
    "    results_df = pd.DataFrame(records)\n",
    "\n",
    "    \n",
    "    summary_rows = []\n",
    "    grouped = results_df.groupby([\"Strategy\", \"Mode\"])\n",
    "    for (strategy_name, mode_name), group_df in grouped:\n",
    "        for metric_name in [\"Citation Precision\", \"Citation Recall\", \"Citation F1\"]:\n",
    "            values = group_df[metric_name].tolist()\n",
    "            mean_value = float(np.mean(values)) if values else 0.0\n",
    "            lower_ci, upper_ci = bootstrap_confidence_interval(\n",
    "                values, num_samples=bootstrap_samples, confidence_level=0.95\n",
    "            )\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"Strategy\": strategy_name,\n",
    "                    \"Mode\": mode_name,\n",
    "                    \"Metric\": metric_name,\n",
    "                    \"Mean\": mean_value,\n",
    "                    \"CI Lower\": lower_ci,\n",
    "                    \"CI Upper\": upper_ci,\n",
    "                }\n",
    "            )\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return results_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d02d5bc7-fcca-467f-a109-e460028ecd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@dataclass\n",
    "class RetrievalSets:\n",
    "    baseline: List[str]\n",
    "    gold_injected: List[str]\n",
    "    oracle: List[str]\n",
    "\n",
    "def run_post_generation_attribution(\n",
    "    query_df: pd.DataFrame,\n",
    "    corpus_df: pd.DataFrame,\n",
    "    retriever,                              \n",
    "    strategies: List[str],\n",
    "    use_llm: bool = False,\n",
    "    llm_client: Optional[object] = None,\n",
    "    sample_fraction: float = 1.0,\n",
    "    random_state: int = 41,\n",
    "    k_retrieval: int = 20,                 \n",
    "    max_context: int = 20,                  \n",
    "    bootstrap_samples: int = 1000,\n",
    "    show_progress: bool = True,\n",
    "    # --- post-gen specific knobs ---\n",
    "    retrieval_query_mode: str = \"q_plus_answer\",   \n",
    "    modes: tuple = (\"baseline\",\"gold_injected\",\"oracle\"),\n",
    "   \n",
    "    content_selector: str = \"grit\",         \n",
    "    top_m_citations: int = 3,               \n",
    "    per_sentence: bool = True,              \n",
    "    max_final_citations: Optional[int] = None,  \n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Post-generation attribution:\n",
    "      1) Generate answer (no retrieved context).\n",
    "      2) Retrieve with Q (+ Answer) -> candidate pool(s).\n",
    "      3) Select citations from the pool (GRIT), score vs gold.\n",
    "    Still reports baseline/gold_injected/oracle *pools* so you can study retrieval order,\n",
    "    but retrieval happens *after* generation and can use the answer.\n",
    "    \"\"\"\n",
    "    def _ensure_list(x):\n",
    "        import numpy as np, ast\n",
    "        if isinstance(x, list): return x\n",
    "        if isinstance(x, np.ndarray): return x.tolist()\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                v = ast.literal_eval(x)\n",
    "                return v if isinstance(v, list) else []\n",
    "            except Exception:\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "    \n",
    "    df = query_df.copy().reset_index(drop=True)\n",
    "    df[\"corpusids\"] = df[\"corpusids\"].apply(_ensure_list)\n",
    "    df = df[df[\"corpusids\"].apply(len) > 0].reset_index(drop=True)\n",
    "    if 0 < sample_fraction < 1:\n",
    "        df = df.sample(frac=sample_fraction, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    id2idx = build_id2idx_from_retriever(retriever)\n",
    "\n",
    "    records: List[Dict[str, object]] = []\n",
    "\n",
    "    total_modes = len(modes)\n",
    "    total_steps = len(df) * max(len(strategies), 1) * max(total_modes, 1)\n",
    "    pbar = tqdm(total=total_steps, desc=\"Post-gen attribution (Q+Answer retrieval)\",\n",
    "                leave=True, mininterval=0.2) if show_progress else None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        original_query = row[\"query\"]\n",
    "        ground_truth = [str(x) for x in row[\"corpusids\"]]\n",
    "\n",
    "        for strategy in strategies:\n",
    "           \n",
    "            modified_query = reformulate_query(original_query, strategy)\n",
    "            answer_text = generate_without_context(\n",
    "                original_query=original_query,\n",
    "                modified_query=modified_query,\n",
    "                strategy=strategy,\n",
    "                client=llm_client,\n",
    "                use_llm=use_llm,\n",
    "            )\n",
    "\n",
    "            \n",
    "            if retrieval_query_mode == \"q_plus_answer\":\n",
    "                retrieval_query = f\"{original_query}\\n\\nAnswer draft:\\n{answer_text}\"\n",
    "            else:\n",
    "                retrieval_query = original_query  # fallback: question only\n",
    "\n",
    "            retrieved_ids = retriever.retrieve(retrieval_query, k=k_retrieval)\n",
    "            retrieved_ids = [str(x) for x in retrieved_ids]\n",
    "\n",
    "           \n",
    "            baseline_ids = retrieved_ids[:max_context]\n",
    "\n",
    "            \n",
    "            gi = []\n",
    "            seen = set()\n",
    "            for x in baseline_ids + ground_truth:\n",
    "                sx = str(x)\n",
    "                if sx not in seen:\n",
    "                    gi.append(sx); seen.add(sx)\n",
    "                if len(gi) >= max_context:\n",
    "                    break\n",
    "            gold_injected_ids = gi\n",
    "\n",
    "            \n",
    "            oracle_ids = ground_truth[:max_context]\n",
    "\n",
    "            ans_vec, part_mat = None, None\n",
    "            if content_selector == \"grit\":\n",
    "                if per_sentence:\n",
    "                    parts = split_sentences(answer_text)\n",
    "                    part_mat = grit_encode_texts(retriever.model, parts)\n",
    "                else:\n",
    "                    ans_vec = grit_encode_texts(retriever.model, [answer_text])[0]\n",
    "\n",
    "            mode_pools = {\n",
    "                \"baseline\": baseline_ids,\n",
    "                \"gold_injected\": gold_injected_ids,\n",
    "                \"oracle\": oracle_ids,\n",
    "            }\n",
    "\n",
    "            for mode_name in modes:\n",
    "                context_ids = mode_pools[mode_name]\n",
    "\n",
    "                if content_selector == \"grit\":\n",
    "                    picked_ids = select_with_grit_preencoded(\n",
    "                        retriever=retriever,\n",
    "                        id2idx=id2idx,\n",
    "                        context_ids=context_ids,\n",
    "                        ans_vec=ans_vec,\n",
    "                        part_mat=part_mat,\n",
    "                        top_m=top_m_citations,\n",
    "                    )\n",
    "                    if max_final_citations is not None:\n",
    "                        picked_ids = picked_ids[:max_final_citations]\n",
    "                else:\n",
    "                    picked_ids = list(map(str, context_ids))  # cite all in the pool\n",
    "\n",
    "                citations = \" \".join(f\"[{cid}]\" for cid in picked_ids)\n",
    "                attributed_answer = f\"{answer_text} {citations}\".strip()\n",
    "\n",
    "                precision, recall, f1 = compute_citation_metrics(picked_ids, ground_truth)\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"Query\": original_query,\n",
    "                        \"Strategy\": strategy,\n",
    "                        \"Mode\": mode_name,\n",
    "                        \"Generated Answer\": attributed_answer,\n",
    "                        \"Context IDs\": context_ids,   \n",
    "                        \"Cited IDs\": picked_ids,      \n",
    "                        \"Ground Truth\": ground_truth,\n",
    "                        \"Citation Precision\": precision,\n",
    "                        \"Citation Recall\": recall,\n",
    "                        \"Citation F1\": f1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if pbar:\n",
    "                    pbar.set_postfix({\"strategy\": strategy, \"mode\": mode_name}, refresh=False)\n",
    "                    pbar.update(1)\n",
    "\n",
    "    if pbar:\n",
    "        pbar.close()\n",
    "\n",
    "    results_df = pd.DataFrame(records)\n",
    "\n",
    "    \n",
    "    summary_rows = []\n",
    "    grouped = results_df.groupby([\"Strategy\", \"Mode\"])\n",
    "    for (strategy_name, mode_name), group_df in grouped:\n",
    "        for metric_name in [\"Citation Precision\", \"Citation Recall\", \"Citation F1\"]:\n",
    "            values = group_df[metric_name].tolist()\n",
    "            mean_value = float(np.mean(values)) if values else 0.0\n",
    "            lower_ci, upper_ci = bootstrap_confidence_interval(\n",
    "                values, num_samples=bootstrap_samples, confidence_level=0.95\n",
    "            )\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"Strategy\": strategy_name,\n",
    "                    \"Mode\": mode_name,\n",
    "                    \"Metric\": metric_name,\n",
    "                    \"Mean\": mean_value,\n",
    "                    \"CI Lower\": lower_ci,\n",
    "                    \"CI Upper\": upper_ci,\n",
    "                }\n",
    "            )\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    return results_df, summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4d1e73-077d-4e04-9365-14eee6212113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gritlm import GritLM\n",
    "\n",
    "# Configuration\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "GROQ_API_KEY = \"\"\n",
    "os.makedirs(WORKSPACE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c059ba1b-9511-4a92-b2eb-9deddb53a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "GRIT_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"grit_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d80add6-ae5c-4772-b1b5-1c3b6d15cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your HF token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '679ea68ce0020282a57dfeef', 'name': 'uskh580e', 'fullname': 'Usman Shafi Khan', 'isPro': False, 'avatarUrl': '/avatars/db21fbd69a85ec3f3e3eae7f9ab1e344.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'hahaha', 'role': 'fineGrained', 'createdAt': '2025-08-12T19:54:06.186Z', 'fineGrained': {'canReadGatedRepos': False, 'global': [], 'scoped': [{'entity': {'_id': '679ea68ce0020282a57dfeef', 'type': 'user', 'name': 'uskh580e'}, 'permissions': []}]}}}}\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "token = getpass(\"Paste your HF token: \")  # input is hidden\n",
    "login(token=token)\n",
    "print(whoami())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c04b49-0ec8-4220-97fa-73719d87ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace set up at: /data/horse/ws/uskh580e-myws\n",
      "Datasets loaded successfully and stored in: /data/horse/ws/uskh580e-myws/huggingface_datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from groq import Groq\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"datasets\")\n",
    "os.environ[\"NLTK_DATA\"] = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TRANSFORMERS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"HF_DATASETS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"NLTK_DATA\"], exist_ok=True)\n",
    "BM25_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"bm25_index.pkl\")\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "NLTK_PATH = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "\n",
    "os.makedirs(NLTK_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "nltk.data.path.append(NLTK_PATH)\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"stopwords\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"wordnet\", download_dir=NLTK_PATH, force=True)\n",
    "print(f\"Workspace set up at: {WORKSPACE_PATH}\")\n",
    "DATASET_CACHE_DIR = os.path.join(WORKSPACE_PATH, \"huggingface_datasets\")\n",
    "\n",
    "\n",
    "os.makedirs(DATASET_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "\n",
    "print(\"Datasets loaded successfully and stored in:\", DATASET_CACHE_DIR)\n",
    "\n",
    "query_df = query_data.to_pandas()\n",
    "corpus_clean_df = corpus_clean_data.to_pandas()\n",
    "corpus_s2orc_df = corpus_s2orc_data.to_pandas()\n",
    "corpus_df = corpus_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cb1e4d6-afe5-4aff-a88f-a6c1142658ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_gold\n",
      "True    597\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>corpusids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are there any research papers on methods to co...</td>\n",
       "      <td>[202719327]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are there any resources available for translat...</td>\n",
       "      <td>[227231792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are there any studies that explore post-hoc te...</td>\n",
       "      <td>[226254579, 204976362]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query               corpusids\n",
       "0  Are there any research papers on methods to co...             [202719327]\n",
       "1  Are there any resources available for translat...             [227231792]\n",
       "2  Are there any studies that explore post-hoc te...  [226254579, 204976362]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "query_df = query_df.copy()\n",
    "query_df[\"corpusids\"] = query_df[\"corpusids\"].apply(\n",
    "    lambda a: a.tolist() if isinstance(a, np.ndarray) else (a if isinstance(a, list) else [])\n",
    ")\n",
    "query_df[\"corpusids\"] = query_df[\"corpusids\"].apply(lambda xs: [int(x) for x in xs])\n",
    "\n",
    "\n",
    "query_df[\"gold_len\"] = query_df[\"corpusids\"].apply(len)\n",
    "query_df[\"has_gold\"] = query_df[\"gold_len\"] > 0\n",
    "print(query_df[\"has_gold\"].value_counts())\n",
    "query_df[[\"query\",\"corpusids\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c967ccbc-7045-4037-9d53-752bc4faae2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99f76aa623b48cba135d26ddd3df27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GritLM: torch.bfloat16 dtype, mean pool, embedding mode, bbcc attn\n",
      "✅ Loaded cached GRIT embeddings\n"
     ]
    }
   ],
   "source": [
    "class GRITRetriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = GritLM(\"GritLM/GritLM-7B\", device_map=\"auto\", torch_dtype=torch.bfloat16, mode=\"embedding\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save GRITLM embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "\n",
    "        print(\"🔄 Computing GRIT embeddings...\")\n",
    "        texts = [f\"<|embed|>\\n{text}\" for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        self.embeddings = self.model.encode(texts, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(GRIT_INDEX_PATH):\n",
    "            try:\n",
    "                with open(GRIT_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"✅ Loaded cached GRIT embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"⚠️ Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(GRIT_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        formatted_query = f\"<|user|>\\nRepresent this query for retrieving relevant documents:\\n<|embed|>\\n{query}\"\n",
    "        query_embedding = self.model.encode([formatted_query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "\n",
    "grit_retriever = GRITRetriever(corpus_df)\n",
    "\n",
    "if grit_retriever.embeddings is None:\n",
    "    print(\"⚠️ GRITLM embeddings not found, computing...\")\n",
    "    grit_retriever._prepare_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65b075c0-aa4f-4feb-8330-480647faf26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable queries in df5: 597\n"
     ]
    }
   ],
   "source": [
    "shuffled = query_df.sample(frac=1, random_state=41).reset_index(drop=True)\n",
    "#df1, df2, df3, df4, df5 = np.array_split(shuffled, 5)\n",
    "\n",
    "df5 = shuffled.copy()\n",
    "df5 = df5[df5[\"corpusids\"].apply(len) > 0].reset_index(drop=True)\n",
    "print(\"Usable queries in df5:\", len(df5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87f8b67d-49be-452a-af81-d8948dd778b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = grit_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71095967-0365-4b5c-9529-f7f14487223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    \"zero-shot\",\n",
    "    \"few-shot\",\n",
    "    \"chain-of-thought (Standard)\",\n",
    "    \"self-verification\",\n",
    "    \"chain-of-verification\",\n",
    "    \"chain-of-citation\",\n",
    "    \"chain-of-quote\"  # include more if you like\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8dd9de1-929b-4129-bbe9-eb4f30e6cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd3d7a44-2fe5-4773-b084-10ca97978ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_series = grit_retriever.corpus_df[\"corpusid\"].astype(str).reset_index(drop=True)\n",
    "ID2IDX = {cid: i for i, cid in enumerate(cid_series)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86339792-3a69-4768-9567-488e308f1582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9b98d03b5248e8aca6155b4d086b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Post-gen attribution (Q+Answer retrieval):   0%|          | 0/4179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-query results: 4179\n",
      "Summary rows: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                               Query  \\\n",
       " 0  Is there a specialized question answering data...   \n",
       " 1  Is there a specialized question answering data...   \n",
       " 2  Is there a specialized question answering data...   \n",
       " 3  Is there a specialized question answering data...   \n",
       " 4  Is there a specialized question answering data...   \n",
       " \n",
       "                       Strategy      Mode  \\\n",
       " 0                    zero-shot  baseline   \n",
       " 1                     few-shot  baseline   \n",
       " 2  chain-of-thought (Standard)  baseline   \n",
       " 3            self-verification  baseline   \n",
       " 4        chain-of-verification  baseline   \n",
       " \n",
       "                                     Generated Answer  \\\n",
       " 0  You're looking for a dataset that focuses on i...   \n",
       " 1  Here's an example of how to answer the questio...   \n",
       " 2  Let's break down the question step by step:\\n\\...   \n",
       " 3  Answer:\\n\\nYes, there is a specialized questio...   \n",
       " 4  **Draft**\\n\\nYes, there is a specialized quest...   \n",
       " \n",
       "                                          Context IDs  \\\n",
       " 0  [232478685, 234093776, 248970443, 9027681, 232...   \n",
       " 1  [232478685, 222341844, 234741852, 232307006, 2...   \n",
       " 2  [222341844, 234093776, 232478685, 215785913, 2...   \n",
       " 3  [1373518, 232478685, 9027681, 222341844, 23230...   \n",
       " 4  [232478685, 232307006, 222341844, 1373518, 215...   \n",
       " \n",
       "                            Cited IDs Ground Truth  Citation Precision  \\\n",
       " 0    [232478685, 9027681, 232307006]  [235623770]                 0.0   \n",
       " 1  [232307006, 234741852, 234093776]  [235623770]                 0.0   \n",
       " 2  [234093776, 215785913, 253083183]  [235623770]                 0.0   \n",
       " 3      [1373518, 238744031, 9027681]  [235623770]                 0.0   \n",
       " 4    [232478685, 1373518, 253083183]  [235623770]                 0.0   \n",
       " \n",
       "    Citation Recall  Citation F1  \n",
       " 0              0.0          0.0  \n",
       " 1              0.0          0.0  \n",
       " 2              0.0          0.0  \n",
       " 3              0.0          0.0  \n",
       " 4              0.0          0.0  ,\n",
       "             Strategy      Mode              Metric      Mean  CI Lower  \\\n",
       " 0  chain-of-citation  baseline  Citation Precision  0.179229  0.164712   \n",
       " 1  chain-of-citation  baseline     Citation Recall  0.518481  0.479722   \n",
       " 2  chain-of-citation  baseline         Citation F1  0.264796  0.245387   \n",
       " 3     chain-of-quote  baseline  Citation Precision  0.159687  0.145170   \n",
       " 4     chain-of-quote  baseline     Citation Recall  0.461251  0.423060   \n",
       " \n",
       "    CI Upper  \n",
       " 0  0.193188  \n",
       " 1  0.557857  \n",
       " 2  0.285093  \n",
       " 3  0.173646  \n",
       " 4  0.498999  )"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df, summary_df = run_post_generation_attribution(\n",
    "    query_df=df5,\n",
    "    corpus_df=corpus_df,\n",
    "    retriever=grit_retriever,\n",
    "    strategies=strategies,\n",
    "    use_llm=True,\n",
    "    llm_client=client,            \n",
    "    sample_fraction=1,\n",
    "    k_retrieval=20,\n",
    "    max_context=20,               \n",
    "    retrieval_query_mode=\"q_plus_answer\",\n",
    "    modes=(\"baseline\",),          \n",
    "    content_selector=\"grit\",\n",
    "    top_m_citations=3,\n",
    "    per_sentence=False,           \n",
    "    max_final_citations=3,\n",
    "    bootstrap_samples=2000,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Per-query results:\", len(results_df))\n",
    "print(\"Summary rows:\", len(summary_df))\n",
    "results_df.head(), summary_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dda9fea7-0d65-462b-b8b3-13b13636ea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>Citation Precision</th>\n",
       "      <th>Citation Recall</th>\n",
       "      <th>Citation F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.160</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric                       Citation Precision  Citation Recall  Citation F1\n",
       "Strategy                                                                     \n",
       "zero-shot                                 0.190            0.548        0.281\n",
       "chain-of-thought (Standard)               0.190            0.548        0.280\n",
       "chain-of-verification                     0.187            0.542        0.276\n",
       "few-shot                                  0.187            0.540        0.276\n",
       "self-verification                         0.183            0.529        0.270\n",
       "chain-of-citation                         0.179            0.518        0.265\n",
       "chain-of-quote                            0.160            0.461        0.236"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>Citation Precision</th>\n",
       "      <th>Citation Recall</th>\n",
       "      <th>Citation F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.190 [0.176, 0.204]</td>\n",
       "      <td>0.548 [0.510, 0.589]</td>\n",
       "      <td>0.281 [0.261, 0.300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.190 [0.176, 0.203]</td>\n",
       "      <td>0.548 [0.509, 0.589]</td>\n",
       "      <td>0.280 [0.259, 0.300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.187 [0.173, 0.201]</td>\n",
       "      <td>0.542 [0.504, 0.583]</td>\n",
       "      <td>0.276 [0.257, 0.296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.187 [0.173, 0.200]</td>\n",
       "      <td>0.540 [0.503, 0.580]</td>\n",
       "      <td>0.276 [0.256, 0.297]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.183 [0.170, 0.197]</td>\n",
       "      <td>0.529 [0.491, 0.568]</td>\n",
       "      <td>0.270 [0.251, 0.290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.179 [0.165, 0.193]</td>\n",
       "      <td>0.518 [0.480, 0.558]</td>\n",
       "      <td>0.265 [0.245, 0.285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.160 [0.145, 0.174]</td>\n",
       "      <td>0.461 [0.423, 0.499]</td>\n",
       "      <td>0.236 [0.216, 0.255]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric                         Citation Precision       Citation Recall  \\\n",
       "Strategy                                                                  \n",
       "zero-shot                    0.190 [0.176, 0.204]  0.548 [0.510, 0.589]   \n",
       "chain-of-thought (Standard)  0.190 [0.176, 0.203]  0.548 [0.509, 0.589]   \n",
       "chain-of-verification        0.187 [0.173, 0.201]  0.542 [0.504, 0.583]   \n",
       "few-shot                     0.187 [0.173, 0.200]  0.540 [0.503, 0.580]   \n",
       "self-verification            0.183 [0.170, 0.197]  0.529 [0.491, 0.568]   \n",
       "chain-of-citation            0.179 [0.165, 0.193]  0.518 [0.480, 0.558]   \n",
       "chain-of-quote               0.160 [0.145, 0.174]  0.461 [0.423, 0.499]   \n",
       "\n",
       "Metric                                Citation F1  \n",
       "Strategy                                           \n",
       "zero-shot                    0.281 [0.261, 0.300]  \n",
       "chain-of-thought (Standard)  0.280 [0.259, 0.300]  \n",
       "chain-of-verification        0.276 [0.257, 0.296]  \n",
       "few-shot                     0.276 [0.256, 0.297]  \n",
       "self-verification            0.270 [0.251, 0.290]  \n",
       "chain-of-citation            0.265 [0.245, 0.285]  \n",
       "chain-of-quote               0.236 [0.216, 0.255]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>Citation Precision</th>\n",
       "      <th>Citation Recall</th>\n",
       "      <th>Citation F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.160</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric                       Citation Precision  Citation Recall  Citation F1\n",
       "Strategy                                                                     \n",
       "zero-shot                                 0.190            0.548        0.281\n",
       "chain-of-thought (Standard)               0.190            0.548        0.280\n",
       "chain-of-verification                     0.187            0.542        0.276\n",
       "few-shot                                  0.187            0.540        0.276\n",
       "self-verification                         0.183            0.529        0.270\n",
       "chain-of-citation                         0.179            0.518        0.265\n",
       "chain-of-quote                            0.160            0.461        0.236"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>Citation Precision</th>\n",
       "      <th>Citation Recall</th>\n",
       "      <th>Citation F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.190 [0.176, 0.204]</td>\n",
       "      <td>0.548 [0.510, 0.589]</td>\n",
       "      <td>0.281 [0.261, 0.300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.190 [0.176, 0.203]</td>\n",
       "      <td>0.548 [0.509, 0.589]</td>\n",
       "      <td>0.280 [0.259, 0.300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.187 [0.173, 0.201]</td>\n",
       "      <td>0.542 [0.504, 0.583]</td>\n",
       "      <td>0.276 [0.257, 0.296]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.187 [0.173, 0.200]</td>\n",
       "      <td>0.540 [0.503, 0.580]</td>\n",
       "      <td>0.276 [0.256, 0.297]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.183 [0.170, 0.197]</td>\n",
       "      <td>0.529 [0.491, 0.568]</td>\n",
       "      <td>0.270 [0.251, 0.290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.179 [0.165, 0.193]</td>\n",
       "      <td>0.518 [0.480, 0.558]</td>\n",
       "      <td>0.265 [0.245, 0.285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.160 [0.145, 0.174]</td>\n",
       "      <td>0.461 [0.423, 0.499]</td>\n",
       "      <td>0.236 [0.216, 0.255]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric                         Citation Precision       Citation Recall  \\\n",
       "Strategy                                                                  \n",
       "zero-shot                    0.190 [0.176, 0.204]  0.548 [0.510, 0.589]   \n",
       "chain-of-thought (Standard)  0.190 [0.176, 0.203]  0.548 [0.509, 0.589]   \n",
       "chain-of-verification        0.187 [0.173, 0.201]  0.542 [0.504, 0.583]   \n",
       "few-shot                     0.187 [0.173, 0.200]  0.540 [0.503, 0.580]   \n",
       "self-verification            0.183 [0.170, 0.197]  0.529 [0.491, 0.568]   \n",
       "chain-of-citation            0.179 [0.165, 0.193]  0.518 [0.480, 0.558]   \n",
       "chain-of-quote               0.160 [0.145, 0.174]  0.461 [0.423, 0.499]   \n",
       "\n",
       "Metric                                Citation F1  \n",
       "Strategy                                           \n",
       "zero-shot                    0.281 [0.261, 0.300]  \n",
       "chain-of-thought (Standard)  0.280 [0.259, 0.300]  \n",
       "chain-of-verification        0.276 [0.257, 0.296]  \n",
       "few-shot                     0.276 [0.256, 0.297]  \n",
       "self-verification            0.270 [0.251, 0.290]  \n",
       "chain-of-citation            0.265 [0.245, 0.285]  \n",
       "chain-of-quote               0.236 [0.216, 0.255]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def strategy_means(summary_df, mode=None):\n",
    "    df = summary_df if mode is None else summary_df[summary_df[\"Mode\"] == mode]\n",
    "    pv = (df.pivot_table(index=\"Strategy\", columns=\"Metric\", values=\"Mean\")\n",
    "            .reindex(columns=[\"Citation Precision\",\"Citation Recall\",\"Citation F1\"])\n",
    "            .sort_values(\"Citation F1\", ascending=False)\n",
    "            .round(3))\n",
    "    return pv\n",
    "\n",
    "def strategy_means_with_ci(summary_df, mode=None):\n",
    "    df = summary_df if mode is None else summary_df[summary_df[\"Mode\"] == mode]\n",
    "    df = df.copy()\n",
    "    df[\"Mean±CI\"] = df.apply(lambda r: f\"{r['Mean']:.3f} [{r['CI Lower']:.3f}, {r['CI Upper']:.3f}]\", axis=1)\n",
    "    pv = (df.pivot(index=\"Strategy\", columns=\"Metric\", values=\"Mean±CI\")\n",
    "            .reindex(columns=[\"Citation Precision\",\"Citation Recall\",\"Citation F1\"]))\n",
    "    \n",
    "    order = (df[df[\"Metric\"]==\"Citation F1\"]\n",
    "             .sort_values(\"Mean\", ascending=False)[\"Strategy\"].tolist())\n",
    "    return pv.loc[order]\n",
    "\n",
    "\n",
    "display(strategy_means(summary_df, mode=\"baseline\"))\n",
    "display(strategy_means_with_ci(summary_df, mode=\"baseline\"))\n",
    "\n",
    "\n",
    "display(strategy_means(summary_df, mode=None))\n",
    "display(strategy_means_with_ci(summary_df, mode=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "647da639-a650-49be-ac53-32715d070b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts):\n",
    "    \"\"\" Generates a concise answer using RAG, citing only the top 1–2 relevant sources. \"\"\"\n",
    "    context = \"\\n\".join([\n",
    "        f\"Corpus ID: {doc_id}\\nTitle: {title}\\nFull Text: {full_text}\"\n",
    "        for doc_id, title, full_text in zip(retrieved_corpus_ids, retrieved_titles, retrieved_texts)\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {modified_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732d7c99-dee5-4b0e-b559-77632b441f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query\n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "        ...\n",
    "        Now, using the retrieved documents below, answer the question while citing sources explicitly:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard)\":\n",
    "        return f\"\"\"Let's break down the question step by step:\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Neutral)\":\n",
    "        return f\"\"\"You are an AI assistant tasked with answering a research question...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Academic Aspiration)\":\n",
    "        return f\"\"\"You are a rising academic in the field of AI...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification\":\n",
    "        return f\"\"\"\n",
    "        Step 1: Generate an initial answer based on retrieved documents.\n",
    "        Step 2: Verify each claim and ensure that citations match the retrieved evidence.\n",
    "        Step 3: If a claim is unsupported, explicitly state that it cannot be verified.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification\":\n",
    "        return f\"\"\"\n",
    "        Let's break this down into a verification process:\n",
    "        ...\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation\":\n",
    "        return f\"\"\"Answer the question step by step, citing the relevant corpus ID after each reasoning step.\n",
    "Use this format: [corpus: document_id]\n",
    "\n",
    "Now, begin reasoning:\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote\":\n",
    "        return f\"\"\"Answer the question step by step, including a short direct quote from the supporting document for each reasoning step.\n",
    "Use this format: “quoted sentence” [corpus: document_id]\n",
    "\n",
    "Now, begin reasoning:\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c4b7a-a30b-405f-a84f-ca48c634b7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
