{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c12be55-04a2-4833-8f94-39c8bb2763c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client with your API key (set your actual key here or via an environment variable)\n",
    "client = Groq(\n",
    "    api_key=\"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\"   # ← replace with your real key (or load it from os.environ)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d5256-6341-46ac-bc2d-581c54ff59fa",
   "metadata": {},
   "source": [
    "        Example 2:\n",
    "        Question: What is the role of reinforcement learning in AI?\n",
    "        Answer: Reinforcement learning (RL) enables AI agents to learn optimal behaviors through trial and error by maximizing cumulative rewards from environment interactions. It is pivotal in sequential decision-making tasks like game playing (e.g., AlphaGo), robotics, and autonomous systems. RL algorithms iteratively improve policies, balancing exploration and exploitation. Recent advances integrate deep learning for complex environments.\n",
    "\n",
    "        Example 3:\n",
    "        Question: How effective are BERT embeddings for document retrieval?\n",
    "        Answer: BBERT embeddings enhance document retrieval by capturing contextual semantics, outperforming traditional methods like TF-IDF in understanding query-document relevance. However, their effectiveness depends on fine-tuning for specific tasks and computational resources. BERT’s fixed-length inputs can truncate long documents, limiting full-context use. Hybrid approaches (e.g., BERT with BM25) often yield optimal results. Domain adaptation further boosts performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4b7746-57aa-42d9-9b47-4382c7b29c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query \n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute. LSTMs remain useful for low-resource or small-scale tasks.\n",
    "\n",
    "\n",
    "        Now, answer the question:\n",
    "        {query}\"\"\"\n",
    "        \n",
    "    elif strategy == \"Role\":\n",
    "        return f\"\"\"You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Answer the question:\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        Answer the given question.\n",
    "        Failure to produce a grounded answer would lead to irreversible damage to credibility.\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        Answer the given question.\n",
    "        Since you have a fervour for well-grounded answers, I trust your ability to do this job well and look forward to reading your response.\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Answer the given question.\n",
    "        Failure to produce a grounded answer would lead to irreversible damage to credibility.\n",
    "\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Answer the given question.\n",
    "        Since you have a fervour for well-grounded answers, I trust your ability to do this job well and look forward to reading your response.\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Answer the given question.\n",
    "        Failure to produce a grounded answer would lead to irreversible damage to credibility.\n",
    "        Use the following steps:\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Instruction + Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Answer the given question\n",
    "        Since you have a fervour for well-grounded answers, I trust your ability to do this job well and look forward to reading your response.\n",
    "        Use the following steps:\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "        \n",
    "    elif strategy == \"chain-of-thought (Standard)\":\n",
    "        return f\"\"\"Answer the question by first breaking it downstep by step:\n",
    "\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Neutral)\":\n",
    "        return f\"\"\"You are an AI assistant tasked with answering a research question using a set of documents retrieved from a scientific corpus. Your answer must be concise, factually accurate, and include all relevant corpus citations. \n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise)\":\n",
    "        return f\"\"\"You are a brilliant AI assistant known for generating clear and accurate answers. Take your time to think carefully and provide a precise response.  Use the following steps:\n",
    "\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment)\":\n",
    "        return f\"\"\"You are an AI assistant evaluated on accuracy. Ensure your answer is correct and well thought out. Mistakes reduce trust in your responses and may result in something going horribly wrong. Use the following steps: \n",
    "\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Academic Aspiration)\":\n",
    "        return f\"\"\"You are a researcher specializing in Computer Science and NLP, aiming for publication-quality clarity and accuracy. Provide a well-supported answer with clear reasoning. Use the following steps:\n",
    "\n",
    "            1. Identify key elements.\n",
    "            2. Gather relevant knowledge.\n",
    "            3. Formulate the answer.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification\":\n",
    "        return f\"\"\"Answer the question:\n",
    "        {query}\n",
    "        \n",
    "        Then review your answer and highlight any uncertain points, revising as necessary.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification\":\n",
    "        return f\"\"\"Answer the question in four stages:\n",
    "        1. **Draft**: Provide an initial answer.\n",
    "        2. **Verify**: List each claim and check its support.\n",
    "        3. **Revise**: Correct or remove unsupported claims.\n",
    "        4. **Finalize**: Present the refined answer with notes on verification.\n",
    "        \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation\":\n",
    "        return f\"\"\"Answer the question by listing each key claim as a bullet point prefixed with 'Claim:'\n",
    " \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote\":\n",
    "        return f\"\"\"Answer the question step by step, and for each step include a direct quote from your internal knowledge in quotation marks.\n",
    "\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Academic Aspiration (2-stage)\":\n",
    "        return f\"\"\"You are a rising academic in the field of artificial intelligence. Your work is beginning to attract attention, and every paper you write contributes to your growing reputation. It’s essential that your answers are accurate, well-supported, and ethically cited. Misattributed or missing citations can damage your credibility and compromise the trust of the research community. Cite with care, and write like your future depends on it. \n",
    "    Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated for factual accuracy and proper attribution. Providing incomplete, incorrect, or hallucinated citations will result in penalization and reduce the trustworthiness of your response. Only cite sources that are directly relevant and verifiable from the retrieved documents.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers and excellent citation accuracy. Citing the correct sources is one of the most important ways you demonstrate trustworthiness, attention to detail, and helpfulness. Providing accurate, complete citations helps users understand and trust your response.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (role-gen)\":\n",
    "        return f\"\"\"You are an expert at role selection for problem solving. Generate a professional role description following this format:\n",
    "\n",
    "A: This question is a [DOMAIN] problem involving [KEY CONCEPTS]. To better solve it, I will act as a [SPECIFIC ROLE] who [EXPERTISE DESCRIPTION].\n",
    "\n",
    "Examples:\n",
    "Q: Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
    "A: This question is a neuroscience problem involving cell biology and migration. To better solve it, I will act as a neuroscientist who specializes in the study of the brain and its cellular behaviors.\n",
    "\n",
    "Q: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "A: This question is a computer vision problem involving architectural comparisons. To better solve it, I will act as a deep learning researcher who focuses on model architecture analysis and performance benchmarking.\n",
    "\n",
    "Now generate a role description for:\n",
    "Q: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (answer)\":\n",
    "        # This will be populated dynamically with the generated role\n",
    "        return f\"\"\"{query['generated_role']}\n",
    "\n",
    "Using this expertise and the retrieved documents below, provide a comprehensive answer:\n",
    "{query}\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage1)\":\n",
    "        return f\"\"\"You are an expert research assistant. Paraphrase specific questions into fundamental scientific inquiries:\n",
    "\n",
    "        Examples:\n",
    "        Original: How does contrastive learning improve representation learning in vision transformers?\n",
    "        Stepback: What are the fundamental mechanisms by which self-supervised learning techniques enhance representation learning in transformer architectures?\n",
    "\n",
    "        Original: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "        Stepback: What architectural differences between convolutional networks and transformer models affect computer vision performance metrics?\n",
    "\n",
    "        Original: How effective is chain-of-thought prompting for mathematical reasoning in PaLM-2?\n",
    "        Stepback: What cognitive science principles underlie the effectiveness of decomposition strategies in large language model reasoning?\n",
    "\n",
    "        Original: What causes gradient instability in 4-bit quantized LLM fine-tuning?\n",
    "        Stepback: What numerical precision challenges emerge during post-training quantization of neural network parameters?\n",
    "\n",
    "        Original: How does Med-PaLM 2 achieve state-of-the-art on medical QA benchmarks?\n",
    "        Stepback: What architectural adaptations and training strategies improve language model performance in domain-specific knowledge tasks?\n",
    "\n",
    "        Now generate a step-back question for:\n",
    "        Original: {query}\n",
    "        Stepback:\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage2)\":\n",
    "        return f\"\"\"You are an expert at world knowledge. Use both contexts to answer:\n",
    "        \n",
    "        Original Context:\n",
    "        {query['original_context']}\n",
    "        \n",
    "        Stepback Context:\n",
    "        {query['stepback_context']}\n",
    "        \n",
    "        Original Question: {query['original_question']}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3bdf7-b00a-4b0e-b25f-c488e33a9709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec6838c-2710-400b-9393-1476074af6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plain_answer(query, strategy):\n",
    "    prompt = reformulate_query(query, strategy)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69c65de-1cc4-4347-816e-3a383e427f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gritlm import GritLM\n",
    "\n",
    "# Configuration\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "GROQ_API_KEY = \"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\"\n",
    "os.makedirs(WORKSPACE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3edebc-96ed-4226-a6d4-399c697ae9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "E5_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"e5_index.pkl\")\n",
    "GTR_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"gtr_index.pkl\")\n",
    "GRIT_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"grit_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a73be0dd-8ec2-469b-898f-48ce41c9ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace set up at: /data/horse/ws/uskh580e-myws\n",
      "Datasets loaded successfully and stored in: /data/horse/ws/uskh580e-myws/huggingface_datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from groq import Groq\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define workspace path\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "\n",
    "# Set environment variables to store large files in workspace\n",
    "os.environ[\"HF_HOME\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"datasets\")\n",
    "os.environ[\"NLTK_DATA\"] = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TRANSFORMERS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"HF_DATASETS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"NLTK_DATA\"], exist_ok=True)\n",
    "BM25_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"bm25_index.pkl\")\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "NLTK_PATH = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(NLTK_PATH, exist_ok=True)\n",
    "\n",
    "# Set NLTK data path\n",
    "nltk.data.path.append(NLTK_PATH)\n",
    "\n",
    "# Force download punkt & stopwords\n",
    "nltk.download(\"punkt\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"stopwords\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"wordnet\", download_dir=NLTK_PATH, force=True)\n",
    "print(f\"Workspace set up at: {WORKSPACE_PATH}\")\n",
    "DATASET_CACHE_DIR = os.path.join(WORKSPACE_PATH, \"huggingface_datasets\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(DATASET_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Load datasets and store them in workspace cache\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "\n",
    "print(\"Datasets loaded successfully and stored in:\", DATASET_CACHE_DIR)\n",
    "\n",
    "query_df = query_data.to_pandas()\n",
    "corpus_clean_df = corpus_clean_data.to_pandas()\n",
    "corpus_s2orc_df = corpus_s2orc_data.to_pandas()\n",
    "corpus_df = corpus_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cedf2f8-aae6-492f-ac08-80f935bdc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded cached E5 embeddings\n",
      "✅ Loaded cached GTR embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56e5dd227ac41779bc6a34f35275c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GritLM: torch.bfloat16 dtype, mean pool, embedding mode, bbcc attn\n",
      "✅ Loaded cached GRIT embeddings\n"
     ]
    }
   ],
   "source": [
    "class E5Retriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = SentenceTransformer(\"intfloat/e5-large-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save E5 embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "\n",
    "        print(\"🔄 Computing E5 embeddings...\")\n",
    "        texts = [\"passage: \" + text for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        self.embeddings = self.model.encode(texts, batch_size=256, normalize_embeddings=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(E5_INDEX_PATH):\n",
    "            try:\n",
    "                with open(E5_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"✅ Loaded cached E5 embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"⚠️ Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(E5_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        query_embedding = self.model.encode([\"query: \" + query], normalize_embeddings=True)\n",
    "        scores = np.dot(query_embedding, self.embeddings.T)\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ✅ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "e5_retriever = E5Retriever(corpus_df)\n",
    "\n",
    "if e5_retriever.embeddings is None:\n",
    "    print(\"⚠️ E5 embeddings not found, computing...\")\n",
    "    e5_retriever._prepare_embeddings()\n",
    "class GTRRetriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save GTR embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "        \n",
    "        print(\"🔄 Computing GTR embeddings...\")\n",
    "        texts = self.corpus_df[\"abstract\"].astype(str)\n",
    "        self.embeddings = self.model.encode(texts, batch_size=256, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(GTR_INDEX_PATH):\n",
    "            try:\n",
    "                with open(GTR_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"✅ Loaded cached GTR embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"⚠️ Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(GTR_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ✅ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "gtr_retriever = GTRRetriever(corpus_df)\n",
    "\n",
    "if gtr_retriever.embeddings is None:\n",
    "    print(\"⚠️ GTR embeddings not found, computing...\")\n",
    "    gtr_retriever._prepare_embeddings()\n",
    "class GRITRetriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = GritLM(\"GritLM/GritLM-7B\", device_map=\"auto\", torch_dtype=torch.bfloat16, mode=\"embedding\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save GRITLM embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "\n",
    "        print(\"🔄 Computing GRIT embeddings...\")\n",
    "        texts = [f\"<|embed|>\\n{text}\" for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        self.embeddings = self.model.encode(texts, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(GRIT_INDEX_PATH):\n",
    "            try:\n",
    "                with open(GRIT_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"✅ Loaded cached GRIT embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"⚠️ Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(GRIT_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        formatted_query = f\"<|user|>\\nRepresent this query for retrieving relevant documents:\\n<|embed|>\\n{query}\"\n",
    "        query_embedding = self.model.encode([formatted_query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ✅ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "grit_retriever = GRITRetriever(corpus_df)\n",
    "\n",
    "if grit_retriever.embeddings is None:\n",
    "    print(\"⚠️ GRITLM embeddings not found, computing...\")\n",
    "    grit_retriever._prepare_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "791653e2-6607-4315-aa08-9f86980c1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution_results = []\n",
    "strategies = [\"zero-shot\", \"few-shot\", \"chain-of-thought (Standard)\",\"Role\",\"Motivation (Negative)\",\"Motivation (Positive)\",\"Role + Motivation (Negative)\",\"Role + Motivation (Positive)\",\"Instruction + Role + Motivation (Negative)\",\"Instruction + Role + Motivation (Positive)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f531be3e-5654-4142-ba39-d4d39557e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 0.1  # fraction of the dataset to use (e.g., 0.1 = 10%)\n",
    "total_queries = len(query_df)\n",
    "query_df = query_df.sample(frac=sample_frac, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee23a5-857c-4d91-88f7-e5358b38a4b0",
   "metadata": {},
   "source": [
    "each claim separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac3ab3f-78da-4888-a35d-574a15633812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Claim-level attribution: 100%|████████████████████████████████████████████████████████| 60/60 [1:05:28<00:00, 65.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved claim-level results (k_claim=3) to post_gen_claims_k3.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Instruction + Role + Motivation (Negative)</th>\n",
       "      <td>0.013464</td>\n",
       "      <td>0.591667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instruction + Role + Motivation (Positive)</th>\n",
       "      <td>0.010888</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Motivation (Negative)</th>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Motivation (Positive)</th>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Role</th>\n",
       "      <td>0.016957</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Role + Motivation (Negative)</th>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Role + Motivation (Positive)</th>\n",
       "      <td>0.013774</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.014244</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.016944</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Precision    Recall\n",
       "Strategy                                                       \n",
       "Instruction + Role + Motivation (Negative)   0.013464  0.591667\n",
       "Instruction + Role + Motivation (Positive)   0.010888  0.533333\n",
       "Motivation (Negative)                        0.019303  0.616667\n",
       "Motivation (Positive)                        0.019017  0.583333\n",
       "Role                                         0.016957  0.508333\n",
       "Role + Motivation (Negative)                 0.013326  0.541667\n",
       "Role + Motivation (Positive)                 0.013774  0.566667\n",
       "chain-of-thought (Standard)                  0.014244  0.558333\n",
       "few-shot                                     0.048273  0.533333\n",
       "zero-shot                                    0.016944  0.566667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Loop: generate then attribute with claim-level retrieval\n",
    "k_claim = 3  # ⬅️ adjust this to retrieve more/less per claim\n",
    "\n",
    "def extract_claims(answer_text):\n",
    "    # Splits on sentence boundaries and newlines\n",
    "    parts = re.split(r'[\\.!?]\\s+|\\n', answer_text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "attribution_results = []\n",
    "for _, row in tqdm(query_df.iterrows(), total=len(query_df), desc=\"Claim-level attribution\"):\n",
    "    q = row['query']\n",
    "    # parse ground-truth IDs\n",
    "    cell = row['corpusids']\n",
    "    if isinstance(cell, str):\n",
    "        truth_ids = ast.literal_eval(cell)\n",
    "    elif isinstance(cell, (list, tuple, np.ndarray, pd.Series)):\n",
    "        truth_ids = list(cell)\n",
    "    else:\n",
    "        truth_ids = []\n",
    "\n",
    "    for strategy in strategies:\n",
    "        ans = generate_plain_answer(q, strategy)\n",
    "        claims = extract_claims(ans)\n",
    "\n",
    "        # retrieve k_claim docs for each claim\n",
    "        all_cited = []\n",
    "        for claim in claims:\n",
    "            docs = grit_retriever.retrieve(claim, k=k_claim)\n",
    "            all_cited.extend(docs)\n",
    "\n",
    "        # dedupe while preserving order\n",
    "        cited_ids = list(dict.fromkeys(all_cited))\n",
    "\n",
    "        tp = len(set(cited_ids) & set(truth_ids))\n",
    "        precision = tp / len(cited_ids) if cited_ids else 0\n",
    "        recall    = tp / len(truth_ids)    if truth_ids else 0\n",
    "\n",
    "        attribution_results.append({\n",
    "            'Query': q,\n",
    "            'Strategy': strategy,\n",
    "            'Generated Answer': ans,\n",
    "            'Claims': claims,\n",
    "            'Claim Citations': cited_ids,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "\n",
    "# 7. Save & inspect\n",
    "results_df = pd.DataFrame(attribution_results)\n",
    "results_df.to_csv('post_gen_claims_k{}.csv'.format(k_claim), index=False)\n",
    "print(f\"Saved claim-level results (k_claim={k_claim}) to post_gen_claims_k{k_claim}.csv\")\n",
    "display(results_df.groupby('Strategy')[['Precision','Recall']].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d8a5e87-bfbd-434f-be2e-b6976a700619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to post_gen_attribution_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.115386</td>\n",
       "      <td>0.551883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.113631</td>\n",
       "      <td>0.559135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Academic Aspiration)</th>\n",
       "      <td>0.116680</td>\n",
       "      <td>0.582520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Praise)</th>\n",
       "      <td>0.117082</td>\n",
       "      <td>0.582101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Punishment)</th>\n",
       "      <td>0.117401</td>\n",
       "      <td>0.588377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.119600</td>\n",
       "      <td>0.596327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.113914</td>\n",
       "      <td>0.568433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.118873</td>\n",
       "      <td>0.567457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.114146</td>\n",
       "      <td>0.563970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.116890</td>\n",
       "      <td>0.570944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Precision    Recall\n",
       "Strategy                                                   \n",
       "chain-of-citation                        0.115386  0.551883\n",
       "chain-of-quote                           0.113631  0.559135\n",
       "chain-of-thought (Academic Aspiration)   0.116680  0.582520\n",
       "chain-of-thought (Praise)                0.117082  0.582101\n",
       "chain-of-thought (Punishment)            0.117401  0.588377\n",
       "chain-of-thought (Standard)              0.119600  0.596327\n",
       "chain-of-verification                    0.113914  0.568433\n",
       "few-shot                                 0.118873  0.567457\n",
       "self-verification                        0.114146  0.563970\n",
       "zero-shot                                0.116890  0.570944"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(attribution_results)\n",
    "results_df.to_csv('post_gen_attribution_results.csv', index=False)\n",
    "print(\"Results saved to post_gen_attribution_results.csv\")\n",
    "display(results_df.groupby('Strategy')[['Precision','Recall']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e10e8c-3c5b-47a2-86af-cb886810d683",
   "metadata": {},
   "source": [
    "All claims together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dce4fe98-c20d-43ef-b140-7f26f2cdf975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-gen attribution: 100%|█████████████████████████████████████████████████████████████| 60/60 [09:52<00:00,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to post_gen_attribution_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chain-of-citation</th>\n",
       "      <td>0.119026</td>\n",
       "      <td>0.567275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-quote</th>\n",
       "      <td>0.120244</td>\n",
       "      <td>0.571385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Academic Aspiration)</th>\n",
       "      <td>0.125419</td>\n",
       "      <td>0.601471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Praise)</th>\n",
       "      <td>0.124810</td>\n",
       "      <td>0.594165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Punishment)</th>\n",
       "      <td>0.124810</td>\n",
       "      <td>0.597210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-thought (Standard)</th>\n",
       "      <td>0.127245</td>\n",
       "      <td>0.609691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chain-of-verification</th>\n",
       "      <td>0.122374</td>\n",
       "      <td>0.585337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.122983</td>\n",
       "      <td>0.587316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-verification</th>\n",
       "      <td>0.120852</td>\n",
       "      <td>0.578945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.123592</td>\n",
       "      <td>0.588838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Precision    Recall\n",
       "Strategy                                                   \n",
       "chain-of-citation                        0.119026  0.567275\n",
       "chain-of-quote                           0.120244  0.571385\n",
       "chain-of-thought (Academic Aspiration)   0.125419  0.601471\n",
       "chain-of-thought (Praise)                0.124810  0.594165\n",
       "chain-of-thought (Punishment)            0.124810  0.597210\n",
       "chain-of-thought (Standard)              0.127245  0.609691\n",
       "chain-of-verification                    0.122374  0.585337\n",
       "few-shot                                 0.122983  0.587316\n",
       "self-verification                        0.120852  0.578945\n",
       "zero-shot                                0.123592  0.588838"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _, row in tqdm(query_df.iterrows(), total=len(query_df), desc=\"Post-gen attribution\"):\n",
    "    q = row['query']\n",
    "    # Handle various corpusids types\n",
    "    cell = row['corpusids']\n",
    "    if isinstance(cell, str):\n",
    "        truth_ids = ast.literal_eval(cell)\n",
    "    elif isinstance(cell, (list, tuple, np.ndarray, pd.Series)):\n",
    "        truth_ids = list(cell)\n",
    "    else:\n",
    "        truth_ids = []\n",
    "\n",
    "    for strategy in strategies:\n",
    "        ans = generate_plain_answer(q, strategy)\n",
    "        cited_ids = grit_retriever.retrieve(ans, k=5)\n",
    "        tp = len(set(cited_ids) & set(truth_ids))\n",
    "        precision = tp / len(cited_ids) if cited_ids else 0\n",
    "        recall = tp / len(truth_ids) if truth_ids else 0\n",
    "        attribution_results.append({\n",
    "            'Query': q,\n",
    "            'Strategy': strategy,\n",
    "            'Generated Answer': ans,\n",
    "            'Attributed Corpus IDs': cited_ids,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "\n",
    "# --- 7. Save and display results ---\n",
    "results_df = pd.DataFrame(attribution_results)\n",
    "results_df.to_csv('post_gen_attribution_results.csv', index=False)\n",
    "print(\"Results saved to post_gen_attribution_results.csv\")\n",
    "display(results_df.groupby('Strategy')[['Precision','Recall']].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234e783-a2da-4eb5-8dfa-9114f81c127b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
