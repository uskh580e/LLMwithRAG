{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d82a9ab-395a-4e31-a628-c88b72720b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32eeb882-226b-4899-a33f-1b1234e5a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df24fc29-1308-4786-a68e-ee2ef06ebf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b937dea-09ec-4559-ba7d-3e5f7b3a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client with your API key (set your actual key here or via an environment variable)\n",
    "client = Groq(\n",
    "    api_key=\"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\"   # ‚Üê replace with your real key (or load it from os.environ)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fcee4-9a84-474d-97f2-4d2a973d9f15",
   "metadata": {},
   "source": [
    "Option 1: Two-Stage (Start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50319a-3a32-4f13-b04f-77f6eabad783",
   "metadata": {},
   "source": [
    "        Example 2:\n",
    "        Question: What is the role of reinforcement learning in AI?\n",
    "        Answer: Reinforcement learning (RL) enables AI agents to learn optimal behaviors through trial and error by maximizing cumulative rewards from environment interactions. It is pivotal in sequential decision-making tasks like game playing (e.g., AlphaGo), robotics, and autonomous systems. RL algorithms iteratively improve policies, balancing exploration and exploitation [Sutton & Barto, 2018; corpus: 2259]. Recent advances integrate deep learning for complex environments [Arulkumaran et al., 2020; corpus: 8403].\n",
    "\n",
    "        Example 3:\n",
    "        Question: How effective are BERT embeddings for document retrieval?\n",
    "        Answer: BBERT embeddings enhance document retrieval by capturing contextual semantics, outperforming traditional methods like TF-IDF in understanding query-document relevance. However, their effectiveness depends on fine-tuning for specific tasks and computational resources. BERT‚Äôs fixed-length inputs can truncate long documents, limiting full-context use. Hybrid approaches (e.g., BERT with BM25) often yield optimal results [Nogueira et al., 2019; corpus: 8596]. Domain adaptation further boosts performance [Zhan et al., 2021; corpus: 23884].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5fa8b-cd4a-474b-adc9-690b5d5cf9dc",
   "metadata": {},
   "source": [
    "Since you have a fervour for well-cited answers, I trust your ability to do this job well and look forward to reading your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1b85d7-399d-45a0-8bf8-bd599510f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query \n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "\n",
    "\n",
    "        Now, process this query\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard)\":\n",
    "        return f\"\"\"Let's break down the question step by step:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Role\":\n",
    "        return f\"\"\"You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Now, process this query:\n",
    "\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.        \n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Negative)\":\n",
    "        return f\"\"\"\n",
    "        Please roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility that will be lifechanging.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "    \n",
    "    elif strategy == \"Instruction + Role + Motivation (Positive)\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Positive) + Example\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        You are known for your ability to generate quality, well-cited answers.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "        The following is an example of how to answer:\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Instruction + Role + Motivation (Negative) + Example\":\n",
    "        return f\"\"\"\n",
    "        You are to roleplay as a researcher specializing in Computer Science and Natural Language Processing.\n",
    "        Failure to cite correctly would lead to irreversible damage to credibility.\n",
    "        Use the following steps:\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "        The following is an example of how to answer:\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformers excel over LSTMs in handling long-range dependencies via self-attention, enabling parallel computation and capturing global context. LSTMs process sequences sequentially, limiting speed and struggling with very long sequences. Transformers dominate in NLP tasks (e.g., BERT, GPT) but require more data and compute [Vaswani et al., 2017; corpus: 9710]. LSTMs remain useful for low-resource or small-scale tasks [Yadav et al., 2021; corpus: 11047].\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Neutral)\":\n",
    "        return f\"\"\"You are an AI assistant tasked with answering a research question using a set of documents retrieved from a scientific corpus. Your answer must be concise, factually accurate, and include all relevant corpus citations. \n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers and excellent citation accuracy. Citing the correct sources is one of the most important ways you demonstrate trustworthiness, attention to detail, and helpfulness. Providing accurate, complete citations helps users understand and trust your response. Be sure to cite every relevant document, and include all sources that support your answer.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated for factual accuracy and proper attribution. Providing incomplete, incorrect, or hallucinated citations will result in penalization and reduce the trustworthiness of your response. Only cite sources that are directly relevant and verifiable from the retrieved documents. Do not fabricate or omit citations.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Academic Aspiration)\":\n",
    "        return f\"\"\"You are a rising academic in the field of artificial intelligence. Your work is beginning to attract attention, and every paper you write contributes to your growing reputation. It‚Äôs essential that your answers are accurate, well-supported, and ethically cited. Misattributed or missing citations can damage your credibility and compromise the trust of the research community. Cite with care, and write like your future depends on it.\n",
    "        The query should be answered using the following steps:\n",
    "\n",
    "        Step 1: Identify key concepts in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant corpus IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification\":\n",
    "        return f\"\"\"\n",
    "    Generate an answer to the question using the documents. Then:\n",
    "    1. List each factual claim and its citation (e.g., \"Claim: ... [corpus ID]\").\n",
    "    2. Verify if the citation supports the claim. If not, state: \"Unsupported: [claim]\".\n",
    "    3. Revise the answer to remove unsupported claims.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification\":\n",
    "        return f\"\"\"\n",
    "        Answer the question as follows:\n",
    "        1. **Draft**: Write an initial answer with citations (e.g., [corpus ID]).\n",
    "        2. **Verify**: For each claim, check if the cited document supports it. If not, find a valid citation.\n",
    "        3. **Revise**: Rewrite the answer using only verified citations. Highlight changes.\n",
    "        4. **Finalize**: Output the revised answer and list verified citations.\n",
    "\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation\":\n",
    "        return f\"\"\"Answer the question step by step using only the provided documents (some may be irrelevant). \n",
    "        Cite the relevant document ID for each factual claim.\n",
    "        Conclude with 'The answer is: [answer]'. \n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote\":\n",
    "        return f\"\"\"Answer the question step by step using only the provided documents (some may be irrelevant).\n",
    "        For each step, include an exact quote from the document and cite it like ‚Äòquote‚Äô\n",
    "        Conclude with \"The answer is: [answer]\".\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"Academic Aspiration (2-stage)\":\n",
    "        return f\"\"\"You are a rising academic in the field of artificial intelligence. Your work is beginning to attract attention, and every paper you write contributes to your growing reputation. It‚Äôs essential that your answers are accurate, well-supported, and ethically cited. Misattributed or missing citations can damage your credibility and compromise the trust of the research community. Cite with care, and write like your future depends on it. \n",
    "    Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated for factual accuracy and proper attribution. Providing incomplete, incorrect, or hallucinated citations will result in penalization and reduce the trustworthiness of your response. Only cite sources that are directly relevant and verifiable from the retrieved documents.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise 2 stage)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers and excellent citation accuracy. Citing the correct sources is one of the most important ways you demonstrate trustworthiness, attention to detail, and helpfulness. Providing accurate, complete citations helps users understand and trust your response.\n",
    "        Please acknowledge this role and confirm that you're ready to proceed.\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (role-gen)\":\n",
    "        return f\"\"\"You are an expert at role selection for problem solving. Generate a professional role description following this format:\n",
    "\n",
    "A: This question is a [DOMAIN] problem involving [KEY CONCEPTS]. To better solve it, I will act as a [SPECIFIC ROLE] who [EXPERTISE DESCRIPTION].\n",
    "\n",
    "Examples:\n",
    "Q: Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
    "A: This question is a neuroscience problem involving cell biology and migration. To better solve it, I will act as a neuroscientist who specializes in the study of the brain and its cellular behaviors.\n",
    "\n",
    "Q: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "A: This question is a computer vision problem involving architectural comparisons. To better solve it, I will act as a deep learning researcher who focuses on model architecture analysis and performance benchmarking.\n",
    "\n",
    "Now generate a role description for:\n",
    "Q: {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning (answer)\":\n",
    "        # This will be populated dynamically with the generated role\n",
    "        return f\"\"\"{query['generated_role']}\n",
    "\n",
    "Using this expertise and the retrieved documents below, provide a comprehensive answer:\n",
    "{query}\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage1)\":\n",
    "        return f\"\"\"You are an expert research assistant. Paraphrase specific questions into fundamental scientific inquiries:\n",
    "\n",
    "        Examples:\n",
    "        Original: How does contrastive learning improve representation learning in vision transformers?\n",
    "        Stepback: What are the fundamental mechanisms by which self-supervised learning techniques enhance representation learning in transformer architectures?\n",
    "\n",
    "        Original: What explains the performance gap between ResNet-50 and Vision Transformer on ImageNet-1k?\n",
    "        Stepback: What architectural differences between convolutional networks and transformer models affect computer vision performance metrics?\n",
    "\n",
    "        Original: How effective is chain-of-thought prompting for mathematical reasoning in PaLM-2?\n",
    "        Stepback: What cognitive science principles underlie the effectiveness of decomposition strategies in large language model reasoning?\n",
    "\n",
    "        Original: What causes gradient instability in 4-bit quantized LLM fine-tuning?\n",
    "        Stepback: What numerical precision challenges emerge during post-training quantization of neural network parameters?\n",
    "\n",
    "        Original: How does Med-PaLM 2 achieve state-of-the-art on medical QA benchmarks?\n",
    "        Stepback: What architectural adaptations and training strategies improve language model performance in domain-specific knowledge tasks?\n",
    "\n",
    "        Now generate a step-back question for:\n",
    "        Original: {query}\n",
    "        Stepback:\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting (stage2)\":\n",
    "        return f\"\"\"You are an expert at world knowledge. Use both contexts to answer:\n",
    "        \n",
    "        Original Context:\n",
    "        {query['original_context']}\n",
    "        \n",
    "        Stepback Context:\n",
    "        {query['stepback_context']}\n",
    "        \n",
    "        Original Question: {query['original_question']}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts, scads_api_key,strategy=None):\n",
    "#    \"\"\"Generates a concise answer using RAG, citing only the top 1‚Äì2 relevant sources\"\"\"\n",
    "def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts, scads_api_key, strategy=None,corpus_df=None):\n",
    "    \"\"\"Generates a concise answer using RAG, citing only the top 1‚Äì2 relevant sources\"\"\"\n",
    "    context = \"\\n\".join([\n",
    "        f\"Corpus ID: {doc_id}\\nTitle: {title}\\nFull Text: {full_text}\"\n",
    "        for doc_id, title, full_text in zip(retrieved_corpus_ids, retrieved_titles, retrieved_texts)\n",
    "    ])\n",
    "\n",
    "    url = \"https://llm.scads.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {scads_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # üß† Stage 1: Send role-setting prompt and get confirmation\n",
    "    if strategy == \"Academic Aspiration (2-stage)\"  or strategy == \"chain-of-thought (Punishment 2 stage)\"  or strategy == \"chain-of-thought (Praise 2 stage)\":\n",
    "        role_prompt = modified_query\n",
    "        role_response = requests.post(url, headers=headers, json={\n",
    "            \"model\": \"openGPT-X/Teuken-7B-instruct-research-v0.4\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": role_prompt}]\n",
    "        })\n",
    "        role_response.raise_for_status()\n",
    "        assistant_ack = role_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # üëá Stage 2: Now send the real question with retrieved context\n",
    "        main_prompt = f\"\"\"\n",
    "{assistant_ack}\n",
    "Only cite sources that are directly relevant and verifiable from the retrieved documents. Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {original_query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning\":\n",
    "        # Stage 1: Generate role description\n",
    "        role_prompt = reformulate_query(original_query, \"self-prompt-tuning (role-gen)\")\n",
    "        role_response = requests.post(url, headers=headers, json={\n",
    "            \"model\": \"openGPT-X/Teuken-7B-instruct-research-v0.4\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": role_prompt}]\n",
    "        }).json()\n",
    "        generated_role = role_response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        \n",
    "        # Stage 2: Generate answer with role context\n",
    "        answer_prompt = reformulate_query({\n",
    "            'generated_role': generated_role,\n",
    "            'query': original_query\n",
    "        }, \"self-prompt-tuning (answer)\")\n",
    "        \n",
    "        main_prompt = f\"\"\"\n",
    "        Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {answer_prompt}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    elif strategy == \"step-back prompting\":\n",
    "        # üß† Stage 1: Generate step-back question\n",
    "        stage1_prompt = reformulate_query(original_query, \"step-back prompting (stage1)\")\n",
    "        stage1_response = requests.post(url, headers=headers, json={\n",
    "            \"model\": \"openGPT-X/Teuken-7B-instruct-research-v0.4\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": stage1_prompt}]\n",
    "        })\n",
    "        stage1_response.raise_for_status()\n",
    "        stepback_question = stage1_response.json()[\"choices\"][0][\"message\"][\"content\"].split('\\n')[-1].strip()\n",
    "\n",
    "        # üîç Retrieve documents for both questions\n",
    "        original_ids = retriever.retrieve(original_query, k=10)\n",
    "        stepback_ids = retriever.retrieve(stepback_question, k=10)\n",
    "        \n",
    "        # üìö Get document details\n",
    "        original_docs = pd.DataFrame({\"corpusid\": original_ids}).merge(\n",
    "            corpus_df[[\"corpusid\", \"title\", \"abstract\"]], \n",
    "            on=\"corpusid\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "        stepback_docs = pd.DataFrame({\"corpusid\": stepback_ids}).merge(\n",
    "            corpus_df[[\"corpusid\", \"title\", \"abstract\"]], \n",
    "            on=\"corpusid\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # üìù Stage 2: Generate final answer with combined context\n",
    "        final_prompt = reformulate_query({\n",
    "            'original_question': original_query,\n",
    "            'original_context': format_context(original_docs),\n",
    "            'stepback_context': format_context(stepback_docs),\n",
    "            'stepback_question': stepback_question\n",
    "        }, \"step-back prompting (stage2)\")\n",
    "\n",
    "        main_prompt = f\"\"\"\n",
    "{final_prompt}\n",
    "Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\"\"\"\n",
    "    else:\n",
    "        # regular strategy\n",
    "        main_prompt = f\"\"\"\n",
    "Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {modified_query}\n",
    "\"\"\"\n",
    "\n",
    "    # üîÅ Final API call (either from single or 2-stage flow)\n",
    "    response = requests.post(url, headers=headers, json={\n",
    "        \"model\": \"openGPT-X/Teuken-7B-instruct-research-v0.4\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": main_prompt}]\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ea565f-6b02-4845-b2b9-a7d1d6f6e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts):\n",
    "    \"\"\"Answer the question using the retrieved documents where relevant, explicitly citing the corpus id's of the documents you cite\"\"\"\n",
    "    context = \"\\n\".join([\n",
    "        f\"Corpus ID: {doc_id}\\nTitle: {title}\\nFull Text: {full_text}\"\n",
    "        for doc_id, title, full_text in zip(retrieved_corpus_ids, retrieved_titles, retrieved_texts)\n",
    "    ])\n",
    "\n",
    "    # üß† Stage 1: Send role-setting prompt and get confirmation\n",
    "    if strategy == \"Academic Aspiration (2-stage)\" or strategy == \"chain-of-thought (Punishment 2 stage)\" or strategy == \"chain-of-thought (Praise 2 stage)\":\n",
    "        role_prompt = modified_query\n",
    "        role_response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": role_prompt}]\n",
    "        )\n",
    "        assistant_ack = role_response.choices[0].message.content\n",
    "\n",
    "        # üëá Stage 2: Now send the real question with retrieved context\n",
    "        main_prompt = f\"\"\"\n",
    "{assistant_ack}\n",
    "Only cite sources that are directly relevant and verifiable from the retrieved documents. Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {original_query}\n",
    "\"\"\"\n",
    "\n",
    "    elif strategy == \"self-prompt-tuning\":\n",
    "        # Stage 1: Generate role description\n",
    "        role_prompt = reformulate_query(original_query, \"self-prompt-tuning (role-gen)\")\n",
    "        role_response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": role_prompt}]\n",
    "        )\n",
    "        generated_role = role_response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Stage 2: Generate answer with role context\n",
    "        answer_prompt = reformulate_query({\n",
    "            'generated_role': generated_role,\n",
    "            'query': original_query\n",
    "        }, \"self-prompt-tuning (answer)\")\n",
    "        \n",
    "        main_prompt = f\"\"\"\n",
    "        Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {answer_prompt}\n",
    "\"\"\"\n",
    "\n",
    "    elif strategy == \"step-back prompting\":\n",
    "        # üß† Stage 1: Generate step-back question\n",
    "        stage1_prompt = reformulate_query(original_query, \"step-back prompting (stage1)\")\n",
    "        stage1_response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": stage1_prompt}]\n",
    "        )\n",
    "        stepback_question = stage1_response.choices[0].message.content.split('\\n')[-1].strip()\n",
    "\n",
    "        # üîç Retrieve documents for both questions\n",
    "        original_ids = retriever.retrieve(original_query, k=10)\n",
    "        stepback_ids = retriever.retrieve(stepback_question, k=10)\n",
    "        \n",
    "        # üìö Get document details\n",
    "        original_docs = pd.DataFrame({\"corpusid\": original_ids}).merge(\n",
    "            corpus_df[[\"corpusid\", \"title\", \"abstract\"]], \n",
    "            on=\"corpusid\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "        stepback_docs = pd.DataFrame({\"corpusid\": stepback_ids}).merge(\n",
    "            corpus_df[[\"corpusid\", \"title\", \"abstract\"]], \n",
    "            on=\"corpusid\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # üìù Stage 2: Generate final answer with combined context\n",
    "        final_prompt = reformulate_query({\n",
    "            'original_question': original_query,\n",
    "            'original_context': format_context(original_docs),\n",
    "            'stepback_context': format_context(stepback_docs),\n",
    "            'stepback_question': stepback_question\n",
    "        }, \"step-back prompting (stage2)\")\n",
    "\n",
    "        main_prompt = f\"\"\"\n",
    "{final_prompt}\n",
    "Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\"\"\"\n",
    "    else:\n",
    "        # regular strategy\n",
    "        main_prompt = f\"\"\"\n",
    "Use this format for citation: corpus ID: [document_id].\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {modified_query}\n",
    "\"\"\"\n",
    "\n",
    "    # üîÅ Final API call (either from single or 2-stage flow)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": main_prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0719c28-de26-46d3-a7a1-b7be241c12a7",
   "metadata": {},
   "source": [
    "Option 2: Simple (End)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f20d4-9d47-4f47-88bf-56ca1fc35a45",
   "metadata": {},
   "source": [
    "Option 2: Simple (start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42cf4f60-a67e-4594-98d5-9164d34dcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query  # No modification, use as-is.\n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "\n",
    "        Example 1:\n",
    "        Question: How do transformer models compare to LSTMs?\n",
    "        Answer: Transformer models outperform LSTMs due to their ability to capture long-range dependencies. \n",
    "        (Source: \"Transformer Models for NLP\", Corpus ID: 12345)\n",
    "\n",
    "        Example 2:\n",
    "        Question: What is the role of reinforcement learning in AI?\n",
    "        Answer: Reinforcement learning is widely used in robotics, game playing, and automated control tasks. \n",
    "        (Source: \"Reinforcement Learning in AI Applications\", Corpus ID: 67890)\n",
    "\n",
    "        Example 3:\n",
    "        Question: How effective are BERT embeddings for document retrieval?\n",
    "        Answer: BERT embeddings improve retrieval performance by encoding semantic similarity, outperforming traditional TF-IDF methods.\n",
    "        (Source: \"Advancements in Document Retrieval using BERT\", Corpus ID: 11223)\n",
    "\n",
    "        Now, using the retrieved documents below, answer the question while citing sources explicitly:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought\":\n",
    "        return f\"\"\"Let's break down the question step by step:\n",
    "\n",
    "        Step 1: Identify key concepts and entities in the question.\n",
    "        Step 2: Search for relevant supporting evidence from retrieved documents.\n",
    "        Step 3: Construct an answer using the evidence while explicitly citing the relevant document IDs.\n",
    "\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "\n",
    "def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts):\n",
    "    \"\"\" Generates an answer using RAG, keeping retrieval separate from query modification. \"\"\"\n",
    "    context = \"\\n\".join([\n",
    "        f\"Document ID: {doc_id}\\nTitle: {title}\\nFull Text: {full_text}\"\n",
    "        for doc_id, title, full_text in zip(retrieved_corpus_ids, retrieved_titles, retrieved_texts)\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"Use the following retrieved documents to answer the question.\n",
    "\n",
    "    Retrieved Documents:\n",
    "    {context}\n",
    "\n",
    "    Question: {modified_query}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",#\"llama3-8b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI that generates well-supported answers using retrieved documents.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content  # Return the generated answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5d103-9be4-4f17-bc4e-bc1aa5729f56",
   "metadata": {},
   "source": [
    "Option 2: Simple (End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d62206-9141-4685-b3d0-4ad4d437b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gritlm import GritLM\n",
    "\n",
    "# Configuration\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "GROQ_API_KEY = \"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\"\n",
    "os.makedirs(WORKSPACE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ed612c-5931-4e4f-b6c7-7153f75a65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "E5_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"e5_index.pkl\")\n",
    "GTR_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"gtr_index.pkl\")\n",
    "GRIT_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"grit_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f111b1b0-2587-408a-b5c2-bfcb5c8132a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace set up at: /data/horse/ws/uskh580e-myws\n",
      "Datasets loaded successfully and stored in: /data/horse/ws/uskh580e-myws/huggingface_datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from groq import Groq\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define workspace path\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "\n",
    "# Set environment variables to store large files in workspace\n",
    "os.environ[\"HF_HOME\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"huggingface\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(WORKSPACE_PATH, \"datasets\")\n",
    "os.environ[\"NLTK_DATA\"] = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TRANSFORMERS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"HF_DATASETS_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"NLTK_DATA\"], exist_ok=True)\n",
    "BM25_INDEX_PATH = os.path.join(WORKSPACE_PATH, \"bm25_index.pkl\")\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "NLTK_PATH = os.path.join(WORKSPACE_PATH, \"nltk_data\")\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(NLTK_PATH, exist_ok=True)\n",
    "\n",
    "# Set NLTK data path\n",
    "nltk.data.path.append(NLTK_PATH)\n",
    "\n",
    "# Force download punkt & stopwords\n",
    "nltk.download(\"punkt\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"stopwords\", download_dir=NLTK_PATH, force=True)\n",
    "nltk.download(\"wordnet\", download_dir=NLTK_PATH, force=True)\n",
    "print(f\"Workspace set up at: {WORKSPACE_PATH}\")\n",
    "DATASET_CACHE_DIR = os.path.join(WORKSPACE_PATH, \"huggingface_datasets\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(DATASET_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Load datasets and store them in workspace cache\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\", cache_dir=DATASET_CACHE_DIR)\n",
    "\n",
    "print(\"Datasets loaded successfully and stored in:\", DATASET_CACHE_DIR)\n",
    "\n",
    "query_df = query_data.to_pandas()\n",
    "corpus_clean_df = corpus_clean_data.to_pandas()\n",
    "corpus_s2orc_df = corpus_s2orc_data.to_pandas()\n",
    "corpus_df = corpus_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be1979d-72f6-4790-929e-552cdd258ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded cached E5 embeddings\n",
      "‚úÖ Loaded cached GTR embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db57a1b2ab4f40ba95fed3aeffcc15af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created GritLM: torch.bfloat16 dtype, mean pool, embedding mode, bbcc attn\n",
      "‚úÖ Loaded cached GRIT embeddings\n"
     ]
    }
   ],
   "source": [
    "class E5Retriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = SentenceTransformer(\"intfloat/e5-large-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save E5 embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "\n",
    "        print(\"üîÑ Computing E5 embeddings...\")\n",
    "        texts = [\"passage: \" + text for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        self.embeddings = self.model.encode(texts, batch_size=256, normalize_embeddings=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(E5_INDEX_PATH):\n",
    "            try:\n",
    "                with open(E5_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"‚úÖ Loaded cached E5 embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(E5_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        query_embedding = self.model.encode([\"query: \" + query], normalize_embeddings=True)\n",
    "        scores = np.dot(query_embedding, self.embeddings.T)\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ‚úÖ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "e5_retriever = E5Retriever(corpus_df)\n",
    "\n",
    "if e5_retriever.embeddings is None:\n",
    "    print(\"‚ö†Ô∏è E5 embeddings not found, computing...\")\n",
    "    e5_retriever._prepare_embeddings()\n",
    "class GTRRetriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = SentenceTransformer(\"sentence-transformers/gtr-t5-large\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save GTR embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "        \n",
    "        print(\"üîÑ Computing GTR embeddings...\")\n",
    "        texts = self.corpus_df[\"abstract\"].astype(str)\n",
    "        self.embeddings = self.model.encode(texts, batch_size=256, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(GTR_INDEX_PATH):\n",
    "            try:\n",
    "                with open(GTR_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"‚úÖ Loaded cached GTR embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(GTR_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ‚úÖ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "gtr_retriever = GTRRetriever(corpus_df)\n",
    "\n",
    "if gtr_retriever.embeddings is None:\n",
    "    print(\"‚ö†Ô∏è GTR embeddings not found, computing...\")\n",
    "    gtr_retriever._prepare_embeddings()\n",
    "class GRITRetriever:\n",
    "    def __init__(self, corpus_df):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model = GritLM(\"GritLM/GritLM-7B\", device_map=\"auto\", torch_dtype=torch.bfloat16, mode=\"embedding\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Compute and save GRITLM embeddings if not cached.\"\"\"\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "\n",
    "        print(\"üîÑ Computing GRIT embeddings...\")\n",
    "        texts = [f\"<|embed|>\\n{text}\" for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        self.embeddings = self.model.encode(texts, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        \"\"\"Load cached embeddings if available.\"\"\"\n",
    "        if os.path.exists(GRIT_INDEX_PATH):\n",
    "            try:\n",
    "                with open(GRIT_INDEX_PATH, \"rb\") as f:\n",
    "                    self.embeddings = pickle.load(f)\n",
    "                    print(\"‚úÖ Loaded cached GRIT embeddings\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è Failed to load cache.\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save embeddings to cache.\"\"\"\n",
    "        with open(GRIT_INDEX_PATH, \"wb\") as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\"Retrieve top-k documents for a query.\"\"\"\n",
    "        formatted_query = f\"<|user|>\\nRepresent this query for retrieving relevant documents:\\n<|embed|>\\n{query}\"\n",
    "        query_embedding = self.model.encode([formatted_query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        ranked_indices = np.argsort(-scores[0])[:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()\n",
    "\n",
    "# ‚úÖ **Ensure Retrieval Occurs Before Query Reformulation**\n",
    "grit_retriever = GRITRetriever(corpus_df)\n",
    "\n",
    "if grit_retriever.embeddings is None:\n",
    "    print(\"‚ö†Ô∏è GRITLM embeddings not found, computing...\")\n",
    "    grit_retriever._prepare_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d874598-e46d-47d0-b952-975619488461",
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in [\"zero-shot\", \"few-shot\", \"chain-of-thought (Standard)\",\"chain-of-thought (Neutral)\",\"chain-of-thought (Praise)\",\n",
    "                 \"chain-of-thought (Punishment)\",\n",
    "                \"chain-of-verification\",\"chain-of-citation\",\"chain-of-quote\",\"Academic Aspiration (2-stage)\",\n",
    "                 \"chain-of-thought (Punishment 2 stage)\",\"chain-of-thought (Praise 2 stage)\",\"self-prompt-tuning (role-gen)\",\"self-prompt-tuning\",\"step-back prompting\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d4eb52-c9f8-4447-9c52-624fd86a3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(retrieved_docs):\n",
    "    if isinstance(retrieved_docs, pd.DataFrame):\n",
    "        return \"\\n\".join([\n",
    "            f\"Corpus ID: {row['corpusid']}\\nTitle: {row['title']}\\nAbstract: {row['abstract']}\"\n",
    "            for _, row in retrieved_docs.iterrows()\n",
    "        ])\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7932861-9fb8-4252-a07d-741d7dc2d4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running Retrieval with GRITLM on 10% dataset...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving with GRITLM: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 298/298 [1:02:43<00:00, 12.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Query</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Retrieved Corpus IDs</th>\n",
       "      <th>Retrieved Abstracts</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Cited Corpus IDs</th>\n",
       "      <th>Ground Truth Corpus IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Can you recommend research that uses an LLM to...</td>\n",
       "      <td>Instruction + Role + Motivation (Negative) + E...</td>\n",
       "      <td>[258960666, 245218561, 252596001, 252716013, 2...</td>\n",
       "      <td>[Retrieval augmentation can aid language model...</td>\n",
       "      <td>Here's the answer:\\n\\nQuestion: Can you recomm...</td>\n",
       "      <td>[258960666, 252596001, 252716013, 258041372]</td>\n",
       "      <td>[229923710]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Can you recommend research that uses an LLM to...</td>\n",
       "      <td>Instruction + Role + Motivation (Positive) + E...</td>\n",
       "      <td>[258960666, 245218561, 252596001, 252716013, 2...</td>\n",
       "      <td>[Retrieval augmentation can aid language model...</td>\n",
       "      <td>Based on the retrieved documents, I identified...</td>\n",
       "      <td>[258960666, 252596001, 252716013]</td>\n",
       "      <td>[229923710]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Can you recommend research that uses an LLM to...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>[258960666, 245218561, 252596001, 252716013, 2...</td>\n",
       "      <td>[Retrieval augmentation can aid language model...</td>\n",
       "      <td>Based on the provided corpus IDs, I recommend ...</td>\n",
       "      <td>[258960666, 252596001, 258041372]</td>\n",
       "      <td>[229923710]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Can you recommend research that uses an LLM to...</td>\n",
       "      <td>few-shot</td>\n",
       "      <td>[258960666, 245218561, 252596001, 252716013, 2...</td>\n",
       "      <td>[Retrieval augmentation can aid language model...</td>\n",
       "      <td>Based on the retrieved documents, I can recomm...</td>\n",
       "      <td>[258960666, 252596001, 252716013]</td>\n",
       "      <td>[229923710]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Can you recommend research that uses an LLM to...</td>\n",
       "      <td>chain-of-thought (Standard)</td>\n",
       "      <td>[258960666, 245218561, 252596001, 252716013, 2...</td>\n",
       "      <td>[Retrieval augmentation can aid language model...</td>\n",
       "      <td>Based on the retrieved documents, I can sugges...</td>\n",
       "      <td>[258960666, 252596001, 252716013]</td>\n",
       "      <td>[229923710]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Could you suggest a study that explores data a...</td>\n",
       "      <td>Motivation (Positive)</td>\n",
       "      <td>[776271, 15960453, 10435668, 49534653, 6452962...</td>\n",
       "      <td>[Anaphoric shell nouns such as this issue and ...</td>\n",
       "      <td>Based on the provided corpus, I'd like to sugg...</td>\n",
       "      <td>[776271]</td>\n",
       "      <td>[245130931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Could you suggest a study that explores data a...</td>\n",
       "      <td>Role + Motivation (Negative)</td>\n",
       "      <td>[776271, 15960453, 10435668, 49534653, 6452962...</td>\n",
       "      <td>[Anaphoric shell nouns such as this issue and ...</td>\n",
       "      <td>A very specific and important question!\\n\\nAft...</td>\n",
       "      <td>[15960453]</td>\n",
       "      <td>[245130931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Could you suggest a study that explores data a...</td>\n",
       "      <td>Role + Motivation (Positive)</td>\n",
       "      <td>[776271, 15960453, 10435668, 49534653, 6452962...</td>\n",
       "      <td>[Anaphoric shell nouns such as this issue and ...</td>\n",
       "      <td>What a great question! \\n\\nAfter reviewing the...</td>\n",
       "      <td>[15960453]</td>\n",
       "      <td>[245130931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Could you suggest a study that explores data a...</td>\n",
       "      <td>Instruction + Role + Motivation (Negative)</td>\n",
       "      <td>[776271, 15960453, 10435668, 49534653, 6452962...</td>\n",
       "      <td>[Anaphoric shell nouns such as this issue and ...</td>\n",
       "      <td>What a great question! As a researcher special...</td>\n",
       "      <td>[15960453, 10435668, 6452962]</td>\n",
       "      <td>[245130931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>GRITLM</td>\n",
       "      <td>Could you suggest a study that explores data a...</td>\n",
       "      <td>Instruction + Role + Motivation (Positive)</td>\n",
       "      <td>[776271, 15960453, 10435668, 49534653, 6452962...</td>\n",
       "      <td>[Anaphoric shell nouns such as this issue and ...</td>\n",
       "      <td>A great question!\\n\\nAfter analyzing the query...</td>\n",
       "      <td>[776271, 15960453, 10435668]</td>\n",
       "      <td>[245130931]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3576 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Retriever                                              Query  \\\n",
       "0       GRITLM  Can you recommend research that uses an LLM to...   \n",
       "1       GRITLM  Can you recommend research that uses an LLM to...   \n",
       "2       GRITLM  Can you recommend research that uses an LLM to...   \n",
       "3       GRITLM  Can you recommend research that uses an LLM to...   \n",
       "4       GRITLM  Can you recommend research that uses an LLM to...   \n",
       "...        ...                                                ...   \n",
       "3571    GRITLM  Could you suggest a study that explores data a...   \n",
       "3572    GRITLM  Could you suggest a study that explores data a...   \n",
       "3573    GRITLM  Could you suggest a study that explores data a...   \n",
       "3574    GRITLM  Could you suggest a study that explores data a...   \n",
       "3575    GRITLM  Could you suggest a study that explores data a...   \n",
       "\n",
       "                                               Strategy  \\\n",
       "0     Instruction + Role + Motivation (Negative) + E...   \n",
       "1     Instruction + Role + Motivation (Positive) + E...   \n",
       "2                                             zero-shot   \n",
       "3                                              few-shot   \n",
       "4                           chain-of-thought (Standard)   \n",
       "...                                                 ...   \n",
       "3571                              Motivation (Positive)   \n",
       "3572                       Role + Motivation (Negative)   \n",
       "3573                       Role + Motivation (Positive)   \n",
       "3574         Instruction + Role + Motivation (Negative)   \n",
       "3575         Instruction + Role + Motivation (Positive)   \n",
       "\n",
       "                                   Retrieved Corpus IDs  \\\n",
       "0     [258960666, 245218561, 252596001, 252716013, 2...   \n",
       "1     [258960666, 245218561, 252596001, 252716013, 2...   \n",
       "2     [258960666, 245218561, 252596001, 252716013, 2...   \n",
       "3     [258960666, 245218561, 252596001, 252716013, 2...   \n",
       "4     [258960666, 245218561, 252596001, 252716013, 2...   \n",
       "...                                                 ...   \n",
       "3571  [776271, 15960453, 10435668, 49534653, 6452962...   \n",
       "3572  [776271, 15960453, 10435668, 49534653, 6452962...   \n",
       "3573  [776271, 15960453, 10435668, 49534653, 6452962...   \n",
       "3574  [776271, 15960453, 10435668, 49534653, 6452962...   \n",
       "3575  [776271, 15960453, 10435668, 49534653, 6452962...   \n",
       "\n",
       "                                    Retrieved Abstracts  \\\n",
       "0     [Retrieval augmentation can aid language model...   \n",
       "1     [Retrieval augmentation can aid language model...   \n",
       "2     [Retrieval augmentation can aid language model...   \n",
       "3     [Retrieval augmentation can aid language model...   \n",
       "4     [Retrieval augmentation can aid language model...   \n",
       "...                                                 ...   \n",
       "3571  [Anaphoric shell nouns such as this issue and ...   \n",
       "3572  [Anaphoric shell nouns such as this issue and ...   \n",
       "3573  [Anaphoric shell nouns such as this issue and ...   \n",
       "3574  [Anaphoric shell nouns such as this issue and ...   \n",
       "3575  [Anaphoric shell nouns such as this issue and ...   \n",
       "\n",
       "                                       Generated Answer  \\\n",
       "0     Here's the answer:\\n\\nQuestion: Can you recomm...   \n",
       "1     Based on the retrieved documents, I identified...   \n",
       "2     Based on the provided corpus IDs, I recommend ...   \n",
       "3     Based on the retrieved documents, I can recomm...   \n",
       "4     Based on the retrieved documents, I can sugges...   \n",
       "...                                                 ...   \n",
       "3571  Based on the provided corpus, I'd like to sugg...   \n",
       "3572  A very specific and important question!\\n\\nAft...   \n",
       "3573  What a great question! \\n\\nAfter reviewing the...   \n",
       "3574  What a great question! As a researcher special...   \n",
       "3575  A great question!\\n\\nAfter analyzing the query...   \n",
       "\n",
       "                                  Cited Corpus IDs Ground Truth Corpus IDs  \n",
       "0     [258960666, 252596001, 252716013, 258041372]             [229923710]  \n",
       "1                [258960666, 252596001, 252716013]             [229923710]  \n",
       "2                [258960666, 252596001, 258041372]             [229923710]  \n",
       "3                [258960666, 252596001, 252716013]             [229923710]  \n",
       "4                [258960666, 252596001, 252716013]             [229923710]  \n",
       "...                                            ...                     ...  \n",
       "3571                                      [776271]             [245130931]  \n",
       "3572                                    [15960453]             [245130931]  \n",
       "3573                                    [15960453]             [245130931]  \n",
       "3574                 [15960453, 10435668, 6452962]             [245130931]  \n",
       "3575                  [776271, 15960453, 10435668]             [245130931]  \n",
       "\n",
       "[3576 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved at: /data/horse/ws/uskh580e-myws/rag_results_e5_gtr_gritlm_10percent.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from groq import Groq\n",
    "\n",
    "# ‚úÖ **Initialize Retrievers**\n",
    "retrievers = {\n",
    "    #\"E5\": e5_retriever,\n",
    "    #\"GTR\": gtr_retriever,\n",
    "    \"GRITLM\": grit_retriever\n",
    "}\n",
    "\n",
    "# ‚úÖ **Ensure Ground-Truth IDs are Correctly Formatted**\n",
    "query_df[\"corpusids\"] = query_df[\"corpusids\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# ‚úÖ **Sample Only 10% of the Dataset**\n",
    "query_sample_df = query_df.sample(frac=0.5, random_state=42)  # Randomly select 10% of rows\n",
    "\n",
    "# ‚úÖ **Run Retrieval, Then Reformulate Query and Run RAG**\n",
    "rag_results = []\n",
    "\n",
    "for retriever_name, retriever in retrievers.items():\n",
    "    print(f\"\\nüöÄ Running Retrieval with {retriever_name} on 10% dataset...\\n\")\n",
    "\n",
    "    for _, row in tqdm(query_sample_df.iterrows(), total=len(query_sample_df), desc=f\"Retrieving with {retriever_name}\"):\n",
    "        original_query = row[\"query\"]\n",
    "        ground_truth_corpus_ids = row[\"corpusids\"]\n",
    "\n",
    "        # **Step 1: Retrieve First, Before Query Reformulation**\n",
    "        retrieved_ids = retriever.retrieve(original_query, k=20)\n",
    "        retrieved_ids_for_llm = retrieved_ids[:5]\n",
    "        \n",
    "        # **Fetch retrieved document details**\n",
    "        ranked_retrieved_df = pd.DataFrame({\"corpusid\": retrieved_ids})\n",
    "        retrieved_docs_info = ranked_retrieved_df.merge(\n",
    "            corpus_df[[\"corpusid\", \"title\", \"abstract\"]], \n",
    "            on=\"corpusid\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "        retrieved_corpus_ids = retrieved_docs_info[\"corpusid\"].tolist()\n",
    "        retrieved_titles = retrieved_docs_info[\"title\"].tolist()\n",
    "        retrieved_abstracts = retrieved_docs_info[\"abstract\"].tolist()\n",
    "\n",
    "        # **Step 2: Reformulate Query AFTER Retrieval**\n",
    "        #for strategy in [\"zero-shot\", \"few-shot\", \"chain-of-thought (Standard)\",\"chain-of-thought (Neutral)\",\"chain-of-thought (Praise)\",\"chain-of-thought (Punishment)\"]:\n",
    "        for strategy in [\"Instruction + Role + Motivation (Negative) + Example\",\"Instruction + Role + Motivation (Positive) + Example\",\"zero-shot\", \"few-shot\", \"chain-of-thought (Standard)\",\"Role\",\"Motivation (Negative)\",\"Motivation (Positive)\",\"Role + Motivation (Negative)\",\"Role + Motivation (Positive)\",\"Instruction + Role + Motivation (Negative)\",\"Instruction + Role + Motivation (Positive)\"]:   \n",
    "            modified_query = reformulate_query(original_query, strategy)\n",
    "\n",
    "            # **Step 3: Run LLM with Retrieved Documents**\n",
    "            generated_answer = generate_with_rag(original_query, modified_query, retrieved_ids_for_llm, retrieved_titles[:5], retrieved_abstracts[:5])\n",
    "\n",
    "            # **Step 4: Extract Cited Corpus IDs from LLM Response**\n",
    "            cited_corpus_ids = [str(doc_id) for doc_id in retrieved_ids_for_llm if str(doc_id) in generated_answer]\n",
    "\n",
    "            # **Step 5: Store Results**\n",
    "            rag_results.append({\n",
    "                \"Retriever\": retriever_name,\n",
    "                \"Query\": original_query,\n",
    "                \"Strategy\": strategy,\n",
    "                \"Retrieved Corpus IDs\": retrieved_corpus_ids,\n",
    "                \"Retrieved Abstracts\": retrieved_abstracts,\n",
    "                \"Generated Answer\": generated_answer,\n",
    "                \"Cited Corpus IDs\": cited_corpus_ids,\n",
    "                \"Ground Truth Corpus IDs\": ground_truth_corpus_ids\n",
    "            })\n",
    "\n",
    "# ‚úÖ Convert Results to DataFrame\n",
    "rag_results_df = pd.DataFrame(rag_results)\n",
    "\n",
    "# ‚úÖ Save Results\n",
    "RAG_RESULTS_PATH = os.path.join(WORKSPACE_PATH, \"rag_results_e5_gtr_gritlm_10percent.csv\")\n",
    "rag_results_df.to_csv(RAG_RESULTS_PATH, index=False)\n",
    "display(rag_results_df)\n",
    "\n",
    "print(f\"‚úÖ Results saved at: {RAG_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b814da85-9ecb-4c30-be9c-1ff2eb631a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/horse/ws/uskh580e-myws/rag_results_e5_gtr_gritlm_10percent.csv\"\n",
    "rag_results_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660d62f7-0cef-496c-ac99-2cdfea08278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Retrieved Corpus IDs Ground Truth Corpus IDs  \\\n",
      "0  [258960666, 245218561, 252596001, 252716013, 2...             [229923710]   \n",
      "1  [258960666, 245218561, 252596001, 252716013, 2...             [229923710]   \n",
      "2  [258960666, 245218561, 252596001, 252716013, 2...             [229923710]   \n",
      "3  [258960666, 245218561, 252596001, 252716013, 2...             [229923710]   \n",
      "4  [258960666, 245218561, 252596001, 252716013, 2...             [229923710]   \n",
      "\n",
      "                                    Cited Corpus IDs  \n",
      "0  ['258960666', '252596001', '252716013', '25804...  \n",
      "1            ['258960666', '252596001', '252716013']  \n",
      "2            ['258960666', '252596001', '258041372']  \n",
      "3            ['258960666', '252596001', '252716013']  \n",
      "4            ['258960666', '252596001', '252716013']  \n"
     ]
    }
   ],
   "source": [
    "print(rag_results_df[[\"Retrieved Corpus IDs\", \"Ground Truth Corpus IDs\", \"Cited Corpus IDs\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924d31e2-e433-4732-ad8a-3bef518ddd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Corpus IDs       [258960666, 245218561, 252596001, 252716013, 2...\n",
      "Ground Truth Corpus IDs                                          [229923710]\n",
      "Cited Corpus IDs                [258960666, 252596001, 252716013, 258041372]\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Recall@20</th>\n",
       "      <th>Citation Precision</th>\n",
       "      <th>Citation Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instruction + Role + Motivation (Negative)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.308166</td>\n",
       "      <td>0.531879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruction + Role + Motivation (Negative) + E...</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.284787</td>\n",
       "      <td>0.552013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instruction + Role + Motivation (Positive)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.309172</td>\n",
       "      <td>0.538031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instruction + Role + Motivation (Positive) + E...</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.271644</td>\n",
       "      <td>0.538031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Motivation (Negative)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.290324</td>\n",
       "      <td>0.522931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Motivation (Positive)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.300615</td>\n",
       "      <td>0.516219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Role</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.311130</td>\n",
       "      <td>0.532998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Role + Motivation (Negative)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.301007</td>\n",
       "      <td>0.527964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Role + Motivation (Positive)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.301678</td>\n",
       "      <td>0.531879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chain-of-thought (Standard)</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.287025</td>\n",
       "      <td>0.531879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>few-shot</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.351902</td>\n",
       "      <td>0.532998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zero-shot</td>\n",
       "      <td>0.598434</td>\n",
       "      <td>0.749441</td>\n",
       "      <td>0.339765</td>\n",
       "      <td>0.497763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Strategy  Recall@5  Recall@20  \\\n",
       "0          Instruction + Role + Motivation (Negative)  0.598434   0.749441   \n",
       "1   Instruction + Role + Motivation (Negative) + E...  0.598434   0.749441   \n",
       "2          Instruction + Role + Motivation (Positive)  0.598434   0.749441   \n",
       "3   Instruction + Role + Motivation (Positive) + E...  0.598434   0.749441   \n",
       "4                               Motivation (Negative)  0.598434   0.749441   \n",
       "5                               Motivation (Positive)  0.598434   0.749441   \n",
       "6                                                Role  0.598434   0.749441   \n",
       "7                        Role + Motivation (Negative)  0.598434   0.749441   \n",
       "8                        Role + Motivation (Positive)  0.598434   0.749441   \n",
       "9                         chain-of-thought (Standard)  0.598434   0.749441   \n",
       "10                                           few-shot  0.598434   0.749441   \n",
       "11                                          zero-shot  0.598434   0.749441   \n",
       "\n",
       "    Citation Precision  Citation Recall  \n",
       "0             0.308166         0.531879  \n",
       "1             0.284787         0.552013  \n",
       "2             0.309172         0.538031  \n",
       "3             0.271644         0.538031  \n",
       "4             0.290324         0.522931  \n",
       "5             0.300615         0.516219  \n",
       "6             0.311130         0.532998  \n",
       "7             0.301007         0.527964  \n",
       "8             0.301678         0.531879  \n",
       "9             0.287025         0.531879  \n",
       "10            0.351902         0.532998  \n",
       "11            0.339765         0.497763  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- 1) Robust parsing into lists of strings ---\n",
    "def parse_id_list(cell):\n",
    "    # If it‚Äôs already a Python list, just stringify each element\n",
    "    if isinstance(cell, list):\n",
    "        return [str(x) for x in cell]\n",
    "    # If it‚Äôs a string, pull out all digit-runs (handles spaces, commas, no commas, brackets, etc.)\n",
    "    if isinstance(cell, str):\n",
    "        return re.findall(r\"\\d+\", cell)\n",
    "    # Anything else (NaN, None, etc.) ‚Üí empty list\n",
    "    return []\n",
    "\n",
    "for col in [\n",
    "    \"Retrieved Corpus IDs\",\n",
    "    \"Ground Truth Corpus IDs\",\n",
    "    \"Cited Corpus IDs\"\n",
    "]:\n",
    "    rag_results_df[col] = rag_results_df[col].apply(parse_id_list)\n",
    "\n",
    "# Optional sanity check:\n",
    "print(rag_results_df.loc[0, [\n",
    "    \"Retrieved Corpus IDs\",\n",
    "    \"Ground Truth Corpus IDs\",\n",
    "    \"Cited Corpus IDs\"\n",
    "]])\n",
    "\n",
    "# --- 2) Compute metrics per row ---\n",
    "def compute_metrics(row):\n",
    "    retrieved = row[\"Retrieved Corpus IDs\"][:20]\n",
    "    ground    = set(row[\"Ground Truth Corpus IDs\"])\n",
    "    cited     = set(row[\"Cited Corpus IDs\"])\n",
    "\n",
    "    tp5   = len(set(retrieved[:5]) & ground)\n",
    "    tp20  = len(set(retrieved)     & ground)\n",
    "    tpc   = len(cited              & ground)\n",
    "\n",
    "    recall5  = tp5  / len(ground) if ground else 0.0\n",
    "    recall20 = tp20 / len(ground) if ground else 0.0\n",
    "    prec     = tpc  / len(cited)  if cited  else 0.0\n",
    "    rec      = tpc  / len(ground) if ground else 0.0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"Recall@5\":           recall5,\n",
    "        \"Recall@20\":          recall20,\n",
    "        \"Citation Precision\": prec,\n",
    "        \"Citation Recall\":    rec\n",
    "    })\n",
    "\n",
    "metrics_df = rag_results_df.apply(compute_metrics, axis=1)\n",
    "rag_results_df = pd.concat([rag_results_df, metrics_df], axis=1)\n",
    "\n",
    "# --- 3) Aggregate by strategy ---\n",
    "# Note: your DataFrame uses column \"Strategy\" (capital S) not \"strategy\"\n",
    "metrics_summary = (\n",
    "    rag_results_df\n",
    "    .groupby(\"Strategy\")[\n",
    "        [\"Recall@5\",\"Recall@20\",\"Citation Precision\",\"Citation Recall\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "display(metrics_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea307549-ae17-47c4-81b8-5b9f9d2fe398",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 280\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Main Execution\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Load your datasets (replace with actual paths)\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# corpus_df = pd.read_csv(\"corpus.csv\")\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# query_df = pd.read_csv(\"queries.csv\")\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Initialize evaluator\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mLitSearchEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# Evaluate all retrievers\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     all_results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[2], line 183\u001b[0m, in \u001b[0;36mLitSearchEvaluator.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroq \u001b[38;5;241m=\u001b[39m Groq(api_key\u001b[38;5;241m=\u001b[39mGROQ_API_KEY)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrievers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m#\"e5\": E5Retriever(corpus_df),\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m#\"gtr\": GTRRetriever(corpus_df),\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrit\u001b[39m\u001b[38;5;124m\"\u001b[39m: GRITRetriever(\u001b[43mcorpus_df\u001b[49m)\n\u001b[1;32m    184\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_df' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gritlm import GritLM\n",
    "\n",
    "# Configuration\n",
    "WORKSPACE_PATH = \"/data/horse/ws/uskh580e-myws\"\n",
    "GROQ_API_KEY = \"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\"\n",
    "os.makedirs(WORKSPACE_PATH, exist_ok=True)\n",
    "\n",
    "# Base Retriever Class (for shared functionality)\n",
    "class BaseRetriever:\n",
    "    def __init__(self, corpus_df, text_column=\"abstract\"):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.text_column = text_column\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "        \n",
    "    def _content_hash(self):\n",
    "        \"\"\"Generate unique hash for corpus state\"\"\"\n",
    "        content = self.corpus_df[self.text_column].astype(str).sum()\n",
    "        metadata = str(self.corpus_df.index.tolist())\n",
    "        return hash(f\"{content}|{metadata}\")\n",
    "    \n",
    "    def _prepare_embeddings(self): \n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def retrieve(self, query, top_k=5):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# E5 Retriever Implementation\n",
    "class E5Retriever(BaseRetriever):\n",
    "    def __init__(self, corpus_df):\n",
    "        self.model_name = \"intfloat/e5-large-v2\"\n",
    "        self.cache_file = os.path.join(WORKSPACE_PATH, \"e5_embeddings.pkl\")\n",
    "        super().__init__(corpus_df)\n",
    "        \n",
    "    def _prepare_embeddings(self):\n",
    "        # Try loading cached embeddings\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, \"rb\") as f:\n",
    "                    stored_hash, embeddings = pickle.load(f)\n",
    "                    if stored_hash == self._content_hash():\n",
    "                        self.embeddings = embeddings\n",
    "                        print(\"‚úÖ Loaded cached E5 embeddings\")\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è E5 cache load failed: {e}\")\n",
    "                \n",
    "        # Compute fresh embeddings\n",
    "        print(\"üîÑ Computing E5 embeddings...\")\n",
    "        model = SentenceTransformer(self.model_name, \n",
    "                                  device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        texts = [\"passage: \" + text for text in self.corpus_df[self.text_column].astype(str)]\n",
    "        self.embeddings = model.encode(\n",
    "            texts, \n",
    "            batch_size=256, \n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump((self._content_hash(), self.embeddings), f)\n",
    "            \n",
    "    def retrieve(self, query, top_k=5):\n",
    "        model = SentenceTransformer(self.model_name)\n",
    "        query = \"query: \" + query\n",
    "        query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        indices = np.argsort(-scores[0])[:top_k]\n",
    "        return self.corpus_df.iloc[indices].to_dict(orient='records')\n",
    "\n",
    "# GTR Retriever Implementation\n",
    "class GTRRetriever(BaseRetriever):\n",
    "    def __init__(self, corpus_df):\n",
    "        self.model_name = \"sentence-transformers/gtr-t5-large\"\n",
    "        self.cache_file = os.path.join(WORKSPACE_PATH, \"gtr_embeddings.pkl\")\n",
    "        super().__init__(corpus_df)\n",
    "        \n",
    "    def _prepare_embeddings(self):\n",
    "        # Try loading cached embeddings\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, \"rb\") as f:\n",
    "                    stored_hash, embeddings = pickle.load(f)\n",
    "                    if stored_hash == self._content_hash():\n",
    "                        self.embeddings = embeddings\n",
    "                        print(\"‚úÖ Loaded cached GTR embeddings\")\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è GTR cache load failed: {e}\")\n",
    "                \n",
    "        # Compute fresh embeddings\n",
    "        print(\"üîÑ Computing GTR embeddings...\")\n",
    "        model = SentenceTransformer(self.model_name, \n",
    "                                  device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.embeddings = model.encode(\n",
    "            self.corpus_df[self.text_column].astype(str),\n",
    "            batch_size=256,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump((self._content_hash(), self.embeddings), f)\n",
    "            \n",
    "    def retrieve(self, query, top_k=5):\n",
    "        model = SentenceTransformer(self.model_name)\n",
    "        query_embedding = model.encode([query])\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        indices = np.argsort(-scores[0])[:top_k]\n",
    "        return self.corpus_df.iloc[indices].to_dict(orient='records')\n",
    "\n",
    "# GRIT Retriever Implementation\n",
    "class GRITRetriever(BaseRetriever):\n",
    "    def __init__(self, corpus_df):\n",
    "        self.model_name = \"GritLM/GritLM-7B\"\n",
    "        self.instruction = \"Represent this query for retrieving relevant documents:\"\n",
    "        self.cache_file = os.path.join(WORKSPACE_PATH, \"grit_embeddings.pkl\")\n",
    "        super().__init__(corpus_df)\n",
    "        \n",
    "    def _prepare_embeddings(self):\n",
    "        # Try loading cached embeddings\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, \"rb\") as f:\n",
    "                    stored_hash, embeddings = pickle.load(f)\n",
    "                    if stored_hash == self._content_hash():\n",
    "                        self.embeddings = embeddings\n",
    "                        print(\"‚úÖ Loaded cached GRIT embeddings\")\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è GRIT cache load failed: {e}\")\n",
    "                \n",
    "        # Compute fresh embeddings\n",
    "        print(\"üîÑ Computing GRIT embeddings... (This may take a while)\")\n",
    "        model = GritLM(self.model_name,\n",
    "                      torch_dtype=torch.bfloat16,\n",
    "                      device_map=\"auto\",\n",
    "                      mode=\"embedding\")\n",
    "        \n",
    "        texts = [f\"<|embed|>\\n{text}\" for text in self.corpus_df[self.text_column].astype(str)]\n",
    "        self.embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=128,  # Reduced for stability\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        # Handle NaN values\n",
    "        self.embeddings = np.nan_to_num(self.embeddings, nan=0.0)\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump((self._content_hash(), self.embeddings), f)\n",
    "            \n",
    "    def retrieve(self, query, top_k=5):\n",
    "        model = GritLM(self.model_name, mode=\"embedding\")\n",
    "        formatted_query = f\"<|user|>\\n{self.instruction}\\n<|embed|>\\n{query}\"\n",
    "        query_embedding = model.encode([formatted_query], convert_to_numpy=True)\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        indices = np.argsort(-scores[0])[:top_k]\n",
    "        return self.corpus_df.iloc[indices].to_dict(orient='records')\n",
    "\n",
    "# RAG Evaluation Pipeline\n",
    "class LitSearchEvaluator:\n",
    "    def __init__(self):\n",
    "        self.groq = Groq(api_key=GROQ_API_KEY)\n",
    "        self.retrievers = {\n",
    "            #\"e5\": E5Retriever(corpus_df),\n",
    "            #\"gtr\": GTRRetriever(corpus_df),\n",
    "            \"grit\": GRITRetriever(corpus_df)\n",
    "        }\n",
    "        \n",
    "    def reformulate_query(self, query, strategy):\n",
    "        if strategy == \"zero-shot\":\n",
    "            return query\n",
    "        elif strategy == \"few-shot\":\n",
    "            return f\"\"\"Answer with citations like (Document ID: X). Examples:\n",
    "            Question: \"Transformer models\"\n",
    "            Answer: \"Use self-attention mechanisms (Document ID: 123). \n",
    "                     Outperform RNNs on long sequences (Document ID: 456).\"\n",
    "            Now answer: {query}\"\"\"\n",
    "        elif strategy == \"chain-of-thought\":\n",
    "            return f\"\"\"Analyze step-by-step:\n",
    "            1. Key concepts in: \"{query}\"\n",
    "            2. Match to Document IDs\n",
    "            3. Cite sources as (Document ID: X)\n",
    "            Final Answer:\"\"\"\n",
    "        return query\n",
    "    \n",
    "    def generate_answer(self, query, context_docs):\n",
    "        if not context_docs:\n",
    "            return \"No relevant documents found\"\n",
    "            \n",
    "        context = \"RETRIEVED DOCUMENTS:\\n\" + \"\\n\\n\".join([\n",
    "            f\"Document ID: {doc['corpusid']}\\nTitle: {doc.get('title', '')}\\nContent: {doc['abstract'][:200]}...\"\n",
    "            for doc in context_docs\n",
    "        ])\n",
    "        \n",
    "        response = self.groq.chat.completions.create(\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a research assistant. Cite sources using (Document ID: X) format.\"\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            }],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0,\n",
    "            max_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def extract_citations(self, text):\n",
    "        citations = re.findall(r'Document ID: (\\d+)', text)\n",
    "        return list(map(int, citations))\n",
    "    \n",
    "    def evaluate_retriever(self, retriever_name, strategies=None, sample_frac=0.1):\n",
    "        \"\"\"Evaluate a single retriever with multiple strategies\"\"\"\n",
    "        if retriever_name not in self.retrievers:\n",
    "            print(f\"‚ö†Ô∏è Retriever {retriever_name} not available\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        strategies = strategies or [\"zero-shot\", \"few-shot\", \"chain-of-thought\"]\n",
    "        results = []\n",
    "        retriever = self.retrievers[retriever_name]\n",
    "        \n",
    "        # Use a sample of queries\n",
    "        query_sample = query_df.sample(frac=sample_frac, random_state=42)\n",
    "        \n",
    "        for _, row in tqdm(query_sample.iterrows(), desc=f\"Evaluating {retriever_name}\"):\n",
    "            ground_truth = set(row[\"corpusids\"])\n",
    "            query = row[\"query\"]\n",
    "            \n",
    "            for strategy in strategies:\n",
    "                modified_query = self.reformulate_query(query, strategy)\n",
    "                retrieved_docs = retriever.retrieve(modified_query)\n",
    "                answer = self.generate_answer(modified_query, retrieved_docs)\n",
    "                citations = self.extract_citations(answer)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                tp = len(set(citations) & ground_truth)\n",
    "                precision = tp / len(citations) if citations else 0\n",
    "                recall = tp / len(ground_truth) if ground_truth else 0\n",
    "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "                \n",
    "                results.append({\n",
    "                    \"retriever\": retriever_name,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"query\": query,\n",
    "                    \"citations\": citations,\n",
    "                    \"ground_truth\": list(ground_truth),\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "                \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets (replace with actual paths)\n",
    "    # corpus_df = pd.read_csv(\"corpus.csv\")\n",
    "    # query_df = pd.read_csv(\"queries.csv\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = LitSearchEvaluator()\n",
    "    \n",
    "    # Evaluate all retrievers\n",
    "    all_results = []\n",
    "    for retriever_name in [\"grit\"]:\n",
    "        results_df = evaluator.evaluate_retriever(retriever_name, sample_frac=0.1)\n",
    "        if not results_df.empty:\n",
    "            results_df.to_csv(os.path.join(WORKSPACE_PATH, f\"{retriever_name}_results.csv\"), index=False)\n",
    "            all_results.append(results_df)\n",
    "    \n",
    "    # Combine and analyze results\n",
    "    if all_results:\n",
    "        full_results = pd.concat(all_results)\n",
    "        full_results.to_csv(os.path.join(WORKSPACE_PATH, \"all_results.csv\"), index=False)\n",
    "        \n",
    "        # Performance summary\n",
    "        summary = full_results.groupby([\"retriever\", \"strategy\"]).agg({\n",
    "            \"precision\": \"mean\",\n",
    "            \"recall\": \"mean\",\n",
    "            \"f1\": \"mean\"\n",
    "        }).reset_index()\n",
    "        \n",
    "        print(\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "        print(summary)\n",
    "        \n",
    "        # Save summary\n",
    "        summary.to_csv(os.path.join(WORKSPACE_PATH, \"performance_summary.csv\"), index=False)\n",
    "        \n",
    "        # Sample output\n",
    "        sample = full_results.iloc[0]\n",
    "        print(\"\\n=== SAMPLE RESULT ===\")\n",
    "        print(f\"Retriever: {sample['retriever']}\")\n",
    "        print(f\"Strategy: {sample['strategy']}\")\n",
    "        print(f\"Query: {sample['query']}\")\n",
    "        print(f\"Answer Excerpt: {sample['answer'][:200]}...\")\n",
    "        print(f\"Cited IDs: {sample['citations']}\")\n",
    "        print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "        print(f\"F1 Score: {sample['f1']:.2f}\")\n",
    "    else:\n",
    "        print(\"No results generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4051109-792f-43d2-9b4e-95cd937b6260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071400cd-7459-40f4-85ab-a54e762f57ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/horse/ws/uskh580e-myws/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace set up at: /data/horse/ws/uskh580e-myws\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907a2c9d-b204-46f4-b0cc-52fe314a198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully and stored in: /data/horse/ws/uskh580e-myws/huggingface_datasets\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84896e6-aa60-4b78-bc6e-54a882edf7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4b196e-cc2f-4f5a-8c1e-05499568b949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_set</th>\n",
       "      <th>query</th>\n",
       "      <th>specificity</th>\n",
       "      <th>quality</th>\n",
       "      <th>corpusids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any research papers on methods to co...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[202719327]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any resources available for translat...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[227231792]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any studies that explore post-hoc te...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[226254579, 204976362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there any tools or studies that have focus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[10961392, 12160022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inline_acl</td>\n",
       "      <td>Are there papers that propose contextualized c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[233296182]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_set                                              query  specificity  \\\n",
       "0  inline_acl  Are there any research papers on methods to co...            0   \n",
       "1  inline_acl  Are there any resources available for translat...            1   \n",
       "2  inline_acl  Are there any studies that explore post-hoc te...            0   \n",
       "3  inline_acl  Are there any tools or studies that have focus...            1   \n",
       "4  inline_acl  Are there papers that propose contextualized c...            1   \n",
       "\n",
       "   quality               corpusids  \n",
       "0        2             [202719327]  \n",
       "1        2             [227231792]  \n",
       "2        2  [226254579, 204976362]  \n",
       "3        2    [10961392, 12160022]  \n",
       "4        2             [233296182]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93301dcd-1138-42b9-82db-0882c0528280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRITRetriever:\n",
    "    def __init__(self, corpus_df, model_name=\"GritLM/GritLM-7B\"):\n",
    "        self.corpus_df = corpus_df\n",
    "        self.model_name = model_name\n",
    "        self.cache_file = os.path.join(WORKSPACE_PATH, \"grit_embeddings.pkl\")\n",
    "        self.embeddings = None\n",
    "        self._prepare_embeddings()\n",
    "    \n",
    "    def _prepare_embeddings(self):\n",
    "        if self._try_load_cache():\n",
    "            return\n",
    "            \n",
    "        print(\"üîÑ Computing GRIT embeddings...\")\n",
    "        model = GritLM(self.model_name, \n",
    "                      torch_dtype=torch.bfloat16,\n",
    "                      device_map=\"auto\",\n",
    "                      mode=\"embedding\")\n",
    "\n",
    "        texts = [f\"<|embed|>\\n{text}\" for text in self.corpus_df[\"abstract\"].astype(str)]\n",
    "        \n",
    "        self.embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=128,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        # Normalize embeddings\n",
    "        norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1e-10  # Avoid division by zero\n",
    "        self.embeddings = self.embeddings / norms\n",
    "        self._save_cache()\n",
    "\n",
    "    def _try_load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, \"rb\") as f:\n",
    "                    stored_hash, embeddings = pickle.load(f)\n",
    "                    self.embeddings = embeddings\n",
    "                    print(\"‚úÖ Loaded cached GRIT embeddings\")\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Cache load failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    def _save_cache(self):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump((None, self.embeddings), f)\n",
    "            \n",
    "    def filter_duplicates(self, docs):\n",
    "        \"\"\"Ensure diversity in retrieved documents\"\"\"\n",
    "        seen_ids = set()\n",
    "        unique_docs = []\n",
    "        for doc in docs:\n",
    "            if doc['corpusid'] not in seen_ids:\n",
    "                seen_ids.add(doc['corpusid'])\n",
    "                unique_docs.append(doc)\n",
    "        return unique_docs[:5]  # Return top 5 unique\n",
    "\n",
    "    def retrieve(self, query, top_k=10):  # Retrieve more then filter\n",
    "        model = GritLM(self.model_name, mode=\"embedding\")\n",
    "        formatted_query = f\"<|user|>\\nRepresent this query for retrieving relevant documents:\\n<|embed|>\\n{query}\"\n",
    "        query_embedding = model.encode([formatted_query], convert_to_numpy=True)\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        scores = query_embedding @ self.embeddings.T\n",
    "        indices = np.argsort(-scores[0])[:top_k]\n",
    "        retrieved = self.corpus_df.iloc[indices].to_dict(orient='records')\n",
    "        return self.filter_duplicates(retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0493c2b5-fc32-474d-91bc-31206ec9d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, retrieved_docs):\n",
    "    # Return early if no documents\n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant documents found for this query.\"\n",
    "    \n",
    "    context = \"RELEVANT DOCUMENTS:\\n\" + \"\\n---\\n\".join([\n",
    "        f\"[Document {doc['corpusid']}] {doc.get('title', 'Untitled')}\\nAbstract: {doc['abstract'][:200]}...\"\n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a precise research assistant. Follow these rules:\n",
    "            1. Base answers ONLY on provided documents\n",
    "            2. Cite sources as (Corpus ID: X) after each claim\n",
    "            3. If document conflicts, mention both\n",
    "            4. For unsupported claims, say \"No supporting evidence\"\n",
    "            5. Keep citations concise and relevant\"\"\"\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{context}\\n\\nQUESTION: {query}\\nANSWER:\"\n",
    "        }],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        temperature=0,  # Balanced for creativity/accuracy\n",
    "        max_tokens=512,\n",
    "        stop=[\"\\n\\n\", \"###\"]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Robust citation extraction\n",
    "def extract_citations(text):\n",
    "    patterns = [\n",
    "        r'\\(Corpus ID: (\\d+)\\)', \n",
    "        r'Corpus ID: (\\d+)',\n",
    "        r'Source: (\\d+)',\n",
    "        r'\\[(\\d+)\\]'\n",
    "    ]\n",
    "    citations = []\n",
    "    for pattern in patterns:\n",
    "        citations.extend(re.findall(pattern, text))\n",
    "    return list(map(int, citations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bba038-a532-4f66-a094-4fe9331b94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, corpus_df, text_column=\"abstract\", stopword_removal=True, use_stemming=True):\n",
    "        \"\"\"\n",
    "        BM25 Retriever for document retrieval.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus_df: Pandas DataFrame containing the corpus\n",
    "        - text_column: Column in corpus_df that contains the text for retrieval\n",
    "        - stopword_removal: Whether to remove stopwords\n",
    "        - use_stemming: Whether to apply stemming\n",
    "        \"\"\"\n",
    "        self.corpus_df = corpus_df\n",
    "        self.text_column = text_column\n",
    "        self.stopword_removal = stopword_removal\n",
    "        self.use_stemming = use_stemming\n",
    "        self.bm25 = None\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def tokenize_and_stem(self, text):\n",
    "        \"\"\" Tokenize, remove stopwords, and apply stemming (if enabled). \"\"\"\n",
    "        tokens = word_tokenize(text.lower())  # Tokenize & lowercase\n",
    "        if self.stopword_removal:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]  # Remove stopwords\n",
    "        if self.use_stemming:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "        return tokens\n",
    "    def create_index(self):\n",
    "        \"\"\" Create BM25 index from the corpus. \"\"\"\n",
    "        print(\"Tokenizing and processing corpus...\")\n",
    "        self.corpus_df[\"tokens\"] = self.corpus_df[self.text_column].astype(str).apply(self.tokenize_and_stem)\n",
    "        self.bm25 = BM25Okapi(self.corpus_df[\"tokens\"].tolist())\n",
    "\n",
    "    def save_index(self, filename=BM25_INDEX_PATH):\n",
    "        \"\"\" Save BM25 index to workspace. \"\"\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self.bm25, f)\n",
    "        print(f\"BM25 index saved to {filename}\")\n",
    "\n",
    "    def load_index(self, filename=BM25_INDEX_PATH):\n",
    "        \"\"\" Load BM25 index from workspace. \"\"\"\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.bm25 = pickle.load(f)\n",
    "        print(f\"BM25 index loaded from {filename}\")\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        \"\"\" Retrieve top-k documents for a query. \"\"\"\n",
    "        tokenized_query = self.tokenize_and_stem(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        ranked_indices = np.argsort(scores)[::-1][:k]\n",
    "        return self.corpus_df.iloc[ranked_indices][\"corpusid\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1655c430-d313-4238-913f-37e0b7f93b44",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:7\u001b[0;36m\u001b[0m\n\u001b[0;31m    def save_index(self, filename=BM25_INDEX_PATH):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2fe4d-fa99-4b02-a41e-d6909068d1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a86fe-748e-474b-be38-fb389a68aead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529dd5c-554c-473b-8b65-0807974bf987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "534b31b5-db0a-42e4-8c79-815bd45e21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[\"corpusids\"] = query_df[\"corpusids\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa75fa2-5cd1-4845-8c38-35ad1aaa16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GritLM in ./myenv/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: mteb in ./myenv/lib/python3.10/site-packages (from GritLM) (1.38.9)\n",
      "Requirement already satisfied: accelerate>=0.26.1 in ./myenv/lib/python3.10/site-packages (from GritLM) (1.6.0)\n",
      "Requirement already satisfied: transformers>=4.37.2 in ./myenv/lib/python3.10/site-packages (from GritLM) (4.51.3)\n",
      "Requirement already satisfied: wandb in ./myenv/lib/python3.10/site-packages (from GritLM) (0.19.11)\n",
      "Requirement already satisfied: datasets>=2.16.1 in ./myenv/lib/python3.10/site-packages (from GritLM) (3.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (2.2.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (0.31.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (2.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (25.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (0.5.3)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.1->GritLM) (7.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (20.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (3.5.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (4.67.1)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (2025.3.0)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (2.2.3)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from datasets>=2.16.1->GritLM) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.10/site-packages (from transformers>=4.37.2->GritLM) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./myenv/lib/python3.10/site-packages (from transformers>=4.37.2->GritLM) (0.21.1)\n",
      "Requirement already satisfied: eval_type_backport>=0.0.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (0.2.2)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (2.11.4)\n",
      "Requirement already satisfied: scikit_learn>=1.0.2 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (1.6.1)\n",
      "Requirement already satisfied: sentence_transformers>=3.0.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (4.1.0)\n",
      "Requirement already satisfied: rich>=0.0.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (14.0.0)\n",
      "Requirement already satisfied: scipy>=0.0.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (1.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (4.13.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.6 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (0.5.7)\n",
      "Requirement already satisfied: polars>=0.20.22 in ./myenv/lib/python3.10/site-packages (from mteb->GritLM) (1.29.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (3.1.44)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (8.2.0)\n",
      "Requirement already satisfied: setproctitle in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (1.3.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (0.4.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (62.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (2.28.0)\n",
      "Requirement already satisfied: platformdirs in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./myenv/lib/python3.10/site-packages (from wandb->GritLM) (6.31.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./myenv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->GritLM) (1.17.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./myenv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (3.11.18)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./myenv/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->GritLM) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.10/site-packages (from pydantic>=2.0.0->mteb->GritLM) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.10/site-packages (from pydantic>=2.0.0->mteb->GritLM) (0.4.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.10/site-packages (from pydantic>=2.0.0->mteb->GritLM) (2.33.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->GritLM) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->GritLM) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->GritLM) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.1->GritLM) (2025.4.26)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./myenv/lib/python3.10/site-packages (from rich>=0.0.0->mteb->GritLM) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./myenv/lib/python3.10/site-packages (from rich>=0.0.0->mteb->GritLM) (3.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.10/site-packages (from scikit_learn>=1.0.2->mteb->GritLM) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.10/site-packages (from scikit_learn>=1.0.2->mteb->GritLM) (1.5.0)\n",
      "Requirement already satisfied: Pillow in ./myenv/lib/python3.10/site-packages (from sentence_transformers>=3.0.0->mteb->GritLM) (11.2.1)\n",
      "Requirement already satisfied: triton==3.3.0 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (3.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./myenv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.1->GritLM) (11.3.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->GritLM) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->GritLM) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.10/site-packages (from pandas->datasets>=2.16.1->GritLM) (2.9.0.post0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (6.4.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (1.20.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.1->GritLM) (0.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./myenv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->GritLM) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./myenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=0.0.0->mteb->GritLM) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.1->GritLM) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.1->GritLM) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/data/horse/ws/uskh580e-llm_citation_ws/myenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install GritLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec87232-0bec-4a1e-966b-17447f949e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gritlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c4123c-425f-4f6c-914d-d77cf8c39e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Computing GRIT embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GritLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grit_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mGRITRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mGRITRetriever.__init__\u001b[0;34m(self, corpus_df, model_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(WORKSPACE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrit_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mGRITRetriever._prepare_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Computing GRIT embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGritLM\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \n\u001b[1;32m     15\u001b[0m               torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     16\u001b[0m               device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m               mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|embed|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     22\u001b[0m     texts,\n\u001b[1;32m     23\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     24\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GritLM' is not defined"
     ]
    }
   ],
   "source": [
    "grit_retriever = GRITRetriever(corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11fc0a3f-76e9-42b3-85be-9fffb85752a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Computing GRIT embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GritLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ‚úÖ **Initialize Retrievers**\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grit_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mGRITRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m retrievers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#\"E5\": e5_retriever,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#\"GTR\": gtr_retriever,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRITLM\u001b[39m\u001b[38;5;124m\"\u001b[39m: grit_retriever\n\u001b[1;32m      7\u001b[0m }\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mGRITRetriever.__init__\u001b[0;34m(self, corpus_df, model_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(WORKSPACE_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrit_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mGRITRetriever._prepare_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Computing GRIT embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGritLM\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \n\u001b[1;32m     15\u001b[0m               torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     16\u001b[0m               device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m               mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|embed|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     22\u001b[0m     texts,\n\u001b[1;32m     23\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     24\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GritLM' is not defined"
     ]
    }
   ],
   "source": [
    "# ‚úÖ **Initialize Retrievers**\n",
    "grit_retriever = GRITRetriever(corpus_df)\n",
    "retrievers = {\n",
    "    #\"E5\": e5_retriever,\n",
    "    #\"GTR\": gtr_retriever,\n",
    "    \"GRITLM\": grit_retriever\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "647da639-a650-49be-ac53-32715d070b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_rag(original_query, modified_query, retrieved_corpus_ids, retrieved_titles, retrieved_texts):\n",
    "    \"\"\" Generates a concise answer using RAG, citing only the top 1‚Äì2 relevant sources. \"\"\"\n",
    "    context = \"\\n\".join([\n",
    "        f\"Corpus ID: {doc_id}\\nTitle: {title}\\nFull Text: {full_text}\"\n",
    "        for doc_id, title, full_text in zip(retrieved_corpus_ids, retrieved_titles, retrieved_texts)\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Question: {modified_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732d7c99-dee5-4b0e-b559-77632b441f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query(query, strategy):\n",
    "    \"\"\" Reformulates the query only for LLM input, not for retrieval. \"\"\"\n",
    "\n",
    "    if strategy == \"zero-shot\":\n",
    "        return query\n",
    "\n",
    "    elif strategy == \"few-shot\":\n",
    "        return f\"\"\"Here are examples of how to answer questions while citing sources from retrieved documents.\n",
    "        ...\n",
    "        Now, using the retrieved documents below, answer the question while citing sources explicitly:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Standard)\":\n",
    "        return f\"\"\"Let's break down the question step by step:\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Neutral)\":\n",
    "        return f\"\"\"You are an AI assistant tasked with answering a research question...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Praise)\":\n",
    "        return f\"\"\"You are an AI assistant known for your high-quality answers...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Punishment)\":\n",
    "        return f\"\"\"You are an AI assistant whose output will be evaluated...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-thought (Academic Aspiration)\":\n",
    "        return f\"\"\"You are a rising academic in the field of AI...\n",
    "        ...\n",
    "        Now, process this query:\n",
    "        {query}\"\"\"\n",
    "\n",
    "    elif strategy == \"self-verification\":\n",
    "        return f\"\"\"\n",
    "        Step 1: Generate an initial answer based on retrieved documents.\n",
    "        Step 2: Verify each claim and ensure that citations match the retrieved evidence.\n",
    "        Step 3: If a claim is unsupported, explicitly state that it cannot be verified.\n",
    "\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-verification\":\n",
    "        return f\"\"\"\n",
    "        Let's break this down into a verification process:\n",
    "        ...\n",
    "        Now, answer the following question:\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-citation\":\n",
    "        return f\"\"\"Answer the question step by step, citing the relevant corpus ID after each reasoning step.\n",
    "Use this format: [corpus: document_id]\n",
    "\n",
    "Now, begin reasoning:\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    elif strategy == \"chain-of-quote\":\n",
    "        return f\"\"\"Answer the question step by step, including a short direct quote from the supporting document for each reasoning step.\n",
    "Use this format: ‚Äúquoted sentence‚Äù [corpus: document_id]\n",
    "\n",
    "Now, begin reasoning:\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c4b7a-a30b-405f-a84f-ca48c634b7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
